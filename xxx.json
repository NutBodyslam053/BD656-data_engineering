{
  "kind": "Listing",
  "data": {
    "after": "t3_1bzufap",
    "dist": 100,
    "modhash": null,
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "",
          "author_fullname": "t2_yeda6sl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "7 Steps to Mastering Data Engineering",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1c2eo9b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3NFroF1UfaqTI2cKPEe_VrolPnLobrEjbZEDcN68iVU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712942072.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "kdnuggets.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.kdnuggets.com/7-steps-to-mastering-data-engineering",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fnWEwhgUbIe2UKAgdmuzwK4z0vcKeECLYt1AyvO1d-I.jpg?auto=webp&amp;s=7c9ffcfccdc26c88c2017269696207f296186e2f",
                  "width": 1000,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fnWEwhgUbIe2UKAgdmuzwK4z0vcKeECLYt1AyvO1d-I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1453a9493cc94b8cd2f4b9f2c50cbb36071ad6f",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://external-preview.redd.it/fnWEwhgUbIe2UKAgdmuzwK4z0vcKeECLYt1AyvO1d-I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1df0daebfa8ac1a1dc6e4a8950a37031b9beb677",
                    "width": 216,
                    "height": 129
                  },
                  {
                    "url": "https://external-preview.redd.it/fnWEwhgUbIe2UKAgdmuzwK4z0vcKeECLYt1AyvO1d-I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a2e5e3126afe8e82af9dec1f9defce8cad5e4e9",
                    "width": 320,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/fnWEwhgUbIe2UKAgdmuzwK4z0vcKeECLYt1AyvO1d-I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3201619f811803f553e6dc07937247d16c260da0",
                    "width": 640,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/fnWEwhgUbIe2UKAgdmuzwK4z0vcKeECLYt1AyvO1d-I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b33af2492b58b9050dc18d7b7687f08694fda8a2",
                    "width": 960,
                    "height": 576
                  }
                ],
                "variants": {},
                "id": "izhJenDrivMm7XBNpWZRDdY3Ex97rqKMqHbXWy826J0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c2eo9b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kingabzpro",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c2eo9b/7_steps_to_mastering_data_engineering/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.kdnuggets.com/7-steps-to-mastering-data-engineering",
          "subreddit_subscribers": 176091,
          "created_utc": 1712942072.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I've been playing around with polars (https://docs.pola.rs/) and really like it. I'm curious though, does anyone know if there's an option for larger than memory datasets? (A bit like spark or dask)",
          "author_fullname": "t2_126zgy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Polars for larger than memory datasets?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1c2dm6a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712939504.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been playing around with polars (&lt;a href=\"https://docs.pola.rs/\"&gt;https://docs.pola.rs/&lt;/a&gt;) and really like it. I&amp;#39;m curious though, does anyone know if there&amp;#39;s an option for larger than memory datasets? (A bit like spark or dask)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c2dm6a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "houseofleft",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c2dm6a/polars_for_larger_than_memory_datasets/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c2dm6a/polars_for_larger_than_memory_datasets/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712939504.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Senior data engineer here with about 6 yoe. I've been casually applying for the past 4 years to keep a pulse on the market. These applications were for decent jobs at Tier 1-3 tech companies, or high growth startups. \n\n2021/2022 - 1 response for every 5 applications. Multiple recruiters hounding me every day. Process were expedited, I could expect to go from application to offer in 20 days. Job searcher rating: 10/10\n\n2023 - 1 response for every 50 applications, if I'm lucky. 1 to 2 recruiter messages a month for jobs I would not consider. I could expect to get ghosted every time. Job searcher rating: 0/10\n\n2024 - 1 response for every 25 applications. 3 to 4 recruiter messages a week, with maybe 1 I'd consider. I could expect 2 to 3 weeks between each stage, 4-6 stages, ghosting, 1 to 2 hour technicals, and take homes. Job searcher rating: 2/10\n\nOverall: \n\n* Entry level is f'd.\n* 1-4 years, it's possible.\n* 5+ years, might as well be an acrobat, cause you're gonna be jumping through so many hoops and hurdles. \n\nAt this point, if you're employed, it's so hard commit that much time to job searching. Thoughts?\n\n&amp;#x200B;",
          "author_fullname": "t2_fhmml14j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Job Market Slightly Better, but Employers are SO PICKY and Hiring Process TOO LONG!!",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c2bcq1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712934069.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Senior data engineer here with about 6 yoe. I&amp;#39;ve been casually applying for the past 4 years to keep a pulse on the market. These applications were for decent jobs at Tier 1-3 tech companies, or high growth startups. &lt;/p&gt;\n\n&lt;p&gt;2021/2022 - 1 response for every 5 applications. Multiple recruiters hounding me every day. Process were expedited, I could expect to go from application to offer in 20 days. Job searcher rating: 10/10&lt;/p&gt;\n\n&lt;p&gt;2023 - 1 response for every 50 applications, if I&amp;#39;m lucky. 1 to 2 recruiter messages a month for jobs I would not consider. I could expect to get ghosted every time. Job searcher rating: 0/10&lt;/p&gt;\n\n&lt;p&gt;2024 - 1 response for every 25 applications. 3 to 4 recruiter messages a week, with maybe 1 I&amp;#39;d consider. I could expect 2 to 3 weeks between each stage, 4-6 stages, ghosting, 1 to 2 hour technicals, and take homes. Job searcher rating: 2/10&lt;/p&gt;\n\n&lt;p&gt;Overall: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Entry level is f&amp;#39;d.&lt;/li&gt;\n&lt;li&gt;1-4 years, it&amp;#39;s possible.&lt;/li&gt;\n&lt;li&gt;5+ years, might as well be an acrobat, cause you&amp;#39;re gonna be jumping through so many hoops and hurdles. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;At this point, if you&amp;#39;re employed, it&amp;#39;s so hard commit that much time to job searching. Thoughts?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c2bcq1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Capable-Jicama2155",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c2bcq1/job_market_slightly_better_but_employers_are_so/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c2bcq1/job_market_slightly_better_but_employers_are_so/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712934069.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I am 31M, I am working as L3/Team Lead SQL server database administrator in my current organisation. I have around 8+ yrs of SQL database administrator experience in my biodata. In which I have experience of 1yrs 8 months in back office and 1yrs 10 month experience as application support. After this job I switched my profile to SQL server database administrator and I show all my experience as SQL server administrator.\n\nNow I am seeking advice for my career advancement. I want to know what should I do to increase my skills and my pay scale?",
          "author_fullname": "t2_e8hp9i3c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Seeking career advice as L3 SQL server database administrator",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c2adp0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1712931922.0,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712931629.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am 31M, I am working as L3/Team Lead SQL server database administrator in my current organisation. I have around 8+ yrs of SQL database administrator experience in my biodata. In which I have experience of 1yrs 8 months in back office and 1yrs 10 month experience as application support. After this job I switched my profile to SQL server database administrator and I show all my experience as SQL server administrator.&lt;/p&gt;\n\n&lt;p&gt;Now I am seeking advice for my career advancement. I want to know what should I do to increase my skills and my pay scale?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c2adp0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Large-Alternative802",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c2adp0/seeking_career_advice_as_l3_sql_server_database/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c2adp0/seeking_career_advice_as_l3_sql_server_database/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712931629.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hello guys . I want a presentation with attached axes .\n- When to move to the cloud?\n- Which provider to choose?\n- Which cloud solution to choose?\n- Which data to put on which type of cloud (Private, public, community, and hybrid)?\n- Make a comparison between existing open-source solutions (AWS, Azure, etc....)\nIf someone has a presentation similar to the following axes please provide me with it",
          "author_fullname": "t2_wd4zpb4g0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Pr\u00e9sentation needed",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c2acin",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712931552.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys . I want a presentation with attached axes .\n- When to move to the cloud?\n- Which provider to choose?\n- Which cloud solution to choose?\n- Which data to put on which type of cloud (Private, public, community, and hybrid)?\n- Make a comparison between existing open-source solutions (AWS, Azure, etc....)\nIf someone has a presentation similar to the following axes please provide me with it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c2acin",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Careful-Edge-7488",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c2acin/pr\u00e9sentation_needed/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c2acin/pr\u00e9sentation_needed/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712931552.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "",
          "author_fullname": "t2_qvgybr5my",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any real enterprise-grade on-premise alternatives to Cloudera Data Platform?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c292wk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OyJNZcJ1rlO_VpvO2DA49-CFoDpoODyU_Yf85FPWOhM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712928288.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ulq9k015v1uc1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ulq9k015v1uc1.png?auto=webp&amp;s=2de71dd43b3adfcd8cd95b85ae95f9fb2638ff0a",
                  "width": 2587,
                  "height": 1521
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ulq9k015v1uc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec8bca345ebb98eebb2756abfd3a0a6f776ebd60",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/ulq9k015v1uc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9197ef8e4edf44920826058613c48a3db03f0d97",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/ulq9k015v1uc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ade5670c1e68977ab3e5ad4f98039739e72547c4",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://preview.redd.it/ulq9k015v1uc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ecacf541510d060c276dfa633989b6320a8fcff",
                    "width": 640,
                    "height": 376
                  },
                  {
                    "url": "https://preview.redd.it/ulq9k015v1uc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e7fad72c0821c996e1c7884030d0bc33eb82436",
                    "width": 960,
                    "height": 564
                  },
                  {
                    "url": "https://preview.redd.it/ulq9k015v1uc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0188a0e6a0c405a3e2544bfd1c7df3dca05b846c",
                    "width": 1080,
                    "height": 634
                  }
                ],
                "variants": {},
                "id": "-ckeBJlLOfQA6WkY8GumNuxDKAGErcfb3lQUPeqs0aw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c292wk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "seaborn_as_sns",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c292wk/are_there_any_real_enterprisegrade_onpremise/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://i.redd.it/ulq9k015v1uc1.png",
          "subreddit_subscribers": 176091,
          "created_utc": 1712928288.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "[In this post](https://www.alphaservesp.com/blog/top-10-data-engineering-tools), we write about the top 10 tools for data engineering that will help industry experts boost performance, scalability, and innovation. \n\n  \n What do you think? \n\nAre you agree or disagree with this top? \n\nWhat would you add? ",
          "author_fullname": "t2_j9t6je88b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TOP Data Engineering Tools",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c292wf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712928287.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.alphaservesp.com/blog/top-10-data-engineering-tools\"&gt;In this post&lt;/a&gt;, we write about the top 10 tools for data engineering that will help industry experts boost performance, scalability, and innovation. &lt;/p&gt;\n\n&lt;p&gt;What do you think? &lt;/p&gt;\n\n&lt;p&gt;Are you agree or disagree with this top? &lt;/p&gt;\n\n&lt;p&gt;What would you add? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/orqvnF0ewbIHqpMLA2STtHReciY6ZwR7XYkuTzqIKUQ.jpg?auto=webp&amp;s=bfe62d641c1abfa50bb3b9c9afff26e54031e110",
                  "width": 1680,
                  "height": 945
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/orqvnF0ewbIHqpMLA2STtHReciY6ZwR7XYkuTzqIKUQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=44a6f6bb8f9002047c26198887148bd7558639ce",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/orqvnF0ewbIHqpMLA2STtHReciY6ZwR7XYkuTzqIKUQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e96e96891b4fa30993f14d3ebe0cf2a6c5a29cc",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/orqvnF0ewbIHqpMLA2STtHReciY6ZwR7XYkuTzqIKUQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=293cca10f89ec0d7049a29bc457e06c5f5035f14",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/orqvnF0ewbIHqpMLA2STtHReciY6ZwR7XYkuTzqIKUQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74e86455e80711e384c50e6c7a4ed056fded690d",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/orqvnF0ewbIHqpMLA2STtHReciY6ZwR7XYkuTzqIKUQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cfec73f4dea9bc1a1e230bbd9236c361b90ae6d5",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/orqvnF0ewbIHqpMLA2STtHReciY6ZwR7XYkuTzqIKUQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=053d8d2a28b8fba597e64e72ea8d2e5fea2e8a2d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "mTqdJ4OYD2O8iGJ6zbrb_pyPrPNpU75VlSQp9sK8e6Q"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c292wf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DariaAlpha",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c292wf/top_data_engineering_tools/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c292wf/top_data_engineering_tools/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712928287.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hello everyone, I'm hoping some of the veterans here can offer me some advice on how to drive my career forward and things I should be doing to land a well-paying career in DE (or DS nominally)  \n\n\n**Background:** Undergrad in Data Science, and worked in BI for 3 years. In my role, I already do a bit of analytics eng work using S3, Snowflake, DBT. I am good with Python and SQL but not a SWE level expert by any means.\n\n**Goal:** I was never truly passionate about math/stats and therefore have not had much exposure to pure DS work. I enjoy analytics work and also programming which is why I now want to switch to DE full-time (or a DS doing eng heavy work and less math/research)\n\n**Current Plan:** I will be starting an MSDS program this year. This will teach me a mix of topics in ML/DL, Statistics, Big Data Systems, Cloud Technologies etc. In addition, I am learning on my own time as well - Udemy for AWS knowledge and reading Designing Data-Intensive Applications for foundational knowledge. I know that doing personal projects is key and I hope to do that as well but it's really hard to find unique ideas.  \n\n\nI'm hoping you all can help me with recommendations on what else I should be doing, or things to focus on more than others. I think I am really close to making the switch from BI/DA to DE or DS and just need a bit of guidance. ",
          "author_fullname": "t2_wrmszt00a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for career advice coming from 3YOE in BI to maximize pay and opportunities",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c29207",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712928217.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;m hoping some of the veterans here can offer me some advice on how to drive my career forward and things I should be doing to land a well-paying career in DE (or DS nominally)  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; Undergrad in Data Science, and worked in BI for 3 years. In my role, I already do a bit of analytics eng work using S3, Snowflake, DBT. I am good with Python and SQL but not a SWE level expert by any means.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; I was never truly passionate about math/stats and therefore have not had much exposure to pure DS work. I enjoy analytics work and also programming which is why I now want to switch to DE full-time (or a DS doing eng heavy work and less math/research)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Plan:&lt;/strong&gt; I will be starting an MSDS program this year. This will teach me a mix of topics in ML/DL, Statistics, Big Data Systems, Cloud Technologies etc. In addition, I am learning on my own time as well - Udemy for AWS knowledge and reading Designing Data-Intensive Applications for foundational knowledge. I know that doing personal projects is key and I hope to do that as well but it&amp;#39;s really hard to find unique ideas.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping you all can help me with recommendations on what else I should be doing, or things to focus on more than others. I think I am really close to making the switch from BI/DA to DE or DS and just need a bit of guidance. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c29207",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loud_Teaching_5769",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c29207/looking_for_career_advice_coming_from_3yoe_in_bi/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c29207/looking_for_career_advice_coming_from_3yoe_in_bi/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712928217.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Does this sound familiar?\n\nYou invest heavily in data, empower employees with self-service analytics... but instead of unlocking value, you end up in a state of total data chaos. This self-service paradox - where giving users more access breeds more confusion, not clarity.\n\nI've this issue plague countless organizations. It often feels like a pendulum swing between too much self-service and excessive governance.\n\nSo, how do you all manage to strike the right balance? What strategies have you found effective in breaking free from this cycle?\n\n[https://www.castordoc.com/blog/the-self-service-paradox](https://www.castordoc.com/blog/the-self-service-paradox)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/uy26f83dn1uc1.png?width=500&amp;format=png&amp;auto=webp&amp;s=bd0b949d2642d4e93d81526aef6471e63c3ea3fa",
          "author_fullname": "t2_9blh4yzs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Self-Service Paradox",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 65,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "uy26f83dn1uc1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 138,
                  "x": 108,
                  "u": "https://preview.redd.it/uy26f83dn1uc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bfd8f723867cc37f0f9be3ad058d5f172e4a27d"
                },
                {
                  "y": 276,
                  "x": 216,
                  "u": "https://preview.redd.it/uy26f83dn1uc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9192c749201f582a0520d94e6ef0d5d8f4fea27"
                },
                {
                  "y": 410,
                  "x": 320,
                  "u": "https://preview.redd.it/uy26f83dn1uc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d6392f71f1db2f069cda17d5f6e375e3a13932e"
                }
              ],
              "s": {
                "y": 641,
                "x": 500,
                "u": "https://preview.redd.it/uy26f83dn1uc1.png?width=500&amp;format=png&amp;auto=webp&amp;s=bd0b949d2642d4e93d81526aef6471e63c3ea3fa"
              },
              "id": "uy26f83dn1uc1"
            }
          },
          "name": "t3_1c286ed",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Meme",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/TnZJ92UL6M-c5741h_zenMx1G0oO7RbaYeiUl7DZ-e0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1712925751.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does this sound familiar?&lt;/p&gt;\n\n&lt;p&gt;You invest heavily in data, empower employees with self-service analytics... but instead of unlocking value, you end up in a state of total data chaos. This self-service paradox - where giving users more access breeds more confusion, not clarity.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve this issue plague countless organizations. It often feels like a pendulum swing between too much self-service and excessive governance.&lt;/p&gt;\n\n&lt;p&gt;So, how do you all manage to strike the right balance? What strategies have you found effective in breaking free from this cycle?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.castordoc.com/blog/the-self-service-paradox\"&gt;https://www.castordoc.com/blog/the-self-service-paradox&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uy26f83dn1uc1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd0b949d2642d4e93d81526aef6471e63c3ea3fa\"&gt;https://preview.redd.it/uy26f83dn1uc1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd0b949d2642d4e93d81526aef6471e63c3ea3fa&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kjbAsuKVyqNPNKq2W5uwXaO5NlVu3wtXyIQxurzoLW4.jpg?auto=webp&amp;s=00708d8df6547345f43d2b1292f64f32b53da7e2",
                  "width": 1320,
                  "height": 622
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kjbAsuKVyqNPNKq2W5uwXaO5NlVu3wtXyIQxurzoLW4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ceeb91c91ac6081dc58703ae35334c58540cc5a3",
                    "width": 108,
                    "height": 50
                  },
                  {
                    "url": "https://external-preview.redd.it/kjbAsuKVyqNPNKq2W5uwXaO5NlVu3wtXyIQxurzoLW4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba197b708dc078f515337130a469bceb49e0a61f",
                    "width": 216,
                    "height": 101
                  },
                  {
                    "url": "https://external-preview.redd.it/kjbAsuKVyqNPNKq2W5uwXaO5NlVu3wtXyIQxurzoLW4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=02907a72f61621b175a38abff48d82ec16798a4b",
                    "width": 320,
                    "height": 150
                  },
                  {
                    "url": "https://external-preview.redd.it/kjbAsuKVyqNPNKq2W5uwXaO5NlVu3wtXyIQxurzoLW4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa84b099b4dde71439a9b91c141c0805fe7e93ae",
                    "width": 640,
                    "height": 301
                  },
                  {
                    "url": "https://external-preview.redd.it/kjbAsuKVyqNPNKq2W5uwXaO5NlVu3wtXyIQxurzoLW4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=03afb90077b012084aea4d4d4a416459dd0afa7f",
                    "width": 960,
                    "height": 452
                  },
                  {
                    "url": "https://external-preview.redd.it/kjbAsuKVyqNPNKq2W5uwXaO5NlVu3wtXyIQxurzoLW4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d3dba2b55230e367dab17d95ad90ab2d1ee4936",
                    "width": 1080,
                    "height": 508
                  }
                ],
                "variants": {},
                "id": "iIet-DucvgtwmMvySAkMLW_7Qr5eVcgfDGfO9840Jvo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff66ac",
          "id": "1c286ed",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Strict_Algae3766",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c286ed/the_selfservice_paradox/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c286ed/the_selfservice_paradox/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712925751.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Currently a cloudera customer. Data is not that huge (3TB) we pay around ~125K USD per year for the licenses. We are 100% on prem so cloud is not an option. \n\nFor those that also work on prem, and is using MinIO, what are the challenges? What SQL engine do you use?\n\nI am thinking of migrating our workload. Any insight is appreciated.",
          "author_fullname": "t2_gy8ywqou9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MinIO + Trino (Or other SQL engine that uses hive metastore) in production",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c26zbc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712922038.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently a cloudera customer. Data is not that huge (3TB) we pay around ~125K USD per year for the licenses. We are 100% on prem so cloud is not an option. &lt;/p&gt;\n\n&lt;p&gt;For those that also work on prem, and is using MinIO, what are the challenges? What SQL engine do you use?&lt;/p&gt;\n\n&lt;p&gt;I am thinking of migrating our workload. Any insight is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c26zbc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mayabangnatao",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c26zbc/minio_trino_or_other_sql_engine_that_uses_hive/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c26zbc/minio_trino_or_other_sql_engine_that_uses_hive/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712922038.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Recently, I started using Great Expectations to validate data on MSSQL, and I encountered some issues that I found challenging to resolve, despite reading the documentation of this tool. I would greatly appreciate everyone's help. Here are some of the issues I've encountered:\n\n1. I need to write custom expectations using SQL to meet some business requirements. However, I find Great Expectation's documentation on this matter unclear, and the code hasn't been updated to the latest version.\n2. I need to accurately identify the error records in a table when running an expectation. I found the return\\_unexpected\\_index\\_query  \n, but when the expectation includes row conditions, the query returned does not include the row condition.\n\nI'm not sure if I've missed something in the documentation or if I've overlooked something. Please help me out.",
          "author_fullname": "t2_2q3ifruw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help Needed with Custom Expectations and Error Identification in Great Expectations on MSSQL",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c26f6a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712920165.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, I started using Great Expectations to validate data on MSSQL, and I encountered some issues that I found challenging to resolve, despite reading the documentation of this tool. I would greatly appreciate everyone&amp;#39;s help. Here are some of the issues I&amp;#39;ve encountered:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I need to write custom expectations using SQL to meet some business requirements. However, I find Great Expectation&amp;#39;s documentation on this matter unclear, and the code hasn&amp;#39;t been updated to the latest version.&lt;/li&gt;\n&lt;li&gt;I need to accurately identify the error records in a table when running an expectation. I found the return_unexpected_index_query&lt;br/&gt;\n, but when the expectation includes row conditions, the query returned does not include the row condition.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if I&amp;#39;ve missed something in the documentation or if I&amp;#39;ve overlooked something. Please help me out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c26f6a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IllogicalAmbassador",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c26f6a/help_needed_with_custom_expectations_and_error/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c26f6a/help_needed_with_custom_expectations_and_error/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712920165.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "How likely am I to find a company that doesn't use any kind of low code/no code ETL tool? I keep applying to places that have a super modern tech stack listed in the posting and then when I get into conversations with the team, it turns out they use Informatica or Datastage or Talend etc.  \n\nGUI ETL tools are much maligned on this subreddit and for good reason but I also kind of get the impression that everyone uses them to some degree?",
          "author_fullname": "t2_t1bnfnc4q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Companies without low code?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c264q4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712919198.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How likely am I to find a company that doesn&amp;#39;t use any kind of low code/no code ETL tool? I keep applying to places that have a super modern tech stack listed in the posting and then when I get into conversations with the team, it turns out they use Informatica or Datastage or Talend etc.  &lt;/p&gt;\n\n&lt;p&gt;GUI ETL tools are much maligned on this subreddit and for good reason but I also kind of get the impression that everyone uses them to some degree?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c264q4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bjogc42069",
          "discussion_type": null,
          "num_comments": 51,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c264q4/companies_without_low_code/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c264q4/companies_without_low_code/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712919198.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "So I am switching from Data visualization to Data engineering. I started working on Qlik then moved to Power BI and now I want to move away from Data visualization to Data engineering. I cleared DP-203 and databricks certifications but I dont have any handson experience. My organisation is a service based organisation and we have very few projects using databricks and hence they cant provide me hands on opportunity.\n\nI am not confident in giving interviews because of lack on experience and also profile is not selected as most of my experience is in data viz.\n\nI have total 5+ years of experience and I cleared the certification in Feb.\n\nWhat I looking for?\n\n1) Either a project which can be challenging and fun through which I can gain experience.\n\n2) Or a way to clear the interviews",
          "author_fullname": "t2_u9cuoqp6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cleared databricks data engineering associate certfication but have \"Zero\" hands on experience",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c24mdd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1712915372.0,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712913213.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am switching from Data visualization to Data engineering. I started working on Qlik then moved to Power BI and now I want to move away from Data visualization to Data engineering. I cleared DP-203 and databricks certifications but I dont have any handson experience. My organisation is a service based organisation and we have very few projects using databricks and hence they cant provide me hands on opportunity.&lt;/p&gt;\n\n&lt;p&gt;I am not confident in giving interviews because of lack on experience and also profile is not selected as most of my experience is in data viz.&lt;/p&gt;\n\n&lt;p&gt;I have total 5+ years of experience and I cleared the certification in Feb.&lt;/p&gt;\n\n&lt;p&gt;What I looking for?&lt;/p&gt;\n\n&lt;p&gt;1) Either a project which can be challenging and fun through which I can gain experience.&lt;/p&gt;\n\n&lt;p&gt;2) Or a way to clear the interviews&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c24mdd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wide-Recognition-607",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c24mdd/cleared_databricks_data_engineering_associate/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c24mdd/cleared_databricks_data_engineering_associate/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712913213.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hello fellow engineers,\n\nCould you please suggest a dependable EL tool for moving data from Clickup to BigQuery? We have tried Fivetran, Meltano, and Airbyte without success. \n\nI see hevodata and portable.io supports Clickup. Are there any better choices? Thank you!",
          "author_fullname": "t2_b4vqw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a reliable EL tool for Clickup",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c231bd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712906688.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow engineers,&lt;/p&gt;\n\n&lt;p&gt;Could you please suggest a dependable EL tool for moving data from Clickup to BigQuery? We have tried Fivetran, Meltano, and Airbyte without success. &lt;/p&gt;\n\n&lt;p&gt;I see hevodata and portable.io supports Clickup. Are there any better choices? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c231bd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "s9ne",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c231bd/looking_for_a_reliable_el_tool_for_clickup/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c231bd/looking_for_a_reliable_el_tool_for_clickup/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712906688.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hello everyone,\n\nI started playing with UC, but can't understand how it stores the dlt tables in the dls. \n\nCurrently, I use Az Synapse Serverless to expose the data to the BI team. UC doesn't seem to keep the same folder structure &lt;schema&gt;/tables/&lt;table name&gt;\n\nCan I query tables, that were created in UC, from Az Synapse serverless? ",
          "author_fullname": "t2_7p1fczdj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tables in Unit Catalog",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c22rb9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712905533.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I started playing with UC, but can&amp;#39;t understand how it stores the dlt tables in the dls. &lt;/p&gt;\n\n&lt;p&gt;Currently, I use Az Synapse Serverless to expose the data to the BI team. UC doesn&amp;#39;t seem to keep the same folder structure &amp;lt;schema&amp;gt;/tables/&amp;lt;table name&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;Can I query tables, that were created in UC, from Az Synapse serverless? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c22rb9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Far-Inspection9930",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c22rb9/tables_in_unit_catalog/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c22rb9/tables_in_unit_catalog/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712905533.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi everyone and thank you in advance for taking some time for reading this post. I am starting my journey of becoming data engineer. However, I am confused about which cloud platform or tools should I master in order to stay relevant in the job market. I have done some research and come across some famous cloud platforms i.e. GCP, AWS, AZURE, SNOWFLAKE AND DATABRICK. Which platforms has more growth and demand in the future? Plus, I want to later transition into AI. So, please consider this too.\n\nMy Background: I have experience in Oracle EBS, Oracle Fusion Cloud, Custom BI reports. I have developed good skill in SQL and Python.  And a total of 2 years of experience in IT domain. \n\nThanks",
          "author_fullname": "t2_d9336uu6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which one to choose among GCP, AWS, AZURE, SNOWFLAKE AND DATABRICK.",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c22koa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712904842.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone and thank you in advance for taking some time for reading this post. I am starting my journey of becoming data engineer. However, I am confused about which cloud platform or tools should I master in order to stay relevant in the job market. I have done some research and come across some famous cloud platforms i.e. GCP, AWS, AZURE, SNOWFLAKE AND DATABRICK. Which platforms has more growth and demand in the future? Plus, I want to later transition into AI. So, please consider this too.&lt;/p&gt;\n\n&lt;p&gt;My Background: I have experience in Oracle EBS, Oracle Fusion Cloud, Custom BI reports. I have developed good skill in SQL and Python.  And a total of 2 years of experience in IT domain. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c22koa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ToughAd3865",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c22koa/which_one_to_choose_among_gcp_aws_azure_snowflake/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c22koa/which_one_to_choose_among_gcp_aws_azure_snowflake/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712904842.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi friends, I'm trying to evaluate which sql parser should I use.  \nI've tried antlr4, but it has memory leak, if use a new cache every time, the performance's bad.\n\nLooks like calcite and sqlglot are very popular, wondering if anyone has tried both of them? and how do you like them? Also there's a JSql Parser.  \nCalcite and Jsql Parser are both built with Javacc, written in Java, while sqlglot is written in python.\n\n  \nI'd love to do some benchmarks myself, but before that, wondering if someone already has done something similar. Thanks!",
          "author_fullname": "t2_3heia02h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What Sql Parser to use",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1yz1r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712892275.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi friends, I&amp;#39;m trying to evaluate which sql parser should I use.&lt;br/&gt;\nI&amp;#39;ve tried antlr4, but it has memory leak, if use a new cache every time, the performance&amp;#39;s bad.&lt;/p&gt;\n\n&lt;p&gt;Looks like calcite and sqlglot are very popular, wondering if anyone has tried both of them? and how do you like them? Also there&amp;#39;s a JSql Parser.&lt;br/&gt;\nCalcite and Jsql Parser are both built with Javacc, written in Java, while sqlglot is written in python.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to do some benchmarks myself, but before that, wondering if someone already has done something similar. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1yz1r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xuanziermao",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1yz1r/what_sql_parser_to_use/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1yz1r/what_sql_parser_to_use/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712892275.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hello everyone,\n\nI've been reflecting on my experiences with cloud data warehouses and am curious to gather insights from this community on whether they truly are the best choice for data management or whether we think they are good because everyone is using them.\n\n**Background:**\n\nIn a previous role, our entire infra was on premise and I recommended cloud data warehouses because our team was small, and managing our infrastructure was a challenge. I was genuinely intrigued by the capabilities of cloud solutions like Snowflake, Redshift, and Databricks.\n\n**Current Reflections:**\n\nSince actually using these services, I've started questioning if a cloud data warehouse is always the right choice. While they offer scalability and efficiency, they also pose potential issues like cost and vendor lock-in and these platforms also need platform-specific skills to use them efficiently.\n\n**Seeking Your Input:**\n\nI understand the decision to use a cloud data warehouse depends on numerous factors like team size, budget, and skills. Here\u2019s where I need your thoughts:\n\n1. **Decision-Making Factors:** How should a team decide whether to opt for a cloud data warehouse? What are the key considerations?\n2. **Pre-Decision Questions:** Before committing to a cloud solution, what questions should be addressed? How can one evaluate if it's the right fit?\n3. **Personal Experiences:** For those who have migrated from on-premises to a cloud data platform, what was your experience? Did the benefits outweigh the challenges? Was it cost-efficient? How did you measure it?",
          "author_fullname": "t2_74wc977i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you decide if cloud data warehouses are really good for you?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1xy9p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712889259.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been reflecting on my experiences with cloud data warehouses and am curious to gather insights from this community on whether they truly are the best choice for data management or whether we think they are good because everyone is using them.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In a previous role, our entire infra was on premise and I recommended cloud data warehouses because our team was small, and managing our infrastructure was a challenge. I was genuinely intrigued by the capabilities of cloud solutions like Snowflake, Redshift, and Databricks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Reflections:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Since actually using these services, I&amp;#39;ve started questioning if a cloud data warehouse is always the right choice. While they offer scalability and efficiency, they also pose potential issues like cost and vendor lock-in and these platforms also need platform-specific skills to use them efficiently.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Seeking Your Input:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I understand the decision to use a cloud data warehouse depends on numerous factors like team size, budget, and skills. Here\u2019s where I need your thoughts:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Decision-Making Factors:&lt;/strong&gt; How should a team decide whether to opt for a cloud data warehouse? What are the key considerations?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Pre-Decision Questions:&lt;/strong&gt; Before committing to a cloud solution, what questions should be addressed? How can one evaluate if it&amp;#39;s the right fit?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Personal Experiences:&lt;/strong&gt; For those who have migrated from on-premises to a cloud data platform, what was your experience? Did the benefits outweigh the challenges? Was it cost-efficient? How did you measure it?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1xy9p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aakashnand",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1xy9p/how_do_you_decide_if_cloud_data_warehouses_are/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1xy9p/how_do_you_decide_if_cloud_data_warehouses_are/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712889259.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I\u2019m an OSS developer (primarily working on Dask) and lately I\u2019ve been talking to users about how they\u2019re using Dask for ETL-style production workflows and this inspired me to make something myself. I wanted a simple example that met the following criteria:\n  \n- **Run locally (optionally)**. Should be easy to try out locally and easily scalable.  \n- **Scalable to cloud**. I didn\u2019t want to think hard about cloud deployment.  \n- **Python forward**. I wanted to use tools familiar to Python users, not an ETL expert.  \n\nThe resulting data pipeline uses Prefect for workflow orchestration, Dask to scale the data processing across a cluster, Delta Lake for storage, and Coiled to deploy Dask on the cloud.\n  \nI really like the outcome, but wanted to get more balanced feedback since lately I\u2019ve been more on the side of building these tools rather than using them heavily for data engineering. Some questions I\u2019ve had include:  \n- **Prefect vs. Airflow vs. Dagster?** For the users I\u2019ve been working with at Coiled, Prefect is the most commonly used tool. I also know Dagster is quite popular and could easily be swapped into this example.  \n- **DeltaLake or something else?** To be honest I mostly see vanilla Parquet in the wild, but I\u2019ve been curious about Delta for a while and mostly wanted an excuse to try it out (pandas and Dask support improved a lot with delta-rs).  \n   \nAnyway, if people have a chance to read things over and give feedback I\u2019d welcome constructive critique.\n\nBlog post: https://docs.coiled.io/blog/easy-scalable-production-etl.html \nCode: https://github.com/coiled/etl-tpch",
          "author_fullname": "t2_w7crvjmu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Example Data Pipeline with Prefect, Delta Lake, and Dask",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1fkv2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712843448.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an OSS developer (primarily working on Dask) and lately I\u2019ve been talking to users about how they\u2019re using Dask for ETL-style production workflows and this inspired me to make something myself. I wanted a simple example that met the following criteria:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Run locally (optionally)&lt;/strong&gt;. Should be easy to try out locally and easily scalable.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scalable to cloud&lt;/strong&gt;. I didn\u2019t want to think hard about cloud deployment.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Python forward&lt;/strong&gt;. I wanted to use tools familiar to Python users, not an ETL expert.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The resulting data pipeline uses Prefect for workflow orchestration, Dask to scale the data processing across a cluster, Delta Lake for storage, and Coiled to deploy Dask on the cloud.&lt;/p&gt;\n\n&lt;p&gt;I really like the outcome, but wanted to get more balanced feedback since lately I\u2019ve been more on the side of building these tools rather than using them heavily for data engineering. Some questions I\u2019ve had include:&lt;br/&gt;\n- &lt;strong&gt;Prefect vs. Airflow vs. Dagster?&lt;/strong&gt; For the users I\u2019ve been working with at Coiled, Prefect is the most commonly used tool. I also know Dagster is quite popular and could easily be swapped into this example.&lt;br/&gt;\n- &lt;strong&gt;DeltaLake or something else?&lt;/strong&gt; To be honest I mostly see vanilla Parquet in the wild, but I\u2019ve been curious about Delta for a while and mostly wanted an excuse to try it out (pandas and Dask support improved a lot with delta-rs).  &lt;/p&gt;\n\n&lt;p&gt;Anyway, if people have a chance to read things over and give feedback I\u2019d welcome constructive critique.&lt;/p&gt;\n\n&lt;p&gt;Blog post: &lt;a href=\"https://docs.coiled.io/blog/easy-scalable-production-etl.html\"&gt;https://docs.coiled.io/blog/easy-scalable-production-etl.html&lt;/a&gt; \nCode: &lt;a href=\"https://github.com/coiled/etl-tpch\"&gt;https://github.com/coiled/etl-tpch&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c1fkv2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dask-jeeves",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1fkv2/example_data_pipeline_with_prefect_delta_lake_and/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1fkv2/example_data_pipeline_with_prefect_delta_lake_and/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712843448.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I'm considering Clickhouse for a project involving analytics and lots of joins. Very often, a large table will need to be joined with multiple smaller (but potentially still big-ish) tables. Sometimes I've read that JOINs are one of Clickhouse's potential pain points, but I've seen that mostly in comments, couldn't really find any in depth articles on why they could be so painful. What's your opinion, if you're using Clickhouse extensively? For example the explanation one can find [here](https://clickhouse.com/blog/common-getting-started-issues-with-clickhouse#joins) seems legit.\n\nSo are a lot of joins a lot of pain in Clickhouse? \n",
          "author_fullname": "t2_n1rtk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are Click house JOINs that bad?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1swf6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712875706.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m considering Clickhouse for a project involving analytics and lots of joins. Very often, a large table will need to be joined with multiple smaller (but potentially still big-ish) tables. Sometimes I&amp;#39;ve read that JOINs are one of Clickhouse&amp;#39;s potential pain points, but I&amp;#39;ve seen that mostly in comments, couldn&amp;#39;t really find any in depth articles on why they could be so painful. What&amp;#39;s your opinion, if you&amp;#39;re using Clickhouse extensively? For example the explanation one can find &lt;a href=\"https://clickhouse.com/blog/common-getting-started-issues-with-clickhouse#joins\"&gt;here&lt;/a&gt; seems legit.&lt;/p&gt;\n\n&lt;p&gt;So are a lot of joins a lot of pain in Clickhouse? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pykhBqtOFQwmZRoCAaXSwrUJOlrlgEKKbaVS66P7SSU.jpg?auto=webp&amp;s=04b0df0d226e8f387a09a454cd7b496a91175a5e",
                  "width": 3431,
                  "height": 1931
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pykhBqtOFQwmZRoCAaXSwrUJOlrlgEKKbaVS66P7SSU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50128cfbcb8a5f6c3780121e931834f95de43af2",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/pykhBqtOFQwmZRoCAaXSwrUJOlrlgEKKbaVS66P7SSU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8659e69b5a59424dd9cec0632c0f9af8889fb473",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/pykhBqtOFQwmZRoCAaXSwrUJOlrlgEKKbaVS66P7SSU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a59c83d1b0e5956213e9953bebf63325f910ffd3",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/pykhBqtOFQwmZRoCAaXSwrUJOlrlgEKKbaVS66P7SSU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d942d0fa31e2b52ce4f1da28de42ed8f9aeb13bc",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/pykhBqtOFQwmZRoCAaXSwrUJOlrlgEKKbaVS66P7SSU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=47d9293f2165130af2cfb1925bda9b11933ec3ce",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/pykhBqtOFQwmZRoCAaXSwrUJOlrlgEKKbaVS66P7SSU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14af539fec7e1e332edeede45f877925409bd213",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Gb7N-LRAxjJaxxs7BQufEjM_PP-t-5jBEzcsBuRMm1s"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1swf6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "stefanondisponibile",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1swf6/are_click_house_joins_that_bad/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1swf6/are_click_house_joins_that_bad/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712875706.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I\u2019m in the process of picking a data contract framework, and I\u2019m testing Soda at the moment. I\u2019ve seen people recommending Soda over great expectations, so decided to give it a shot. \n\nI have a few questions: \n\n1. Do you need API key for using Soda CLI? \n2. What\u2019s included in free tier of Soda cloud (if anything)? Are you able to get API keys?\n\nMy case: \nI would like to run checks on s3 files (mostly csv and parquet data formats). It seems that soda has no integration with s3 which sucks. My solution was to pull data from s3 and run soda on local files, using dask-sql source and python. It seems there is no CLI available for local files. And I needed to get an API key to run Soda programmatically. I\u2019m not sure if what I\u2019m describing here is a viable approach long term given that you can only get api keys from their cloud service. Idk, this entire setup was not pleasant. \n\nSorry for a partial rant. ",
          "author_fullname": "t2_72w13hsg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Data contracts: Soda",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1sgzr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712874654.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in the process of picking a data contract framework, and I\u2019m testing Soda at the moment. I\u2019ve seen people recommending Soda over great expectations, so decided to give it a shot. &lt;/p&gt;\n\n&lt;p&gt;I have a few questions: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Do you need API key for using Soda CLI? &lt;/li&gt;\n&lt;li&gt;What\u2019s included in free tier of Soda cloud (if anything)? Are you able to get API keys?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My case: \nI would like to run checks on s3 files (mostly csv and parquet data formats). It seems that soda has no integration with s3 which sucks. My solution was to pull data from s3 and run soda on local files, using dask-sql source and python. It seems there is no CLI available for local files. And I needed to get an API key to run Soda programmatically. I\u2019m not sure if what I\u2019m describing here is a viable approach long term given that you can only get api keys from their cloud service. Idk, this entire setup was not pleasant. &lt;/p&gt;\n\n&lt;p&gt;Sorry for a partial rant. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1sgzr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sashathecrimean",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1sgzr/data_contracts_soda/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1sgzr/data_contracts_soda/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712874654.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I work at a time series company and we\u2019re debating Polars vs DuckDB for upping our data processing game on speed for particular python jobs. We retrieve our data from a PostgreSQL Database. I tried both at a shallow level to try implement a weekly rolling average.\n\nI was pretty impressed with DuckDB\u2019s completeness and ease of use. It was fast, and didn\u2019t require any format conversions between pandas, or have any sensitivity to variable formats etc, it handled a lot under the hood.\n\n I found some friction trying it in Polars however. Was very disappointed with their SQL interface that was completely rigid since it translates to polars calls, and once I got over their specificities for format, the rolling average they returned was incorrect. It just returned the same value for everything. After this I had a go at trying to type their calls directly but got put off once seeing they stated their functionality for rolling average unstable in their docs. \n\nDoes it make sense to have a tech stack that has both DuckDB and polars? Some are debating it should be one over the other for some reasons also highlighted in the below article. And does it make sense to have DuckDB when we already have a PostgreSQL database we have to reference in the first place?\n\nLeast I think it\u2019s handy for situations where you want to retrieve a lot of data (ie in my case for ML) but also perform analytics on it fast. At least DuckDB lets you work off the same big data frame you\u2019ve already queried rather than making the query twice.\n\nLet me know your opinions! Looking for anyone with experience and thoughts.\n\nA resource I found on the matter:\nhttps://www.confessionsofadataguy.com/duckdb-vs-polars-for-data-engineering/",
          "author_fullname": "t2_8wnmfyij",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Polars vs DuckDB",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1rpym",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712872900.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a time series company and we\u2019re debating Polars vs DuckDB for upping our data processing game on speed for particular python jobs. We retrieve our data from a PostgreSQL Database. I tried both at a shallow level to try implement a weekly rolling average.&lt;/p&gt;\n\n&lt;p&gt;I was pretty impressed with DuckDB\u2019s completeness and ease of use. It was fast, and didn\u2019t require any format conversions between pandas, or have any sensitivity to variable formats etc, it handled a lot under the hood.&lt;/p&gt;\n\n&lt;p&gt;I found some friction trying it in Polars however. Was very disappointed with their SQL interface that was completely rigid since it translates to polars calls, and once I got over their specificities for format, the rolling average they returned was incorrect. It just returned the same value for everything. After this I had a go at trying to type their calls directly but got put off once seeing they stated their functionality for rolling average unstable in their docs. &lt;/p&gt;\n\n&lt;p&gt;Does it make sense to have a tech stack that has both DuckDB and polars? Some are debating it should be one over the other for some reasons also highlighted in the below article. And does it make sense to have DuckDB when we already have a PostgreSQL database we have to reference in the first place?&lt;/p&gt;\n\n&lt;p&gt;Least I think it\u2019s handy for situations where you want to retrieve a lot of data (ie in my case for ML) but also perform analytics on it fast. At least DuckDB lets you work off the same big data frame you\u2019ve already queried rather than making the query twice.&lt;/p&gt;\n\n&lt;p&gt;Let me know your opinions! Looking for anyone with experience and thoughts.&lt;/p&gt;\n\n&lt;p&gt;A resource I found on the matter:\n&lt;a href=\"https://www.confessionsofadataguy.com/duckdb-vs-polars-for-data-engineering/\"&gt;https://www.confessionsofadataguy.com/duckdb-vs-polars-for-data-engineering/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jByxci1GM9Z6sNIN_KMwvKesJhqjDNSBcqmwf7dSYqA.jpg?auto=webp&amp;s=48ac1f81f6782b452a20e490bae17572c8042edd",
                  "width": 1030,
                  "height": 687
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jByxci1GM9Z6sNIN_KMwvKesJhqjDNSBcqmwf7dSYqA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2af6b5545231fe519d18933a6623ebabc5fb1252",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/jByxci1GM9Z6sNIN_KMwvKesJhqjDNSBcqmwf7dSYqA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f129340e45b7fc5c02316d2d9b02d84bb58321b7",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/jByxci1GM9Z6sNIN_KMwvKesJhqjDNSBcqmwf7dSYqA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69a740ce831c03980f22da38b69ef9e953969c60",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/jByxci1GM9Z6sNIN_KMwvKesJhqjDNSBcqmwf7dSYqA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0cd5c5b06703ce1128246e7f7cc2364c45a4eb7",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/jByxci1GM9Z6sNIN_KMwvKesJhqjDNSBcqmwf7dSYqA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea47eb31c4b5550c66fd514712d58bcf4ab85a51",
                    "width": 960,
                    "height": 640
                  }
                ],
                "variants": {},
                "id": "Z1q3kCzLCLjbLxAhQ1NkLwGmRvfU0JXMBAuKkzGJVhg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1rpym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "devrus123",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1rpym/polars_vs_duckdb/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1rpym/polars_vs_duckdb/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712872900.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I\u2019m a data engineer currently having difficulty persuading my workplace to give me access to developer tools as they conflict with our security policies. Scripting is blocked for all normal users, local admin accounts aren\u2019t allowed and with VS Code there are concerns with respect to extensions. How do other employers solve this? ",
          "author_fullname": "t2_vnrltsqf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Developer Tools in Work",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1och3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712864928.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a data engineer currently having difficulty persuading my workplace to give me access to developer tools as they conflict with our security policies. Scripting is blocked for all normal users, local admin accounts aren\u2019t allowed and with VS Code there are concerns with respect to extensions. How do other employers solve this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1och3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Salt-Major9311",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1och3/developer_tools_in_work/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1och3/developer_tools_in_work/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712864928.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "My company is in the market for a SaaS ingestion tool for our snowflake environment.\n\nWe\u2019ve looked at Fivetran, Rivery, and Stitch.\n\nNone of these have a native open edge connector for our progress database.\n\nDoes anyone know of any tool that has a native connector?",
          "author_fullname": "t2_136mtv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Data extraction tool",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1mcqu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712860122.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is in the market for a SaaS ingestion tool for our snowflake environment.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve looked at Fivetran, Rivery, and Stitch.&lt;/p&gt;\n\n&lt;p&gt;None of these have a native open edge connector for our progress database.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of any tool that has a native connector?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1mcqu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Crackerjack8",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1mcqu/data_extraction_tool/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1mcqu/data_extraction_tool/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712860122.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Our teams already use Snowflake extensively and also primarily work in Azure. Does anyone have any experience with Azure AI Search or Snowflake for RAG (that is, as vector stores to augment working with LLMs)? Ideally, someone out there has worked with both and can offer comparison but all experiences would be helpful. Thank you!",
          "author_fullname": "t2_ycsml",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any Experience with Azure AI Search or Snowflake for RAG?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1l16m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712856905.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our teams already use Snowflake extensively and also primarily work in Azure. Does anyone have any experience with Azure AI Search or Snowflake for RAG (that is, as vector stores to augment working with LLMs)? Ideally, someone out there has worked with both and can offer comparison but all experiences would be helpful. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1l16m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kevinpostlewaite",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1l16m/any_experience_with_azure_ai_search_or_snowflake/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1l16m/any_experience_with_azure_ai_search_or_snowflake/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712856905.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I have the Data Warehouse book by Kimball, I haven't read all of it. I like that it's fairly abstract (not lots of code) though it has left me wondering about how things look like 'in practice' a bit. \n\nFor example - I have a fact table with columns `time | location | sensor | value`, fine. For analytics purposes there are often a few joins to be made on `sensor` and / or `location` in order to get all the necessary data to filter on / group by as the tables are (fairly) normalised. This seems like a candidate for creating a \"star schema\" with in order to have an easier time analysis wise.\n\nIt might be silly - but something that's getting me a little bit confused is - if i have multiple fact tables that share THE SAME dimensions, should I store them in the same table? Or should I duplicate that dimension table so that it relates to a single fact table only? \n\nEg if I have: \n\n```\nfct_1 -&gt; dim_1\n      -&gt; dim_2\n\nfct_2 -&gt; dim_1\n      -&gt; dim_3\n```\n\nThen should `dim_1` be stored twice? Or should it be stored once? I don't care that much about storage space here, I'm more interested in common workflows and things making sense logically. \n\nAdditionally - in the database schema I would have: \n\n```\ndim_1\ndim_2\ndim_3\nfct_1\nfct_2\n```\nOr I could have: \n\n```\n&lt;schema&gt;.&lt;table&gt;\nsome_fct1.dim_1\nsome_fct1.dim_2\nsome_fct1.fct_1\n\nsome_fct2.dim_1\nsome_fct2.dim_3\nsome_fct2.fct_2\n```\n\nIs that clearer / more common? \n\nI'm not sure if some of what I've written indicates the type of response to give here, or if it's just a mess! Basically I'm interested in whether there are any common workflows/practices around naming/storage when converting a normalised structure into facts and dimensions for others to analyse.\n\nCheers",
          "author_fullname": "t2_tczfts4v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "data modelling - facts and dims - how much of the data is duplicated? How do you structure / store star schema?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1kin3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712855624.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have the Data Warehouse book by Kimball, I haven&amp;#39;t read all of it. I like that it&amp;#39;s fairly abstract (not lots of code) though it has left me wondering about how things look like &amp;#39;in practice&amp;#39; a bit. &lt;/p&gt;\n\n&lt;p&gt;For example - I have a fact table with columns &lt;code&gt;time | location | sensor | value&lt;/code&gt;, fine. For analytics purposes there are often a few joins to be made on &lt;code&gt;sensor&lt;/code&gt; and / or &lt;code&gt;location&lt;/code&gt; in order to get all the necessary data to filter on / group by as the tables are (fairly) normalised. This seems like a candidate for creating a &amp;quot;star schema&amp;quot; with in order to have an easier time analysis wise.&lt;/p&gt;\n\n&lt;p&gt;It might be silly - but something that&amp;#39;s getting me a little bit confused is - if i have multiple fact tables that share THE SAME dimensions, should I store them in the same table? Or should I duplicate that dimension table so that it relates to a single fact table only? &lt;/p&gt;\n\n&lt;p&gt;Eg if I have: &lt;/p&gt;\n\n&lt;p&gt;```\nfct_1 -&amp;gt; dim_1\n      -&amp;gt; dim_2&lt;/p&gt;\n\n&lt;p&gt;fct_2 -&amp;gt; dim_1\n      -&amp;gt; dim_3\n```&lt;/p&gt;\n\n&lt;p&gt;Then should &lt;code&gt;dim_1&lt;/code&gt; be stored twice? Or should it be stored once? I don&amp;#39;t care that much about storage space here, I&amp;#39;m more interested in common workflows and things making sense logically. &lt;/p&gt;\n\n&lt;p&gt;Additionally - in the database schema I would have: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\ndim_1\ndim_2\ndim_3\nfct_1\nfct_2\n&lt;/code&gt;\nOr I could have: &lt;/p&gt;\n\n&lt;p&gt;```\n&amp;lt;schema&amp;gt;.&amp;lt;table&amp;gt;\nsome_fct1.dim_1\nsome_fct1.dim_2\nsome_fct1.fct_1&lt;/p&gt;\n\n&lt;p&gt;some_fct2.dim_1\nsome_fct2.dim_3\nsome_fct2.fct_2\n```&lt;/p&gt;\n\n&lt;p&gt;Is that clearer / more common? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if some of what I&amp;#39;ve written indicates the type of response to give here, or if it&amp;#39;s just a mess! Basically I&amp;#39;m interested in whether there are any common workflows/practices around naming/storage when converting a normalised structure into facts and dimensions for others to analyse.&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1kin3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Subject_Fix2471",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1kin3/data_modelling_facts_and_dims_how_much_of_the/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1kin3/data_modelling_facts_and_dims_how_much_of_the/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712855624.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "",
          "author_fullname": "t2_ln5sx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[DEMO] LLMs, SQL, &amp; Reverse ETL",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 51,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1k92g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/ORtL40lNiDJfrKTp1VcbNPgxsS5QpK-iC52jft4sx98.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712854951.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "segment.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://segment.com/blog/introducing-segment-reverse-etl-with-snowflake-cortex/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Z1Amwv1T8rcKxtmGDxkIGS50L5SXZNTWk8oZDL53iqQ.jpg?auto=webp&amp;s=c6424a7b50925300cbc934240c0fa10e07847bcf",
                  "width": 1772,
                  "height": 656
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Z1Amwv1T8rcKxtmGDxkIGS50L5SXZNTWk8oZDL53iqQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1b4f0bbf140a3dcaacf935b83d2312b89c8d525",
                    "width": 108,
                    "height": 39
                  },
                  {
                    "url": "https://external-preview.redd.it/Z1Amwv1T8rcKxtmGDxkIGS50L5SXZNTWk8oZDL53iqQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f7941d1230e74e133b597ccf86501a63a4292ec",
                    "width": 216,
                    "height": 79
                  },
                  {
                    "url": "https://external-preview.redd.it/Z1Amwv1T8rcKxtmGDxkIGS50L5SXZNTWk8oZDL53iqQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ada414ee57e62df31f5dc276cee07cfa83dd92bf",
                    "width": 320,
                    "height": 118
                  },
                  {
                    "url": "https://external-preview.redd.it/Z1Amwv1T8rcKxtmGDxkIGS50L5SXZNTWk8oZDL53iqQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bf2aa7d35bff9be96dbef9a1ebdfbf6fa69ef31",
                    "width": 640,
                    "height": 236
                  },
                  {
                    "url": "https://external-preview.redd.it/Z1Amwv1T8rcKxtmGDxkIGS50L5SXZNTWk8oZDL53iqQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a86bd8e74cb99ead68bb5aa7948bd2febb9af5e",
                    "width": 960,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/Z1Amwv1T8rcKxtmGDxkIGS50L5SXZNTWk8oZDL53iqQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc501b769945736042497f63f5c8bc90d39ed926",
                    "width": 1080,
                    "height": 399
                  }
                ],
                "variants": {},
                "id": "yU2jbCKW_dyTDY0znKYES-ZQgLN8x-X94JdhaCwGNOY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c1k92g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "n2parko",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1k92g/demo_llms_sql_reverse_etl/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://segment.com/blog/introducing-segment-reverse-etl-with-snowflake-cortex/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712854951.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I have been working in this field for 3 years now, and I have this feeling that I have no clue what I am doing and where I stand. Are there any tests, or ways to figure out whether I know what I need to know for this Job?\n\nOne of the problems is that I have majored as a research master in applied statistics in psychology, so I constantly have the feeling that I might be missing fundamentals. Before starting my role as a data engineer, I have worked for 2 years as an R developer for my university and then followed this up with a two year traineeship in Data Science with the highlight of training a BERT classifier for a specialized categorization task. In the last 3 years I have single handedly taken a company from having zero automation to having a 1 TB SQL Server database that is being filled by dockerized Airflow (all on premise) and now I am porting the whole thing to AWS so I can make it cloud native.  I have read Clean Code, Grokking Algorithms and designing data intensive applications.. but I still feel like I am doing things \"wrong\", despite things pretty much working. Are there any people out there that have similar feelings and background? What can you do about it? \n\n ",
          "author_fullname": "t2_8r13fthn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ways to get rid of impostor syndrome",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1k5qd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712854731.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working in this field for 3 years now, and I have this feeling that I have no clue what I am doing and where I stand. Are there any tests, or ways to figure out whether I know what I need to know for this Job?&lt;/p&gt;\n\n&lt;p&gt;One of the problems is that I have majored as a research master in applied statistics in psychology, so I constantly have the feeling that I might be missing fundamentals. Before starting my role as a data engineer, I have worked for 2 years as an R developer for my university and then followed this up with a two year traineeship in Data Science with the highlight of training a BERT classifier for a specialized categorization task. In the last 3 years I have single handedly taken a company from having zero automation to having a 1 TB SQL Server database that is being filled by dockerized Airflow (all on premise) and now I am porting the whole thing to AWS so I can make it cloud native.  I have read Clean Code, Grokking Algorithms and designing data intensive applications.. but I still feel like I am doing things &amp;quot;wrong&amp;quot;, despite things pretty much working. Are there any people out there that have similar feelings and background? What can you do about it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1k5qd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FunBrilliant5712",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1k5qd/ways_to_get_rid_of_impostor_syndrome/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1k5qd/ways_to_get_rid_of_impostor_syndrome/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712854731.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi all, I have been working as a data analyst for a while now, and was tasked with creating a data model for my company. We were advised to go down the synapse route, and created a synapse environment for the whole thing.\n\nThe way that we had done this was using a medallion structure, bronze, silver and gold layout. First ingesting the data from the SQL server databases and putting them into our bronze layer on the storage blob account. This was relatively cheap, but we could not turn it into a parquet file due to certain limitations and had to do this as part of the bronze to silver layer in dataflows (This was pretty expensive), from there we used what we called the Gold Database where we created external tables on the silver layer and could create joins using views for reporting purposes. \n\nHonestly at the start I had no idea where I was going or what to do, as I was just a humble report builder, but it does work from start to finish and we can use it for reporting purposes.\n\nIt does seem now that people are using microsoft fabric lakehouse and we were looking and transitioning to this solution, or would it make sense to do any of the stuff we do in dataflows (such as making derived columns etc) in sql?\n\nAny advice greatly appreciated",
          "author_fullname": "t2_sihhnnlo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Creation of a data model using Synapse",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1iqar",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712851232.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I have been working as a data analyst for a while now, and was tasked with creating a data model for my company. We were advised to go down the synapse route, and created a synapse environment for the whole thing.&lt;/p&gt;\n\n&lt;p&gt;The way that we had done this was using a medallion structure, bronze, silver and gold layout. First ingesting the data from the SQL server databases and putting them into our bronze layer on the storage blob account. This was relatively cheap, but we could not turn it into a parquet file due to certain limitations and had to do this as part of the bronze to silver layer in dataflows (This was pretty expensive), from there we used what we called the Gold Database where we created external tables on the silver layer and could create joins using views for reporting purposes. &lt;/p&gt;\n\n&lt;p&gt;Honestly at the start I had no idea where I was going or what to do, as I was just a humble report builder, but it does work from start to finish and we can use it for reporting purposes.&lt;/p&gt;\n\n&lt;p&gt;It does seem now that people are using microsoft fabric lakehouse and we were looking and transitioning to this solution, or would it make sense to do any of the stuff we do in dataflows (such as making derived columns etc) in sql?&lt;/p&gt;\n\n&lt;p&gt;Any advice greatly appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1iqar",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mathlete7",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1iqar/creation_of_a_data_model_using_synapse/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1iqar/creation_of_a_data_model_using_synapse/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712851232.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "",
          "author_fullname": "t2_vk94wnpj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why are your Databricks jobs performances changing over time?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1iiug",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/2cErXs1LeAz_K3q7L3dhokuihe1i3NSMyQzlnJ3NOQo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712850714.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "synccomputing.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://synccomputing.com/?p=2134&amp;preview=true&amp;_thumbnail_id=2135",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UlBQsDgDjZY5fDDc3qRNrrLFn_DkGRbj5fHuYoJG-O0.jpg?auto=webp&amp;s=faf86706fd97a04d2653ddf1964e8be814c27e89",
                  "width": 2560,
                  "height": 1550
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UlBQsDgDjZY5fDDc3qRNrrLFn_DkGRbj5fHuYoJG-O0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9fe3d0b920debe83525f25628419ccc35bb64e67",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://external-preview.redd.it/UlBQsDgDjZY5fDDc3qRNrrLFn_DkGRbj5fHuYoJG-O0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=87b040021d931b15def847fd29739dfa810a2c48",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://external-preview.redd.it/UlBQsDgDjZY5fDDc3qRNrrLFn_DkGRbj5fHuYoJG-O0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=40dcdc23bf14d40895074738188e108f6a4fa251",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://external-preview.redd.it/UlBQsDgDjZY5fDDc3qRNrrLFn_DkGRbj5fHuYoJG-O0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09a42ba22c0c76a4be6fea6021bcb8dc216cd72e",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://external-preview.redd.it/UlBQsDgDjZY5fDDc3qRNrrLFn_DkGRbj5fHuYoJG-O0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=490530839e510212066c47e9409fe3a28ed59770",
                    "width": 960,
                    "height": 581
                  },
                  {
                    "url": "https://external-preview.redd.it/UlBQsDgDjZY5fDDc3qRNrrLFn_DkGRbj5fHuYoJG-O0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9cfb4b5ec209e643ae7f004dc5bb6828ec13de34",
                    "width": 1080,
                    "height": 653
                  }
                ],
                "variants": {},
                "id": "xnbg_YxAfvllUmFceyVlXNTm2KDP-UrwVkmeqTdpmHY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c1iiug",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sync_jeff",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1iiug/why_are_your_databricks_jobs_performances/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://synccomputing.com/?p=2134&amp;preview=true&amp;_thumbnail_id=2135",
          "subreddit_subscribers": 176091,
          "created_utc": 1712850714.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I recently encountered a concerning billing situation with my Azure subscription and would greatly appreciate some advice and insight from the community.Background: I initially used the Azure free trial and later upgraded to pay-as-you-go to access additional services beyond the trial period. During this transition, I had minimal resources deployed, including one resource group with a Data Factory comprising two pipelines and a single database.Issue: After not checking my Azure account for four days, I was shocked to discover that I had been charged approximately 75,000 INR over that period. The bulk of the charges, around 74,000 INR, were attributed to the Data Factory service.Action Taken: Given the unexpected and exorbitant charges, I've promptly deleted the subscription. Moreover, the payment method associated with the subscription is a debit card that holds no significance to me, and I'm prepared to close the account if necessary.Request for Assistance: As an Azure user from India, I'm seeking guidance on several fronts. Firstly, I'm keen to understand the reasons behind such substantial charges incurred within a short timeframe, particularly within the Data Factory service. Additionally, I'm uncertain about the appropriate steps to take to address this billing issue and prevent similar occurrences in the future.Any insights, advice, or recommendations would be immensely helpful as I navigate this situation.",
          "author_fullname": "t2_66o9yyui",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need urgent help - Azure billing",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1h4im",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712847303.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently encountered a concerning billing situation with my Azure subscription and would greatly appreciate some advice and insight from the community.Background: I initially used the Azure free trial and later upgraded to pay-as-you-go to access additional services beyond the trial period. During this transition, I had minimal resources deployed, including one resource group with a Data Factory comprising two pipelines and a single database.Issue: After not checking my Azure account for four days, I was shocked to discover that I had been charged approximately 75,000 INR over that period. The bulk of the charges, around 74,000 INR, were attributed to the Data Factory service.Action Taken: Given the unexpected and exorbitant charges, I&amp;#39;ve promptly deleted the subscription. Moreover, the payment method associated with the subscription is a debit card that holds no significance to me, and I&amp;#39;m prepared to close the account if necessary.Request for Assistance: As an Azure user from India, I&amp;#39;m seeking guidance on several fronts. Firstly, I&amp;#39;m keen to understand the reasons behind such substantial charges incurred within a short timeframe, particularly within the Data Factory service. Additionally, I&amp;#39;m uncertain about the appropriate steps to take to address this billing issue and prevent similar occurrences in the future.Any insights, advice, or recommendations would be immensely helpful as I navigate this situation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1h4im",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Siddboss195803",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1h4im/need_urgent_help_azure_billing/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1h4im/need_urgent_help_azure_billing/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712847303.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I studied computer engineering. In my last year i studied .net and react because most of the jobs are web dev jobs and these two tech most popular tech in my country. But web dev doesnt interest me anymore. Especially frontend-react. I almost hate it.  Also, since there are two different areas, I could not go deep enough in either of them. And even though it's been 3 months since I graduated, I still haven't gotten a job. Still no job. The other think i interest is working with data. The most i like data engineering but im ok with data analysis, data science. Im thinking to give up web dev and give a chance to data engineering, maybe some data analysis. Kinda more job in my country. The maximum pay i can go is 75$. So datacamp. Ive read some good and bad comments about it. I know python and sql, db design from university. Is datacamp can help to learn de, and get a job? Is it good idea to give up fullstack web dev and start from 0 when already 3 month jobless. Any alternative to datacamp, cheaper and better?",
          "author_fullname": "t2_vrf5ex5r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "from .net+react to data engineering, datacamp and other paid courses",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1ga82",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712845221.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I studied computer engineering. In my last year i studied .net and react because most of the jobs are web dev jobs and these two tech most popular tech in my country. But web dev doesnt interest me anymore. Especially frontend-react. I almost hate it.  Also, since there are two different areas, I could not go deep enough in either of them. And even though it&amp;#39;s been 3 months since I graduated, I still haven&amp;#39;t gotten a job. Still no job. The other think i interest is working with data. The most i like data engineering but im ok with data analysis, data science. Im thinking to give up web dev and give a chance to data engineering, maybe some data analysis. Kinda more job in my country. The maximum pay i can go is 75$. So datacamp. Ive read some good and bad comments about it. I know python and sql, db design from university. Is datacamp can help to learn de, and get a job? Is it good idea to give up fullstack web dev and start from 0 when already 3 month jobless. Any alternative to datacamp, cheaper and better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c1ga82",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tricky-War3608",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1ga82/from_netreact_to_data_engineering_datacamp_and/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1ga82/from_netreact_to_data_engineering_datacamp_and/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712845221.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Having recently seeing a lot of newer companies moving towards building their transformations using Dbt or LookML . I couldn\u2019t find much about it . They are using either snowflake or big query as the data warehouse ",
          "author_fullname": "t2_jm8x8jsx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Difference between LookML and Dbt?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1g48u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712844801.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Having recently seeing a lot of newer companies moving towards building their transformations using Dbt or LookML . I couldn\u2019t find much about it . They are using either snowflake or big query as the data warehouse &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1g48u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sydneysweeney69",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1g48u/difference_between_lookml_and_dbt/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1g48u/difference_between_lookml_and_dbt/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712844801.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "for 99% of you, that's because of building things and making them reliable, but what is the 2nd most important factor? in your current professional situation \n\n[View Poll](https://www.reddit.com/poll/1c1fzsm)",
          "author_fullname": "t2_c0yysfvj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Data Engineering is better than DS/DA because ...",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1fzsm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712844501.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;for 99% of you, that&amp;#39;s because of building things and making them reliable, but what is the 2nd most important factor? in your current professional situation &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1c1fzsm\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1fzsm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FaithlessnessNorth65",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "poll_data": {
            "prediction_status": null,
            "total_stake_amount": null,
            "voting_end_timestamp": 1713103701724,
            "options": [
              {
                "text": "NO interactions with clients",
                "id": "27718239"
              },
              {
                "text": "endless learning of tools and practical aspects",
                "id": "27718240"
              },
              {
                "text": "fewer meetings",
                "id": "27718241"
              },
              {
                "text": "peace and more time",
                "id": "27718242"
              },
              {
                "text": "something else...",
                "id": "27718243"
              }
            ],
            "vote_updates_remained": null,
            "is_prediction": false,
            "resolved_option_id": null,
            "user_won_amount": null,
            "user_selection": null,
            "total_vote_count": 152,
            "tournament_id": null
          },
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1fzsm/data_engineering_is_better_than_dsda_because/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "mod_reports": [],
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1fzsm/data_engineering_is_better_than_dsda_because/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712844501.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Snowflake recently delivered support for Iceberg format, but they also have these new types of tables called Hybrid. I read about it and to me it seems that both solutions are intended for the same use case, which is ACID support + capability of doing analytics in the same table. Am I missing something?\n",
          "author_fullname": "t2_s9ka2uyhs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Snowflake: Apache Iceberg vs. Hybrid Tables",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1fyxj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712844441.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Snowflake recently delivered support for Iceberg format, but they also have these new types of tables called Hybrid. I read about it and to me it seems that both solutions are intended for the same use case, which is ACID support + capability of doing analytics in the same table. Am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1fyxj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Corn_OrangeCat",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1fyxj/snowflake_apache_iceberg_vs_hybrid_tables/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1fyxj/snowflake_apache_iceberg_vs_hybrid_tables/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712844441.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I made a transition and a data analyst job 6 months back earlier I was a mechanical engineer(quality engineer) in a small company here in india.I majorly work with excel,google sheets and big query(sql) I am interested in data engineering and I want to know from you people what should I learn next and please also recommend some basic to advance projects ideas as well.",
          "author_fullname": "t2_6ybces6n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I know good sql amd basic python what to learn next to move towars data engineering",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1eifq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712840564.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a transition and a data analyst job 6 months back earlier I was a mechanical engineer(quality engineer) in a small company here in india.I majorly work with excel,google sheets and big query(sql) I am interested in data engineering and I want to know from you people what should I learn next and please also recommend some basic to advance projects ideas as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1eifq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Narrow-Tea-9187",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1eifq/i_know_good_sql_amd_basic_python_what_to_learn/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1eifq/i_know_good_sql_amd_basic_python_what_to_learn/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712840564.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hello!\n\nFirst of all thanks to you, thanks to this community I am gradually moving forward on my free time to improve my understanding and my data engineering skills.\n\n&amp;#x200B;\n\nThere is my problem:\n\nMy company wants me to create a data warehouse project from scratch, i'm a 6month junior DA.\n\nIt\u2019s a marketing company, we have a dozen different marketing data sources. Initially we worked with a direct connector tool to looker studio, however dashboards became slow when I crossed data from several sources in looker studio.\n\nSo I thought of a solution, to go through Bigquery for the data warehouse with Fivetran as an ETL and always looker studio for the dataviz. Is this a good setup ?\n\n&amp;#x200B;\n\nMoreover i would like advice or links to documentation or courses on how to optimize my process so that it costs as little as possible. Do you have ideas for best practices for this kind of project?\n\n&amp;#x200B;\n\nCurrently I am doing tests for one of our customers with 4 marketing sources, I sorted as much as possible unnecessary tables and columns in relation to reporting needs. What more can I do? Whether in my ETL or in BQ?\n\n&amp;#x200B;\n\nThank you in advance! And I hope to exchange with you in the near future on more interesting projects than this one.\n\n&amp;#x200B;",
          "author_fullname": "t2_a8t1zh7y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Data warehouse project for a newbie",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1eh6h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712840475.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;First of all thanks to you, thanks to this community I am gradually moving forward on my free time to improve my understanding and my data engineering skills.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;There is my problem:&lt;/p&gt;\n\n&lt;p&gt;My company wants me to create a data warehouse project from scratch, i&amp;#39;m a 6month junior DA.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s a marketing company, we have a dozen different marketing data sources. Initially we worked with a direct connector tool to looker studio, however dashboards became slow when I crossed data from several sources in looker studio.&lt;/p&gt;\n\n&lt;p&gt;So I thought of a solution, to go through Bigquery for the data warehouse with Fivetran as an ETL and always looker studio for the dataviz. Is this a good setup ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Moreover i would like advice or links to documentation or courses on how to optimize my process so that it costs as little as possible. Do you have ideas for best practices for this kind of project?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Currently I am doing tests for one of our customers with 4 marketing sources, I sorted as much as possible unnecessary tables and columns in relation to reporting needs. What more can I do? Whether in my ETL or in BQ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance! And I hope to exchange with you in the near future on more interesting projects than this one.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1eh6h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fair-Celery4044",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1eh6h/data_warehouse_project_for_a_newbie/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1eh6h/data_warehouse_project_for_a_newbie/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712840475.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I work for a startup and we're very short-staffed, with little engineering expertise. I'm essentially Director, Tech Lead, Architect, and Senior Dev all in one. Workload is reasonable right now so it's holding up, but I digress...\n\nWe've got a lot of data provided in csv format - in the 100's of GB right now, but grows in the 10's of GB every month. There are 7 files/tables and we get a monthly dump for each. That's 12 large files per table, every year, and goes back to 2018. \n\nThe first thing I was tasked with, was making this query-able and useful for some basic market research. Most common use case is rolling analysis of the past few months, but historical comparisons are common. For consumers, it's read-only data. There aren't any \"real-time-ish\" requirements around availability of new data. We'll build basic reports and perhaps use QuickSight for visualization.\n\nSo...I built a simple process to ingest the files, validate the data, and batch insert them into pg in RDS. It works well and I've proven the concept, but obviously it's unwise to continue to dump all of this data into a single untuned pg instance forever. \n\nI'm not specifically a \"data engineer\", but have been building large-scale web apps and platforms for a long time now, so data engineering isn't new to me, per se. I figured it'd be wise to get some community feedback and ideas on how best to proceed.\n\nMy first thought was to partition the data by year and create a separate schema and tables as such. Again, this needs to be as simple and under-engineered as possible as we don't have the resources to go wild with it. Redshift is way too expensive. Another thought was the S3 + Athena approach, but I've never worked with this and would need to do some research. I don't doubt that it's cheaper and scales better...but is it as capable and fast? I like SQL and it's easier to hire for it in the future. \n\nI'm also familiar w/ big data stores like Cassandra and Dynamo. Dynamo is out of the question on costs and complexity. Can't see that fitting here. There seems to be too much ad-hoc querying and evolving requirements to know access patterns up front, anyhow...so noSQL in general may not be a great fit. \n\nWe're sticking w/ AWS for now, so not looking for other cloud solutions. It's very sensitive data and we're under great privacy scrutiny, so keeping it within the AWS fence is preferred.\n\nThoughts are appreciated!",
          "author_fullname": "t2_do270eaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Some tips for Postgres (RDS) for a simple data warehouse",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1e1o7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712839245.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a startup and we&amp;#39;re very short-staffed, with little engineering expertise. I&amp;#39;m essentially Director, Tech Lead, Architect, and Senior Dev all in one. Workload is reasonable right now so it&amp;#39;s holding up, but I digress...&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve got a lot of data provided in csv format - in the 100&amp;#39;s of GB right now, but grows in the 10&amp;#39;s of GB every month. There are 7 files/tables and we get a monthly dump for each. That&amp;#39;s 12 large files per table, every year, and goes back to 2018. &lt;/p&gt;\n\n&lt;p&gt;The first thing I was tasked with, was making this query-able and useful for some basic market research. Most common use case is rolling analysis of the past few months, but historical comparisons are common. For consumers, it&amp;#39;s read-only data. There aren&amp;#39;t any &amp;quot;real-time-ish&amp;quot; requirements around availability of new data. We&amp;#39;ll build basic reports and perhaps use QuickSight for visualization.&lt;/p&gt;\n\n&lt;p&gt;So...I built a simple process to ingest the files, validate the data, and batch insert them into pg in RDS. It works well and I&amp;#39;ve proven the concept, but obviously it&amp;#39;s unwise to continue to dump all of this data into a single untuned pg instance forever. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not specifically a &amp;quot;data engineer&amp;quot;, but have been building large-scale web apps and platforms for a long time now, so data engineering isn&amp;#39;t new to me, per se. I figured it&amp;#39;d be wise to get some community feedback and ideas on how best to proceed.&lt;/p&gt;\n\n&lt;p&gt;My first thought was to partition the data by year and create a separate schema and tables as such. Again, this needs to be as simple and under-engineered as possible as we don&amp;#39;t have the resources to go wild with it. Redshift is way too expensive. Another thought was the S3 + Athena approach, but I&amp;#39;ve never worked with this and would need to do some research. I don&amp;#39;t doubt that it&amp;#39;s cheaper and scales better...but is it as capable and fast? I like SQL and it&amp;#39;s easier to hire for it in the future. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also familiar w/ big data stores like Cassandra and Dynamo. Dynamo is out of the question on costs and complexity. Can&amp;#39;t see that fitting here. There seems to be too much ad-hoc querying and evolving requirements to know access patterns up front, anyhow...so noSQL in general may not be a great fit. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re sticking w/ AWS for now, so not looking for other cloud solutions. It&amp;#39;s very sensitive data and we&amp;#39;re under great privacy scrutiny, so keeping it within the AWS fence is preferred.&lt;/p&gt;\n\n&lt;p&gt;Thoughts are appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1e1o7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zambizzi",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1e1o7/some_tips_for_postgres_rds_for_a_simple_data/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1e1o7/some_tips_for_postgres_rds_for_a_simple_data/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712839245.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi all, wanted get some advice on office soft skills.\n\nBackground: I was hired as a contractor several months in a big company. High monthly salary, colleagues are nice and fun, great WLB, job is pretty easy (building data pipelines in Python and Airflow - sometimes design backend database for an internal product)\n\nCons: this is my first contractor job (hired through an agency). I feel very passive as I was given task by task to do. My manager never involves me in product design meeting, or never tells me about what\u2019s the project plan or what\u2019s other related projects are working on. Thus, I always feel that my designs are short-sighted and this doesnt give me a good satisfaction at work. Also, I feel that my skill level is going down the job is not challenging. \nI do wish I could be involved more - but quite scared if it is appropriate to do so as a contractor with limited role. \n\nShould I ask my manager to give me more work and share with me more about the project vision? And how should I approach him that I am capable of doing more? \n\n",
          "author_fullname": "t2_mw8v1urx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to ask to be involved in more work, as a contractor",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1crqh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712835186.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, wanted get some advice on office soft skills.&lt;/p&gt;\n\n&lt;p&gt;Background: I was hired as a contractor several months in a big company. High monthly salary, colleagues are nice and fun, great WLB, job is pretty easy (building data pipelines in Python and Airflow - sometimes design backend database for an internal product)&lt;/p&gt;\n\n&lt;p&gt;Cons: this is my first contractor job (hired through an agency). I feel very passive as I was given task by task to do. My manager never involves me in product design meeting, or never tells me about what\u2019s the project plan or what\u2019s other related projects are working on. Thus, I always feel that my designs are short-sighted and this doesnt give me a good satisfaction at work. Also, I feel that my skill level is going down the job is not challenging. \nI do wish I could be involved more - but quite scared if it is appropriate to do so as a contractor with limited role. &lt;/p&gt;\n\n&lt;p&gt;Should I ask my manager to give me more work and share with me more about the project vision? And how should I approach him that I am capable of doing more? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c1crqh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Laidbackwoman",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1crqh/how_to_ask_to_be_involved_in_more_work_as_a/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1crqh/how_to_ask_to_be_involved_in_more_work_as_a/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712835186.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "",
          "author_fullname": "t2_rr6r6b8v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Common DE pipelines and their tech stacks on AWS, GCP and Azure",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1cbfg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 363,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 363,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/JdYdGtVQVnSftADJ9S_TGwu1VB1GP_7EZmUyxt6_G0k.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712833615.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/1t2lrh8q1utc1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/1t2lrh8q1utc1.jpeg?auto=webp&amp;s=e7c6287896a3746fbeab7ae4c03df67247f207d7",
                  "width": 1290,
                  "height": 2796
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/1t2lrh8q1utc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2831aa2411dbb9d45189664b26e6d3c42bf3df1",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/1t2lrh8q1utc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=584c5866fb706ca519f68aa0b70be5d3021c9d27",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/1t2lrh8q1utc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6898fe1d07d16dfb2a4ebf122a4771dd712f9d6b",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/1t2lrh8q1utc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e242c870557abd947e849494dd559877940cff4",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/1t2lrh8q1utc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=94ca14b5a587d9927f5b1918c0ccdd6e43ccb899",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/1t2lrh8q1utc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d432f8e4311640030e220683449e3b3a6a2c68c",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "rIDU3VSkhr77he5QpUdlZNXuyx1uie85DgMSvzYg-48"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1cbfg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_areebpasha",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1cbfg/common_de_pipelines_and_their_tech_stacks_on_aws/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://i.redd.it/1t2lrh8q1utc1.jpeg",
          "subreddit_subscribers": 176091,
          "created_utc": 1712833615.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Edit: thank you everyone for the discussion and the suggestions!\n\nWe want to collect all kinds of information from the tables in the `hive_metastore` to generate metrics for future optimizations.\n \nThe `hive_metastore` contains hundreds of schemas and a total of ~150K tables.\n \nIs there an efficient/parallelizable way to e.g. run a `spark.sql(\"SHOW TABLES in {schema}).collect()` for every schema and then run commands like `spark.sql(\"DESCRIBE EXTENDED {table_name}\").collect()` for each table?\n \nThe question is intended preferably for usage with PySpark, but in case nothing is possible with the Python API, then we can use Scala.\n \nLet me know if this is enough information on the case.\n \nThanks a lot in advance!",
          "author_fullname": "t2_2wp6hrkb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Spark (Databricks): how to efficiently DESCRIBE thousands of tables?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1c39b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1712907247.0,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712832820.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit: thank you everyone for the discussion and the suggestions!&lt;/p&gt;\n\n&lt;p&gt;We want to collect all kinds of information from the tables in the &lt;code&gt;hive_metastore&lt;/code&gt; to generate metrics for future optimizations.&lt;/p&gt;\n\n&lt;p&gt;The &lt;code&gt;hive_metastore&lt;/code&gt; contains hundreds of schemas and a total of ~150K tables.&lt;/p&gt;\n\n&lt;p&gt;Is there an efficient/parallelizable way to e.g. run a &lt;code&gt;spark.sql(&amp;quot;SHOW TABLES in {schema}).collect()&lt;/code&gt; for every schema and then run commands like &lt;code&gt;spark.sql(&amp;quot;DESCRIBE EXTENDED {table_name}&amp;quot;).collect()&lt;/code&gt; for each table?&lt;/p&gt;\n\n&lt;p&gt;The question is intended preferably for usage with PySpark, but in case nothing is possible with the Python API, then we can use Scala.&lt;/p&gt;\n\n&lt;p&gt;Let me know if this is enough information on the case.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1c39b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cyberZamp",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1c39b/spark_databricks_how_to_efficiently_describe/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1c39b/spark_databricks_how_to_efficiently_describe/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712832820.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi All,\n\nProduct Director here at Conduktor - wanted to share news that we recently opened up our **free tier** to make it accessible to everyone regardless of your data streaming experience.\n\nTL;DR What is Conduktor? A collaborative Kafka development platform that connects Developers, Architects and Platform teams working in the Kafka ecosystem. \n\nFew taster features:\n\n* Drill deep into topic data (JSON, Avro, Protobuf, custom SerDes)\n* Live consumer\n* Embedded monitoring and alerting (consumer lag, topic msg in/out etc.)\n* Kafka Connect auto-restart\n* Dead Letter Queue (DLQ) management\n* CLI + APIs for automation + GitOps\n* E2E Encryption through our Kafka proxy\n* Complete RBAC model (topics, subjects, consumer groups, connectors etc.)\n\nYou can read the full announcement here and getting started here:\n\n[https://v2.conduktor.io/](https://v2.conduktor.io/)\n\nAny questions - feel free to shoot :) ",
          "author_fullname": "t2_ve0spvbx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Conduktor 2.0 - Free Tier Expansion - Collaborative Kafka Development Platform",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1bigb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712830612.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;Product Director here at Conduktor - wanted to share news that we recently opened up our &lt;strong&gt;free tier&lt;/strong&gt; to make it accessible to everyone regardless of your data streaming experience.&lt;/p&gt;\n\n&lt;p&gt;TL;DR What is Conduktor? A collaborative Kafka development platform that connects Developers, Architects and Platform teams working in the Kafka ecosystem. &lt;/p&gt;\n\n&lt;p&gt;Few taster features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Drill deep into topic data (JSON, Avro, Protobuf, custom SerDes)&lt;/li&gt;\n&lt;li&gt;Live consumer&lt;/li&gt;\n&lt;li&gt;Embedded monitoring and alerting (consumer lag, topic msg in/out etc.)&lt;/li&gt;\n&lt;li&gt;Kafka Connect auto-restart&lt;/li&gt;\n&lt;li&gt;Dead Letter Queue (DLQ) management&lt;/li&gt;\n&lt;li&gt;CLI + APIs for automation + GitOps&lt;/li&gt;\n&lt;li&gt;E2E Encryption through our Kafka proxy&lt;/li&gt;\n&lt;li&gt;Complete RBAC model (topics, subjects, consumer groups, connectors etc.)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can read the full announcement here and getting started here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://v2.conduktor.io/\"&gt;https://v2.conduktor.io/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any questions - feel free to shoot :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/teXNnud3l7H1bzkC7pxpBCk3REFVaMe5qnKptPtTIAw.jpg?auto=webp&amp;s=e476fc4733ca4a0ec2391fc4274beed20c880637",
                  "width": 1944,
                  "height": 1104
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/teXNnud3l7H1bzkC7pxpBCk3REFVaMe5qnKptPtTIAw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=978006fb0400c30af5efe5deca1179a2477f2b7a",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://external-preview.redd.it/teXNnud3l7H1bzkC7pxpBCk3REFVaMe5qnKptPtTIAw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86ca9a1353eee5befcc0367e6a28d896687c7da7",
                    "width": 216,
                    "height": 122
                  },
                  {
                    "url": "https://external-preview.redd.it/teXNnud3l7H1bzkC7pxpBCk3REFVaMe5qnKptPtTIAw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=af2b2e3592680ee01d754d963a4334cec1efbd6b",
                    "width": 320,
                    "height": 181
                  },
                  {
                    "url": "https://external-preview.redd.it/teXNnud3l7H1bzkC7pxpBCk3REFVaMe5qnKptPtTIAw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d451d9c9565ee21babe8ef92bedbc5bd312623d3",
                    "width": 640,
                    "height": 363
                  },
                  {
                    "url": "https://external-preview.redd.it/teXNnud3l7H1bzkC7pxpBCk3REFVaMe5qnKptPtTIAw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb3e77f4c725c6da779e22e0aa8bf229eae92495",
                    "width": 960,
                    "height": 545
                  },
                  {
                    "url": "https://external-preview.redd.it/teXNnud3l7H1bzkC7pxpBCk3REFVaMe5qnKptPtTIAw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e5ceac84f8463a3c0f33a0ec7f388b21957cc4f",
                    "width": 1080,
                    "height": 613
                  }
                ],
                "variants": {},
                "id": "t9pbhQoDIApQqK-ubGmVi5qWOl3rBo3XhWI67CYxxgY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c1bigb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "data-stash",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1bigb/conduktor_20_free_tier_expansion_collaborative/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1bigb/conduktor_20_free_tier_expansion_collaborative/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712830612.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "",
          "author_fullname": "t2_gxesw7ji",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Bytebase 2.15.0 - brand new GitOps for database schema change",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1b4xv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://b.thumbs.redditmedia.com/a_bt0SQ-sxKlz2iiUbKGhHDYG_9E1WCsWwpiQfs_w8k.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712829177.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "bytebase.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.bytebase.com/changelog/bytebase-2-15-0/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1OJffjqJrs50uh28Gw8wUPK84jHsS34uMgtkOinV5ZY.jpg?auto=webp&amp;s=18a5cf943cca6789fd1ef65b0e9383da79dff2f6",
                  "width": 1600,
                  "height": 900
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1OJffjqJrs50uh28Gw8wUPK84jHsS34uMgtkOinV5ZY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=feab6482092efc236e186c8b8943d3dd200b379a",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/1OJffjqJrs50uh28Gw8wUPK84jHsS34uMgtkOinV5ZY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ca532ec321a05259e05a46eb252bc16166af376b",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/1OJffjqJrs50uh28Gw8wUPK84jHsS34uMgtkOinV5ZY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1742417093b314bd3d92117306347ce42981248",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/1OJffjqJrs50uh28Gw8wUPK84jHsS34uMgtkOinV5ZY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=05750193a2157242edd1d3e70c830312df1cd6ae",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/1OJffjqJrs50uh28Gw8wUPK84jHsS34uMgtkOinV5ZY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9871be30ea244857c5002e34bea77709907409bf",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/1OJffjqJrs50uh28Gw8wUPK84jHsS34uMgtkOinV5ZY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d9f80f9d7c5ac8dc3f5a62d2e00c9a167925ab5",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ID1jwyfW95XYNRo3t1bZM_TB_BLc1IybFNjPZPgoq7k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c1b4xv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Adela_freedom",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1b4xv/bytebase_2150_brand_new_gitops_for_database/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.bytebase.com/changelog/bytebase-2-15-0/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712829177.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "As we all know, AI and generative AI are ubiquitous. I often come across numerous new ideas and side projects utilizing generative AI. I\u2019m wondering, as a data engineer, how we can contribute to and take advantage of this trend, supporting data scientists as a side project.\n\nPS: I\u2019m looking for building something, not using generative ai ",
          "author_fullname": "t2_ci308gob",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How Can Data Engineers Harness Generative AI for Innovation?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1an72",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.48,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712827079.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As we all know, AI and generative AI are ubiquitous. I often come across numerous new ideas and side projects utilizing generative AI. I\u2019m wondering, as a data engineer, how we can contribute to and take advantage of this trend, supporting data scientists as a side project.&lt;/p&gt;\n\n&lt;p&gt;PS: I\u2019m looking for building something, not using generative ai &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Data Engineer",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c1an72",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Delicious_Attempt_99",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/dataengineering/comments/1c1an72/how_can_data_engineers_harness_generative_ai_for/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1an72/how_can_data_engineers_harness_generative_ai_for/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712827079.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi,\n\nI am using spark.sql in Scala to perform transformations on the hive table. I am creating temporary views of spark.sql statements and then using those in subsequent spark.sql statements.\n\nHow can I optimize this?\n\nSuppose there are multiple spark.sql statements using the result from a previous spark.sql statement, will spark automatically optimize this? Or will it recompute the results twice?\n\nHere is dummy code:  \nval df1 = spark.sql(\"select \\* from some\\_table\")\n\ndf1.createOrReplaceTempView(\"df1\\_res\")\n\nval df2 = spark.sql(\"select \\* from df1\\_res\")  \nval df3 = spark.sql(\"select \\* from df1\\_res\")\n\ndf2.collect()\n\ndf3.collect()\n\nWill df1 be computed twice?\n\nOr are there any other suggestions to improve job performance.\n\n&amp;#x200B;",
          "author_fullname": "t2_4wpyzwqh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Doubts in multiple spark.sql statements usage in spark scala",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c188uj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712817267.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am using spark.sql in Scala to perform transformations on the hive table. I am creating temporary views of spark.sql statements and then using those in subsequent spark.sql statements.&lt;/p&gt;\n\n&lt;p&gt;How can I optimize this?&lt;/p&gt;\n\n&lt;p&gt;Suppose there are multiple spark.sql statements using the result from a previous spark.sql statement, will spark automatically optimize this? Or will it recompute the results twice?&lt;/p&gt;\n\n&lt;p&gt;Here is dummy code:&lt;br/&gt;\nval df1 = spark.sql(&amp;quot;select * from some_table&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;df1.createOrReplaceTempView(&amp;quot;df1_res&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;val df2 = spark.sql(&amp;quot;select * from df1_res&amp;quot;)&lt;br/&gt;\nval df3 = spark.sql(&amp;quot;select * from df1_res&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;df2.collect()&lt;/p&gt;\n\n&lt;p&gt;df3.collect()&lt;/p&gt;\n\n&lt;p&gt;Will df1 be computed twice?&lt;/p&gt;\n\n&lt;p&gt;Or are there any other suggestions to improve job performance.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c188uj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Varun_123",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c188uj/doubts_in_multiple_sparksql_statements_usage_in/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c188uj/doubts_in_multiple_sparksql_statements_usage_in/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712817267.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi Everyone,\n\nI wanted to gain an insight into food industry working as a data engineer. What does your day to day look like, common tools you use and mainly what type of data do you work with. Any specific trends you see in this industry? \n\nI see a lot of data engineers in Insurance, Finance, Healthcare but wanted to see what its like in food industry. \n\n&amp;#x200B;\n\nThank you",
          "author_fullname": "t2_7mwddmfl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any Data-engineers in Food industry?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c140s0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712802985.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt;\n\n&lt;p&gt;I wanted to gain an insight into food industry working as a data engineer. What does your day to day look like, common tools you use and mainly what type of data do you work with. Any specific trends you see in this industry? &lt;/p&gt;\n\n&lt;p&gt;I see a lot of data engineers in Insurance, Finance, Healthcare but wanted to see what its like in food industry. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c140s0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Any-Insurance4148",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c140s0/any_dataengineers_in_food_industry/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c140s0/any_dataengineers_in_food_industry/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712802985.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": " I wrote this wall of text at 3AM, I'm sorry if there are any grammatical or syntactic errors\n\n**TLDR: I've been working for 2 years in a foreign country and I feel like I want to quit and go back home but what I think it's an imposter syndrome that won't let me abandon my job security. I shared my short career journey in this post to seek advice based on it.**\n\nAny life, career, reality check or psychological advice is welcomed.\n\n&amp;#x200B;\n\nBig story time for context of my career path as of now. \n\nItaly, 2022, I graduated from a highschool that had most of the subjects focused on programming (\"Istituto Tecnico\"). From this, I managed to get a diploma and learned my bases in programming (Java and Python OOP, concurrent programming, data structures like trees and lists, sockets and client-server applications). During my graduation year, I also worked part-time after school as a front-end developer at a very small local consulting company. \n\nFinished highschool I wanted to continue working at the same company, so I started talking directly with the owner of the company (there was no HR department) if there was a possibility of continuing with a new normal full-time contract. They ended up offering me an apprenticeship of at least 6 months, 40h weekly hours with a salary of 800\u20ac a month (sadly this is something \"normal\" in Italy for young people). In my mind, I said \"Fuck no, I'm better than this\" so I accepted the offer but in the meanwhile started applying for jobs around Europe via Linkedin, confident enough of my skills and my English proficiency. After sending something like 5 total applications I found a junior Java developer offer (21k yearly) to relocate to Spain and work for a multinational American company, starting with a 3 month paid in-person bootcamp consisting of basic level programming company courses and after that 6-month probation contract, at the end of those 6 months the contract would have become automatically of indefinite duration. I managed to get the interviews, pass the technical test and technical interviews and sign the contract.\n\nNow imagine the confidence boost that I got at the time by managing to land a position at a big company as a 20 year old fresh out of highschool and managing to leave my mother's house and finally becoming independent, starting to build my own life.\n\nFast forward, after the bootcamp somehow, based on my performances and \"luck\", I got assigned to a data engineering team that uses as a main language Scala with Spark and a tech stack that I would be seeing for the first time (Hadoop, Neo4j, Airflow, ETL processes and pipelines...). Even tho the initial job offer on Linkedin was for a Java dev, I didn't complain, mainly because I was eager to learn something new and start specializing in a field. \n\nFast forward again almost 2 years, it's 2024, I'm 22 and almost on the verge of a mental breakdown. Shortly I realized that independence is not flowers and rainbows and I started feeling lonely as fuck, starting to fall into depression.\n\nTo resolve this problem I was thinking of moving back to Italy, back to my family, my close friends and where there is someone that cares about me. I would need to find a new job and I would want to continue pursuing data engineering. I would have to leave everything behind, basically starting from scratch again, which psychologically terrorizes me because I feel like I don't have the actual work experience and knowledge of what a junior Data Engineer is supposed to have and I get anxious only by thinking of doing job interviews.\n\nI never got the opportunity to write a real spark application from scratch (I did some basic ones on my own but they are nowhere near as complex as real-world ones), everything that I did until now was maintenance and small code changes for the many company projects under my squad's ownership. At this point, I know how to run the full ETL processes autonomously without the help of a senior behind me but I still struggle to understand the full workflow and how it works, why it works and the architectural logic behind it. Also, our tech stack is so vast that I have some basic experience with DEV/OPS stuff and Jenkins CI/CD pipelines but again, if you would ask me to write down the whole CI/CD Jenkinsfiles from scratch I feel like I wouldn't be able to do it.\n\nThe team and manager are happy with my performance, in these 2 years thanks to a \"company internal performance programme\" I managed to get every 6 months small salary rises, with the projection of arriving at 30k yearly at the end of this year.\n\nI used to be confident in myself and now, I don't know if I have some kind of imposter syndrome that reached a point where I feel like I would not even be able to find another job in the data engineering field because maybe I'm not good enough for it.\n\nI'm afraid of abandoning my job security thinking that maybe it's true, maybe I will not be able to find something better and I torment myself with enduring this unhappy life in a place that I don't want to live anymore without anyone around me.\n\nBefore anyone tells me \"You just need to go outside and start making some friends\" or \"Go to the gym\": \n\n* I mainly go outside for groceries or personal errands. I don't have a \"why\" to go outside for other things, I'm not a party animal, I don't drink and I don't really like discos\n* Yes, I go to the gym daily, because I don't really have anything else to do besides that and videogames\n* Yes, I socialize with the people there or my colleagues at work\n* No, I don't consider them friends\n* No, I also don't have people that I consider real friends outside of the working environment. Keep in mind I'm a foreigner in this country and I personally find it difficult making connections with completely random people\n\nAgain any life, career, reality check or psychological advice is welcomed.",
          "author_fullname": "t2_335kn9t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I used to be confident of my skills but now I'm not so sure anymore",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c12gcs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712798491.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote this wall of text at 3AM, I&amp;#39;m sorry if there are any grammatical or syntactic errors&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR: I&amp;#39;ve been working for 2 years in a foreign country and I feel like I want to quit and go back home but what I think it&amp;#39;s an imposter syndrome that won&amp;#39;t let me abandon my job security. I shared my short career journey in this post to seek advice based on it.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Any life, career, reality check or psychological advice is welcomed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Big story time for context of my career path as of now. &lt;/p&gt;\n\n&lt;p&gt;Italy, 2022, I graduated from a highschool that had most of the subjects focused on programming (&amp;quot;Istituto Tecnico&amp;quot;). From this, I managed to get a diploma and learned my bases in programming (Java and Python OOP, concurrent programming, data structures like trees and lists, sockets and client-server applications). During my graduation year, I also worked part-time after school as a front-end developer at a very small local consulting company. &lt;/p&gt;\n\n&lt;p&gt;Finished highschool I wanted to continue working at the same company, so I started talking directly with the owner of the company (there was no HR department) if there was a possibility of continuing with a new normal full-time contract. They ended up offering me an apprenticeship of at least 6 months, 40h weekly hours with a salary of 800\u20ac a month (sadly this is something &amp;quot;normal&amp;quot; in Italy for young people). In my mind, I said &amp;quot;Fuck no, I&amp;#39;m better than this&amp;quot; so I accepted the offer but in the meanwhile started applying for jobs around Europe via Linkedin, confident enough of my skills and my English proficiency. After sending something like 5 total applications I found a junior Java developer offer (21k yearly) to relocate to Spain and work for a multinational American company, starting with a 3 month paid in-person bootcamp consisting of basic level programming company courses and after that 6-month probation contract, at the end of those 6 months the contract would have become automatically of indefinite duration. I managed to get the interviews, pass the technical test and technical interviews and sign the contract.&lt;/p&gt;\n\n&lt;p&gt;Now imagine the confidence boost that I got at the time by managing to land a position at a big company as a 20 year old fresh out of highschool and managing to leave my mother&amp;#39;s house and finally becoming independent, starting to build my own life.&lt;/p&gt;\n\n&lt;p&gt;Fast forward, after the bootcamp somehow, based on my performances and &amp;quot;luck&amp;quot;, I got assigned to a data engineering team that uses as a main language Scala with Spark and a tech stack that I would be seeing for the first time (Hadoop, Neo4j, Airflow, ETL processes and pipelines...). Even tho the initial job offer on Linkedin was for a Java dev, I didn&amp;#39;t complain, mainly because I was eager to learn something new and start specializing in a field. &lt;/p&gt;\n\n&lt;p&gt;Fast forward again almost 2 years, it&amp;#39;s 2024, I&amp;#39;m 22 and almost on the verge of a mental breakdown. Shortly I realized that independence is not flowers and rainbows and I started feeling lonely as fuck, starting to fall into depression.&lt;/p&gt;\n\n&lt;p&gt;To resolve this problem I was thinking of moving back to Italy, back to my family, my close friends and where there is someone that cares about me. I would need to find a new job and I would want to continue pursuing data engineering. I would have to leave everything behind, basically starting from scratch again, which psychologically terrorizes me because I feel like I don&amp;#39;t have the actual work experience and knowledge of what a junior Data Engineer is supposed to have and I get anxious only by thinking of doing job interviews.&lt;/p&gt;\n\n&lt;p&gt;I never got the opportunity to write a real spark application from scratch (I did some basic ones on my own but they are nowhere near as complex as real-world ones), everything that I did until now was maintenance and small code changes for the many company projects under my squad&amp;#39;s ownership. At this point, I know how to run the full ETL processes autonomously without the help of a senior behind me but I still struggle to understand the full workflow and how it works, why it works and the architectural logic behind it. Also, our tech stack is so vast that I have some basic experience with DEV/OPS stuff and Jenkins CI/CD pipelines but again, if you would ask me to write down the whole CI/CD Jenkinsfiles from scratch I feel like I wouldn&amp;#39;t be able to do it.&lt;/p&gt;\n\n&lt;p&gt;The team and manager are happy with my performance, in these 2 years thanks to a &amp;quot;company internal performance programme&amp;quot; I managed to get every 6 months small salary rises, with the projection of arriving at 30k yearly at the end of this year.&lt;/p&gt;\n\n&lt;p&gt;I used to be confident in myself and now, I don&amp;#39;t know if I have some kind of imposter syndrome that reached a point where I feel like I would not even be able to find another job in the data engineering field because maybe I&amp;#39;m not good enough for it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m afraid of abandoning my job security thinking that maybe it&amp;#39;s true, maybe I will not be able to find something better and I torment myself with enduring this unhappy life in a place that I don&amp;#39;t want to live anymore without anyone around me.&lt;/p&gt;\n\n&lt;p&gt;Before anyone tells me &amp;quot;You just need to go outside and start making some friends&amp;quot; or &amp;quot;Go to the gym&amp;quot;: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I mainly go outside for groceries or personal errands. I don&amp;#39;t have a &amp;quot;why&amp;quot; to go outside for other things, I&amp;#39;m not a party animal, I don&amp;#39;t drink and I don&amp;#39;t really like discos&lt;/li&gt;\n&lt;li&gt;Yes, I go to the gym daily, because I don&amp;#39;t really have anything else to do besides that and videogames&lt;/li&gt;\n&lt;li&gt;Yes, I socialize with the people there or my colleagues at work&lt;/li&gt;\n&lt;li&gt;No, I don&amp;#39;t consider them friends&lt;/li&gt;\n&lt;li&gt;No, I also don&amp;#39;t have people that I consider real friends outside of the working environment. Keep in mind I&amp;#39;m a foreigner in this country and I personally find it difficult making connections with completely random people&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Again any life, career, reality check or psychological advice is welcomed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c12gcs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Yveltal444",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c12gcs/i_used_to_be_confident_of_my_skills_but_now_im/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c12gcs/i_used_to_be_confident_of_my_skills_but_now_im/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712798491.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "",
          "author_fullname": "t2_1s3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using Python and Pandas to calculate net issuance of U.S. treasury securities",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c11nfb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/31knaxx8yqtc1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/31knaxx8yqtc1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/31knaxx8yqtc1/DASHPlaylist.mpd?a=1715534830%2CYjZjOGVlZjZmMTQ2YjliYzNmM2RlZDgzYjJlOTE3MjA3ODVjNTM0NDI4ODcxOTc2YWYxYzcwY2M1OTQzY2VlYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 610,
              "hls_url": "https://v.redd.it/31knaxx8yqtc1/HLSPlaylist.m3u8?a=1715534830%2CMDI0ZTJlYzM4ZGIxMzgwODFjNzVkZTY4MzNkZTU3MDllYjQ0ZDM2NmQ5NWM2NzdkNmY3ZTA0OTYzYWFjNGRlMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Personal Project Showcase",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/emp6M2ZyZm15cXRjMSLDhom3Y0WF4wWRAWI1NdmCiLA8ZUdyd0vslwNEzVb_.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f253d025ffc3bff6bfab950c16a109c3d2e64319",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712796249.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/31knaxx8yqtc1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/emp6M2ZyZm15cXRjMSLDhom3Y0WF4wWRAWI1NdmCiLA8ZUdyd0vslwNEzVb_.png?format=pjpg&amp;auto=webp&amp;s=949f61356fe19d3bee3c6359ef43fd94090e03ae",
                  "width": 3840,
                  "height": 2160
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/emp6M2ZyZm15cXRjMSLDhom3Y0WF4wWRAWI1NdmCiLA8ZUdyd0vslwNEzVb_.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5bf876c93a53eb2ceb84eaab268df8714d10fd1b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/emp6M2ZyZm15cXRjMSLDhom3Y0WF4wWRAWI1NdmCiLA8ZUdyd0vslwNEzVb_.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=164dec688f70d1d94e37e997a218da0b9388c20a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/emp6M2ZyZm15cXRjMSLDhom3Y0WF4wWRAWI1NdmCiLA8ZUdyd0vslwNEzVb_.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e0a1def8e44ffeaf8c2e0135f9cc4e409e24db85",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/emp6M2ZyZm15cXRjMSLDhom3Y0WF4wWRAWI1NdmCiLA8ZUdyd0vslwNEzVb_.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1ecaebfa2bade6d22aa65aa3e88364d55a29416b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/emp6M2ZyZm15cXRjMSLDhom3Y0WF4wWRAWI1NdmCiLA8ZUdyd0vslwNEzVb_.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc6e44c1c8b22215c06a082d7bc54fdf9a19f1b3",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/emp6M2ZyZm15cXRjMSLDhom3Y0WF4wWRAWI1NdmCiLA8ZUdyd0vslwNEzVb_.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fe4c643703ce734ddd559d121cfe61d54235d2c7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "emp6M2ZyZm15cXRjMSLDhom3Y0WF4wWRAWI1NdmCiLA8ZUdyd0vslwNEzVb_"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ddbd37",
          "id": "1c11nfb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dharmatech",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c11nfb/using_python_and_pandas_to_calculate_net_issuance/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://v.redd.it/31knaxx8yqtc1",
          "subreddit_subscribers": 176091,
          "created_utc": 1712796249.0,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/31knaxx8yqtc1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/31knaxx8yqtc1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/31knaxx8yqtc1/DASHPlaylist.mpd?a=1715534830%2CYjZjOGVlZjZmMTQ2YjliYzNmM2RlZDgzYjJlOTE3MjA3ODVjNTM0NDI4ODcxOTc2YWYxYzcwY2M1OTQzY2VlYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 610,
              "hls_url": "https://v.redd.it/31knaxx8yqtc1/HLSPlaylist.m3u8?a=1715534830%2CMDI0ZTJlYzM4ZGIxMzgwODFjNzVkZTY4MzNkZTU3MDllYjQ0ZDM2NmQ5NWM2NzdkNmY3ZTA0OTYzYWFjNGRlMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "[https://medium.com/towards-data-engineering/how-to-choose-the-right-olap-storage-349d9543fe8e](https://medium.com/towards-data-engineering/how-to-choose-the-right-olap-storage-349d9543fe8e)",
          "author_fullname": "t2_12exln",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to choose the Right OLAP Storage",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c10vai",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712794051.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/towards-data-engineering/how-to-choose-the-right-olap-storage-349d9543fe8e\"&gt;https://medium.com/towards-data-engineering/how-to-choose-the-right-olap-storage-349d9543fe8e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Fjt4d2MsGrePyLxqZoRu506nwybtpu2nzimbanpbnSQ.jpg?auto=webp&amp;s=f33ff39beba33bda982676521128a302294754c4",
                  "width": 1200,
                  "height": 798
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Fjt4d2MsGrePyLxqZoRu506nwybtpu2nzimbanpbnSQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=46e169a20d5e43ef7cdcdf1f54cea4582e8d1b4d",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://external-preview.redd.it/Fjt4d2MsGrePyLxqZoRu506nwybtpu2nzimbanpbnSQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a624c22fa029db707a722aeb4f3ba03f818f083",
                    "width": 216,
                    "height": 143
                  },
                  {
                    "url": "https://external-preview.redd.it/Fjt4d2MsGrePyLxqZoRu506nwybtpu2nzimbanpbnSQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=778bc7f4f2b0a06cde1e7889f6eb21d601b4fd93",
                    "width": 320,
                    "height": 212
                  },
                  {
                    "url": "https://external-preview.redd.it/Fjt4d2MsGrePyLxqZoRu506nwybtpu2nzimbanpbnSQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74d8d8ab966ab2c56d0b27f219d28f6f44519814",
                    "width": 640,
                    "height": 425
                  },
                  {
                    "url": "https://external-preview.redd.it/Fjt4d2MsGrePyLxqZoRu506nwybtpu2nzimbanpbnSQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3284bfd92c22f6d037bb9e6f86125e5169371277",
                    "width": 960,
                    "height": 638
                  },
                  {
                    "url": "https://external-preview.redd.it/Fjt4d2MsGrePyLxqZoRu506nwybtpu2nzimbanpbnSQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=598c7bcec73140262cb6fb919d716faf2f19c55f",
                    "width": 1080,
                    "height": 718
                  }
                ],
                "variants": {},
                "id": "bUC_5rOdt4XfH49oGSFlrAjRhW11Bhf1r9FlsdOB0yg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c10vai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Opiumred14",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c10vai/how_to_choose_the_right_olap_storage/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c10vai/how_to_choose_the_right_olap_storage/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712794051.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Quick background: currently my company uses Autosys which calls shell scripts, which then runs pyspark code to process data. Each application has a simple text file associated with it, and it stores one date value for the last successful run for the respective application. On initiation, the shell scripts reads this value, and decides if the spark process should run based on some logic. \n\nThe problem is that these shell scripts can reach 100-150+ lines of code and become difficult to maintain. I'm planning on moving some of the burden away from the shell into the Pyspark code itself for easier management. \n\nI need suggestions on how I can implement a checkpoint type system with this. Obviously I can just go back to the raw date file, but I wanted to make it better. \n\nSome rough requirements I can think of: \n1) Detect the last successful run date\n2) Check the last successful stage completed\n3) Process coordination: process A should be able to check the last run for process B\n\n\nMy first idea was to use a table for storing partition id and the time when it was processed \n\n\n\n```\n[\n20240401: processed at 20240401 12:23:34\n20240402: processed at 20240402 12:13:12\n]\n```\n\n\n\nBut with hundreds of applications, I don't see a way to design this without creating hundreds of tables. \n\nPlease send me any ideas your way. Thanks!",
          "author_fullname": "t2_3oxisa0t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I need design suggestions for a major overhaul. What are some good ways to implement checkpoints / date control in Pyspark - Hadoop ETL pipelines?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c1021w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712791846.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick background: currently my company uses Autosys which calls shell scripts, which then runs pyspark code to process data. Each application has a simple text file associated with it, and it stores one date value for the last successful run for the respective application. On initiation, the shell scripts reads this value, and decides if the spark process should run based on some logic. &lt;/p&gt;\n\n&lt;p&gt;The problem is that these shell scripts can reach 100-150+ lines of code and become difficult to maintain. I&amp;#39;m planning on moving some of the burden away from the shell into the Pyspark code itself for easier management. &lt;/p&gt;\n\n&lt;p&gt;I need suggestions on how I can implement a checkpoint type system with this. Obviously I can just go back to the raw date file, but I wanted to make it better. &lt;/p&gt;\n\n&lt;p&gt;Some rough requirements I can think of: \n1) Detect the last successful run date\n2) Check the last successful stage completed\n3) Process coordination: process A should be able to check the last run for process B&lt;/p&gt;\n\n&lt;p&gt;My first idea was to use a table for storing partition id and the time when it was processed &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n[\n20240401: processed at 20240401 12:23:34\n20240402: processed at 20240402 12:13:12\n]\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;But with hundreds of applications, I don&amp;#39;t see a way to design this without creating hundreds of tables. &lt;/p&gt;\n\n&lt;p&gt;Please send me any ideas your way. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c1021w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "knightfall0",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c1021w/i_need_design_suggestions_for_a_major_overhaul/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c1021w/i_need_design_suggestions_for_a_major_overhaul/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712791846.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hey, i have a table about 1 million rows that is taking a long time to write the table. I\u2019m not sure what why or how to fix it. It looks like it\u2019s not starting the writing process, it\u2019s just scanning the files. It can take up to 30 hours to write the delta table. Actually ingesting and displaying the top 100 rows only takes 2 minutes. It\u2019s very inconsistent for the delta table saving portion. I\u2019ve tried repartitioning, changing the schema to all nullable string types, and splitting the dataframe up into 4-10 equal sizes to append to the table. When i repartition the dataframe and count the number of partitions, it takes over an hour to print out the number of partitions as well. I\u2019ve never let the count finish, i always stopped it before as i assumed its not normal",
          "author_fullname": "t2_j4kzc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Databricks taking long time to save pyspark dataframe as a delta table",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0zb08",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712789904.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, i have a table about 1 million rows that is taking a long time to write the table. I\u2019m not sure what why or how to fix it. It looks like it\u2019s not starting the writing process, it\u2019s just scanning the files. It can take up to 30 hours to write the delta table. Actually ingesting and displaying the top 100 rows only takes 2 minutes. It\u2019s very inconsistent for the delta table saving portion. I\u2019ve tried repartitioning, changing the schema to all nullable string types, and splitting the dataframe up into 4-10 equal sizes to append to the table. When i repartition the dataframe and count the number of partitions, it takes over an hour to print out the number of partitions as well. I\u2019ve never let the count finish, i always stopped it before as i assumed its not normal&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0zb08",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bongdong42O",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0zb08/databricks_taking_long_time_to_save_pyspark/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0zb08/databricks_taking_long_time_to_save_pyspark/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712789904.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "For what I understand scan could be thought of as a stream, while read_csv_batched is, well, in batches... \n\nFor context I am working with several different datasets, that although related in topic, the authors have ALL decided to use different information schemas.\n\nWhat I want to know is: if I only care to get a quick feel of the dataset, which would be better memory-wise? (Because kernel keeps restarting)\n\nAnd second, related question: If I open several files in the same fashion, would the previous scans/batches die, get converted into full dataframes, or something else?",
          "author_fullname": "t2_ra02aic6l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Polars' scan_csv vs read_csv_batched",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0zaze",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712789902.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For what I understand scan could be thought of as a stream, while read_csv_batched is, well, in batches... &lt;/p&gt;\n\n&lt;p&gt;For context I am working with several different datasets, that although related in topic, the authors have ALL decided to use different information schemas.&lt;/p&gt;\n\n&lt;p&gt;What I want to know is: if I only care to get a quick feel of the dataset, which would be better memory-wise? (Because kernel keeps restarting)&lt;/p&gt;\n\n&lt;p&gt;And second, related question: If I open several files in the same fashion, would the previous scans/batches die, get converted into full dataframes, or something else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Junior Data Engineer",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0zaze",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "soposih_jaevel",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/dataengineering/comments/1c0zaze/polars_scan_csv_vs_read_csv_batched/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0zaze/polars_scan_csv_vs_read_csv_batched/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712789902.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I\u2019m struggling to find an alternative to Unity Catalog. We have a mix of Glue and Unity with a lot of workloads moving to Unity. My fear is that as a platform this locks us into the Databricks ecosystem. Their Hive Metastore API interface gives us some flexibility but should they remove the feature we would be stuck. \n\n( I know Glue is a alternative but from a feature standpoint it struggles to compete)",
          "author_fullname": "t2_opyfv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Alternative to Unity Catalog",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0yckq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712787465.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m struggling to find an alternative to Unity Catalog. We have a mix of Glue and Unity with a lot of workloads moving to Unity. My fear is that as a platform this locks us into the Databricks ecosystem. Their Hive Metastore API interface gives us some flexibility but should they remove the feature we would be stuck. &lt;/p&gt;\n\n&lt;p&gt;( I know Glue is a alternative but from a feature standpoint it struggles to compete)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0yckq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Letmeout1",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0yckq/alternative_to_unity_catalog/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0yckq/alternative_to_unity_catalog/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712787465.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "\n\nHello. A bit new to the spark framework\n\nI run a sequence of spark submit commands with a jar file. Each command alters the arguments being passed to the jar file. Is there a way to reuse the session? This all happens to yarn with cluster mode. So for example\n\nSpark-submit master yarn mode cluster A.jar input=1\nSpark-submit master yarn mode cluster A.jar input=2\nSpark-submit master yarn mode cluster A.jar input=3\nSpark-submit master yarn mode cluster A.jar input=4\n\nThe above commands work perfectly. All the configs for the spark are identical. And so I was thinking of doing it from a session. Can this be done with pyspark? ",
          "author_fullname": "t2_ac1s0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Passing arguments to scala jar from a spark session",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0y2y9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712786814.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. A bit new to the spark framework&lt;/p&gt;\n\n&lt;p&gt;I run a sequence of spark submit commands with a jar file. Each command alters the arguments being passed to the jar file. Is there a way to reuse the session? This all happens to yarn with cluster mode. So for example&lt;/p&gt;\n\n&lt;p&gt;Spark-submit master yarn mode cluster A.jar input=1\nSpark-submit master yarn mode cluster A.jar input=2\nSpark-submit master yarn mode cluster A.jar input=3\nSpark-submit master yarn mode cluster A.jar input=4&lt;/p&gt;\n\n&lt;p&gt;The above commands work perfectly. All the configs for the spark are identical. And so I was thinking of doing it from a session. Can this be done with pyspark? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0y2y9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "khante",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0y2y9/passing_arguments_to_scala_jar_from_a_spark/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0y2y9/passing_arguments_to_scala_jar_from_a_spark/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712786814.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hey folks, first of all, pardon my potentially ignorant question. I searched the sub trying to find answers to what I'm trying to learn here, but couldn't \\_quite\\_ get the answers I was looking for.\n\nAt my previous role, we used Databricks and Pyspark a ton, and we never had the opportunity to use DBT (I didn't actually know anything about DBT, really, until after I left the role). I keep thinking about that role, and some of the technical challenges faced. It would often take a month to fully implement ingesting a new data vendor, from \"bronze\" to \"gold\", because we (as in, the team, including efforts done before my arrival at the company) didn't have a standardized set of functions that applied across all types of columns of data to do data cleaning, and we didn't use anything like Great Expectations until near the end of my time there. We had a \"policy engine\" that would decide which vendor to pick for which gold attributes, and all of this was done in either Notebooks or Python Wheels. Testing was a nightmare (again, no GX OSS), especially with Notebook Workflows, and it all sort of amounts to me asking the question:\n\n\\- What the actual heck were we doing without DBT?\n\nIt just sort of seems like everything comes \"pre-rolled\" with DBT in a really nice way. You can write tests super easily, configuration seems straight forward, things like SCD Type 2 are a one liner, but then I ask myself:\n\n\\- what about when it gets more complicated?\n\nDoes DBT really start to suffer in this regard?\n\nFor example, what happens if you have really complicated sets of data cleaning steps? Or you want to apply an ML model to some cleaned data? Or you need to decide which 3rd party vendor data gets used from the Silver Medallion table to enrich your Gold data set? Does it all sort of fall apart? Or was it a mistake to lean so heavily on PySpark?\n\nLast last question:\n\nIf you were to set up a DBX environment today for:\n\n* testing,\n* easily implement SCD Type 2,\n* or even do Entity Resolution\n\nhow would you go about doing so in an idiomatic way using ONLY pyspark related technologies? I can't help but feel like I (and my team) were left in the dust by newer (not even that much newer!) tech.\n\nThank you for engaging in this! Cheers!",
          "author_fullname": "t2_2lrnc259",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Databricks and DBT; would it have been better to simply use dbt-core over pyspark?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0y1eg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712786708.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, first of all, pardon my potentially ignorant question. I searched the sub trying to find answers to what I&amp;#39;m trying to learn here, but couldn&amp;#39;t _quite_ get the answers I was looking for.&lt;/p&gt;\n\n&lt;p&gt;At my previous role, we used Databricks and Pyspark a ton, and we never had the opportunity to use DBT (I didn&amp;#39;t actually know anything about DBT, really, until after I left the role). I keep thinking about that role, and some of the technical challenges faced. It would often take a month to fully implement ingesting a new data vendor, from &amp;quot;bronze&amp;quot; to &amp;quot;gold&amp;quot;, because we (as in, the team, including efforts done before my arrival at the company) didn&amp;#39;t have a standardized set of functions that applied across all types of columns of data to do data cleaning, and we didn&amp;#39;t use anything like Great Expectations until near the end of my time there. We had a &amp;quot;policy engine&amp;quot; that would decide which vendor to pick for which gold attributes, and all of this was done in either Notebooks or Python Wheels. Testing was a nightmare (again, no GX OSS), especially with Notebook Workflows, and it all sort of amounts to me asking the question:&lt;/p&gt;\n\n&lt;p&gt;- What the actual heck were we doing without DBT?&lt;/p&gt;\n\n&lt;p&gt;It just sort of seems like everything comes &amp;quot;pre-rolled&amp;quot; with DBT in a really nice way. You can write tests super easily, configuration seems straight forward, things like SCD Type 2 are a one liner, but then I ask myself:&lt;/p&gt;\n\n&lt;p&gt;- what about when it gets more complicated?&lt;/p&gt;\n\n&lt;p&gt;Does DBT really start to suffer in this regard?&lt;/p&gt;\n\n&lt;p&gt;For example, what happens if you have really complicated sets of data cleaning steps? Or you want to apply an ML model to some cleaned data? Or you need to decide which 3rd party vendor data gets used from the Silver Medallion table to enrich your Gold data set? Does it all sort of fall apart? Or was it a mistake to lean so heavily on PySpark?&lt;/p&gt;\n\n&lt;p&gt;Last last question:&lt;/p&gt;\n\n&lt;p&gt;If you were to set up a DBX environment today for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;testing,&lt;/li&gt;\n&lt;li&gt;easily implement SCD Type 2,&lt;/li&gt;\n&lt;li&gt;or even do Entity Resolution&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;how would you go about doing so in an idiomatic way using ONLY pyspark related technologies? I can&amp;#39;t help but feel like I (and my team) were left in the dust by newer (not even that much newer!) tech.&lt;/p&gt;\n\n&lt;p&gt;Thank you for engaging in this! Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0y1eg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "50mm_foto",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0y1eg/databricks_and_dbt_would_it_have_been_better_to/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0y1eg/databricks_and_dbt_would_it_have_been_better_to/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712786708.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi,\n\nI have been working at my current company for the past 5 years as a Senior ML and data engineer and we have our own data center so all of our resources are OnPrem. I have used GCP during my free time to build some pipelines for fun or playaround with some big datasets in BigQuery.\n\nNow onto my main problem, I have sent in so many applications for data engineering positions and if I get to have a meeting with the HR, I almost always get to the 2nd round to have a meeting with the hiring manager but that's where it ends for me as I get a rejection saying we need someone with more \"professional\" cloud experience. I know I will able to keep up after joining as I enjoy data engineering problems but how do I navigate this problem of having no \"professional\" cloud experience? ",
          "author_fullname": "t2_bmxda7ioc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Trying to switch jobs, how do I navigate the having no \"professional\" cloud experience problem?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0wscx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712783662.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have been working at my current company for the past 5 years as a Senior ML and data engineer and we have our own data center so all of our resources are OnPrem. I have used GCP during my free time to build some pipelines for fun or playaround with some big datasets in BigQuery.&lt;/p&gt;\n\n&lt;p&gt;Now onto my main problem, I have sent in so many applications for data engineering positions and if I get to have a meeting with the HR, I almost always get to the 2nd round to have a meeting with the hiring manager but that&amp;#39;s where it ends for me as I get a rejection saying we need someone with more &amp;quot;professional&amp;quot; cloud experience. I know I will able to keep up after joining as I enjoy data engineering problems but how do I navigate this problem of having no &amp;quot;professional&amp;quot; cloud experience? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c0wscx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Even_Work_7995",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0wscx/trying_to_switch_jobs_how_do_i_navigate_the/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0wscx/trying_to_switch_jobs_how_do_i_navigate_the/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712783662.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi,   I was wondering if anyone has gotten into performance engineering for their pipelines on AWS with any tools that do introspection ?  We have a few tools for our web services that can analyze at run time to understand where time is being spent and identify hotspots for optimization. \n\n My org is about to do a large scale performance test to make sure our analytic pipelines can handle roll out of a new product across the country that will dramatically increase the volume coming through.  Right now their plan is to just wing it and start/stop which seems really dumb.  \n\nI've done performance engineering on mainframe and web servers, but not on AWS data pipelines.  We use step functions running a series of lambdas and glue jobs mostly using glue 3, few glue 1 for python shell.  ",
          "author_fullname": "t2_7eumd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Performance Analysis tools for a Step Function Flow ?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0vsve",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712781198.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,   I was wondering if anyone has gotten into performance engineering for their pipelines on AWS with any tools that do introspection ?  We have a few tools for our web services that can analyze at run time to understand where time is being spent and identify hotspots for optimization. &lt;/p&gt;\n\n&lt;p&gt;My org is about to do a large scale performance test to make sure our analytic pipelines can handle roll out of a new product across the country that will dramatically increase the volume coming through.  Right now their plan is to just wing it and start/stop which seems really dumb.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done performance engineering on mainframe and web servers, but not on AWS data pipelines.  We use step functions running a series of lambdas and glue jobs mostly using glue 3, few glue 1 for python shell.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0vsve",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vandelay82",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0vsve/performance_analysis_tools_for_a_step_function/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0vsve/performance_analysis_tools_for_a_step_function/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712781198.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "**Backstory:**\n\nI've been in my role for a while, and most of my tasks are ETL that run on a daily basis to fetch the data (batch). and the pipeline was there and built by previous DEs who left without any handovers to me, my daily tasks are all about copying this pipeline to use other APIs\n\nthe pipeline starts with an API endpoints to pull the data then does some transformations to the JSON files and lastly pushes it to BQ. My job ends there and then ML Engineers and analysts start their work from this point.\n\n**My tech stack is:**\n\nCloud Composer(triggers) -&gt; GKEs(python code) -&gt; GCS -&gt; BQ.\n\nall of the work happens in the second step where I write Python code to do the API calls and all the transformations from JSON into JSONL and lastly push it to GCS and BQ.\n\n**My Questions:**  \nIs this still a relevant data pipeline? Should I consider some other tools? like dataflow? Cloud run? to ease the process?\n\nAlso, I face a problem, that the ETL tasks are deeply coupled, if there's an error in the last stage(Load) I would re-run all the stages starting from the API calls which sometimes leads to rate limits\n\n1- in which step (and where?) should I transform the raw json into jsonL?\n\n2- should I use gcs after every stage? put raw json into gcs and then pull them again to transform and then load?\n\n3- should I transform raw json to csv over jsonL for easier and faster transformations?\n\nAll input is greatly appreciated.",
          "author_fullname": "t2_14ebvg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "APIs to BigQuery Pipeline - thoughts required",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0v01p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1712787200.0,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712779285.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Backstory:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been in my role for a while, and most of my tasks are ETL that run on a daily basis to fetch the data (batch). and the pipeline was there and built by previous DEs who left without any handovers to me, my daily tasks are all about copying this pipeline to use other APIs&lt;/p&gt;\n\n&lt;p&gt;the pipeline starts with an API endpoints to pull the data then does some transformations to the JSON files and lastly pushes it to BQ. My job ends there and then ML Engineers and analysts start their work from this point.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My tech stack is:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Cloud Composer(triggers) -&amp;gt; GKEs(python code) -&amp;gt; GCS -&amp;gt; BQ.&lt;/p&gt;\n\n&lt;p&gt;all of the work happens in the second step where I write Python code to do the API calls and all the transformations from JSON into JSONL and lastly push it to GCS and BQ.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Questions:&lt;/strong&gt;&lt;br/&gt;\nIs this still a relevant data pipeline? Should I consider some other tools? like dataflow? Cloud run? to ease the process?&lt;/p&gt;\n\n&lt;p&gt;Also, I face a problem, that the ETL tasks are deeply coupled, if there&amp;#39;s an error in the last stage(Load) I would re-run all the stages starting from the API calls which sometimes leads to rate limits&lt;/p&gt;\n\n&lt;p&gt;1- in which step (and where?) should I transform the raw json into jsonL?&lt;/p&gt;\n\n&lt;p&gt;2- should I use gcs after every stage? put raw json into gcs and then pull them again to transform and then load?&lt;/p&gt;\n\n&lt;p&gt;3- should I transform raw json to csv over jsonL for easier and faster transformations?&lt;/p&gt;\n\n&lt;p&gt;All input is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0v01p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AhmidAziz",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0v01p/apis_to_bigquery_pipeline_thoughts_required/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0v01p/apis_to_bigquery_pipeline_thoughts_required/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712779285.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "As a long-time Databricks user, I've seen this trend toward them being solely focused on making as much money as possible. Unless they plan to change pricing as well, this will double the Job cost of people currently running Standard. Can you imagine waking up and the cost of your data platform doubling overnight?\n\n\"But you get so many more features, it's a great price, you can optimize this and that, blah, blah, blah.\" Well, maybe I don't want or need Unity Catalog.\n\nI still say it's the beginning of a new era of Corporate Databricks, bow before us and bring your tribute. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/vzco1rabfptc1.png?width=1620&amp;format=png&amp;auto=webp&amp;s=0f4d2e84bbf4e9efaafaa609c2d78b11d0170573",
          "author_fullname": "t2_hdte75ow1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "And so it begins. Databricks just couldn't help themselves. Get your wallets out.",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 76,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vzco1rabfptc1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/vzco1rabfptc1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7caae4cce1f84e59a5846fd1f68ffef9bbdfa573"
                },
                {
                  "y": 118,
                  "x": 216,
                  "u": "https://preview.redd.it/vzco1rabfptc1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=456feb583e05bb4c2c359e1410f1d16342e094ec"
                },
                {
                  "y": 175,
                  "x": 320,
                  "u": "https://preview.redd.it/vzco1rabfptc1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1cdf53ba98676ce1f860f73602571b2a6b6873ac"
                },
                {
                  "y": 350,
                  "x": 640,
                  "u": "https://preview.redd.it/vzco1rabfptc1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76c91eb591e2d487eca3eaa279cbc0428e05e814"
                },
                {
                  "y": 526,
                  "x": 960,
                  "u": "https://preview.redd.it/vzco1rabfptc1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c5191e833863ff557522fec98bec39bb04c3cce"
                },
                {
                  "y": 592,
                  "x": 1080,
                  "u": "https://preview.redd.it/vzco1rabfptc1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a9798d3f7d7f96d4a71fdecc00f0b11a2ce6b65"
                }
              ],
              "s": {
                "y": 888,
                "x": 1620,
                "u": "https://preview.redd.it/vzco1rabfptc1.png?width=1620&amp;format=png&amp;auto=webp&amp;s=0f4d2e84bbf4e9efaafaa609c2d78b11d0170573"
              },
              "id": "vzco1rabfptc1"
            }
          },
          "name": "t3_1c0uf21",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 316,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 316,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/9QtsS1Iy9X5cXw-919bczvofiEM0VZ4U9O1pPQ2IBwc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712777900.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a long-time Databricks user, I&amp;#39;ve seen this trend toward them being solely focused on making as much money as possible. Unless they plan to change pricing as well, this will double the Job cost of people currently running Standard. Can you imagine waking up and the cost of your data platform doubling overnight?&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;But you get so many more features, it&amp;#39;s a great price, you can optimize this and that, blah, blah, blah.&amp;quot; Well, maybe I don&amp;#39;t want or need Unity Catalog.&lt;/p&gt;\n\n&lt;p&gt;I still say it&amp;#39;s the beginning of a new era of Corporate Databricks, bow before us and bring your tribute. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vzco1rabfptc1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f4d2e84bbf4e9efaafaa609c2d78b11d0170573\"&gt;https://preview.redd.it/vzco1rabfptc1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f4d2e84bbf4e9efaafaa609c2d78b11d0170573&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0uf21",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dataengineeringdude",
          "discussion_type": null,
          "num_comments": 188,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0uf21/and_so_it_begins_databricks_just_couldnt_help/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0uf21/and_so_it_begins_databricks_just_couldnt_help/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712777900.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I'm tasked with building a new Data Lakehouse because the current DWH is a bit messy and evolved \"historically\" = technical debt.\n\nSince I'm a new DE and was working as an Analytical Engineer before I have no clue about such stuff from an architectural point of view. As I am the only DE in this team I have the lead in the end and need to make decisions in the next few weeks.\n\nStorage costs are no concern atm and won't be for the next 5+ years because we have under 100TB for the foreseeable future. Compute is the most expensive and that's based on unoptimized Databricks pySpark jobs (will be migrated to dbt) and unoptimized Tableau dashboards for a big part (not my responsibility).\n\nSo far I'm thinking about the following:\n\n* medallion lakehouse architecture with bronze/silver/gold layer\n* bronze=raw, gold=final for 1:1 table import in dashboarding tools (Tableau here)\n* structured raw data in bronze, unstructured raw data as parquet files on GCP Storage\n* raw data / bronze as type STRING for all columns (based on this discussion: [VARCHAR for everything](https://old.reddit.com/r/dataengineering/comments/1byxjo6/using_varchar_for_everything/))\n* silver + gold tables require a partition on creation and partitions_required in queries so that analysts in my company can make their own queries without using `select * from table_a` and creating \u20ac\u20ac\u20ac \n* ELT: pipelines with dbt-core because the website/app tracking solution we use and won't change comes with a dbt package included that takes the raw event data and spits out ready-to-use tables and we need more copy+paste solutions because we are a small team and need plug &amp; play solutions where available (+ we have a 2 weeks consulting from a consulting company for setting up dbt correctly/best practices coming next month because that decision was made before I came into the company)\n* orchestration: dagster\n\nIn the future maybe migrating to SQLMesh because dbt is just a dumb string template engine while SQLMesh understands SQL on an abstract level via AST and can leverage that power for cooler things than dbt but until 1.0 = stable API and a reasonable license model something to look out in +3 years probably.\n\nAny pros/cons or something I'm missing?",
          "author_fullname": "t2_vjnnpobbl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a Data Lakehouse in BigQuery - best practices?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0tanl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1712824172.0,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712775176.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m tasked with building a new Data Lakehouse because the current DWH is a bit messy and evolved &amp;quot;historically&amp;quot; = technical debt.&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;m a new DE and was working as an Analytical Engineer before I have no clue about such stuff from an architectural point of view. As I am the only DE in this team I have the lead in the end and need to make decisions in the next few weeks.&lt;/p&gt;\n\n&lt;p&gt;Storage costs are no concern atm and won&amp;#39;t be for the next 5+ years because we have under 100TB for the foreseeable future. Compute is the most expensive and that&amp;#39;s based on unoptimized Databricks pySpark jobs (will be migrated to dbt) and unoptimized Tableau dashboards for a big part (not my responsibility).&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;m thinking about the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;medallion lakehouse architecture with bronze/silver/gold layer&lt;/li&gt;\n&lt;li&gt;bronze=raw, gold=final for 1:1 table import in dashboarding tools (Tableau here)&lt;/li&gt;\n&lt;li&gt;structured raw data in bronze, unstructured raw data as parquet files on GCP Storage&lt;/li&gt;\n&lt;li&gt;raw data / bronze as type STRING for all columns (based on this discussion: &lt;a href=\"https://old.reddit.com/r/dataengineering/comments/1byxjo6/using_varchar_for_everything/\"&gt;VARCHAR for everything&lt;/a&gt;)&lt;/li&gt;\n&lt;li&gt;silver + gold tables require a partition on creation and partitions_required in queries so that analysts in my company can make their own queries without using &lt;code&gt;select * from table_a&lt;/code&gt; and creating \u20ac\u20ac\u20ac &lt;/li&gt;\n&lt;li&gt;ELT: pipelines with dbt-core because the website/app tracking solution we use and won&amp;#39;t change comes with a dbt package included that takes the raw event data and spits out ready-to-use tables and we need more copy+paste solutions because we are a small team and need plug &amp;amp; play solutions where available (+ we have a 2 weeks consulting from a consulting company for setting up dbt correctly/best practices coming next month because that decision was made before I came into the company)&lt;/li&gt;\n&lt;li&gt;orchestration: dagster&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the future maybe migrating to SQLMesh because dbt is just a dumb string template engine while SQLMesh understands SQL on an abstract level via AST and can leverage that power for cooler things than dbt but until 1.0 = stable API and a reasonable license model something to look out in +3 years probably.&lt;/p&gt;\n\n&lt;p&gt;Any pros/cons or something I&amp;#39;m missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Data Engineer",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0tanl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PuddingGryphon",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/dataengineering/comments/1c0tanl/building_a_data_lakehouse_in_bigquery_best/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0tanl/building_a_data_lakehouse_in_bigquery_best/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712775176.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "What is the best way to create a data model for top 10 products in outlets. If I have 1000 outlet_ids should I create 10 rows for top 10 products (productid, name, quantities, price) or 10 columns for each in snowflake. This model runs every week.  ",
          "author_fullname": "t2_tv6ka9ikq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Efficient data model",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0syna",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712774355.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the best way to create a data model for top 10 products in outlets. If I have 1000 outlet_ids should I create 10 rows for top 10 products (productid, name, quantities, price) or 10 columns for each in snowflake. This model runs every week.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0syna",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "laterunner-3",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0syna/efficient_data_model/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0syna/efficient_data_model/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712774355.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I've recently joined a project and that project was running for past 3 or 3 and half years with snowflake for a Data warehouse and For ETL and ELT transformations.But, my client switched matillion for ETL and ELT.And the reason they are stating is that snowflake cost so much money for them for Running TASKS and They are having JavaScript stored procedure's to make a load from Source to fact or dim tables.so,they also stating it's toughest to maintain code quality.So, we are switching matillion for cost cut and better code structure using components.But, in matillion also they are mostly using many SQL queries with individual SQL components and My thinking is that snowflake cost that much cost for tasks and still matillion run with snowflake as a warehouse and computer engine etc. what you guys think about this? \n\nWhy ETL or ETL in Matillion ? Not in snowflake ?",
          "author_fullname": "t2_w8gq4h9uo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why Matillion with Snowflake?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0r1l9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712769628.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently joined a project and that project was running for past 3 or 3 and half years with snowflake for a Data warehouse and For ETL and ELT transformations.But, my client switched matillion for ETL and ELT.And the reason they are stating is that snowflake cost so much money for them for Running TASKS and They are having JavaScript stored procedure&amp;#39;s to make a load from Source to fact or dim tables.so,they also stating it&amp;#39;s toughest to maintain code quality.So, we are switching matillion for cost cut and better code structure using components.But, in matillion also they are mostly using many SQL queries with individual SQL components and My thinking is that snowflake cost that much cost for tasks and still matillion run with snowflake as a warehouse and computer engine etc. what you guys think about this? &lt;/p&gt;\n\n&lt;p&gt;Why ETL or ETL in Matillion ? Not in snowflake ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0r1l9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "avin_045",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0r1l9/why_matillion_with_snowflake/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0r1l9/why_matillion_with_snowflake/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712769628.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Help!! | have around 60 task in my snowflake All the task will run around 13 times in a day through the task it's call the procedure which check the stream for any inserted or updated values if there is any newly inserted or updated values its get populated into my tables.what i need to know the count of values after the task runs for daily weekly and monthly basis. Remember every task will run around 13 times a day i need the count of each single time.\nThanks",
          "author_fullname": "t2_cb4lxzkf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Task Monitoring in snowflake ",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0qhrc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712768276.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Help!! | have around 60 task in my snowflake All the task will run around 13 times in a day through the task it&amp;#39;s call the procedure which check the stream for any inserted or updated values if there is any newly inserted or updated values its get populated into my tables.what i need to know the count of values after the task runs for daily weekly and monthly basis. Remember every task will run around 13 times a day i need the count of each single time.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0qhrc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "New_Mechanic_9502",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0qhrc/task_monitoring_in_snowflake/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0qhrc/task_monitoring_in_snowflake/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712768276.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I posted this in the streamlit sub but I don't think there are as many people viewing that sub. Hopefully some of you here will have input. See below...  \n\n\nNot sure how many are familiar with the BI tool Sigma but we have a  client who is planning to use Sigma as a tool to ingest tabular (excel)  reports into Snowflake and then make updates to those reports. Keep in  mind there will probably be 100+ reports being uploaded as individual  tables (and then additional uploads to those tables to keep the data  fresh with new excel files). Once these reports are uploaded they'll be updated on a weekly basis by multiple users (40+ potentially). Sigma is a  great tool for this use but all those users require a Sigma license  which can make using Sigma expensive for the client.  \nThis is where  Streamlit comes in. My boss suggested maybe building an app to handle  all this functionality in Streamlit since we're a snowflake shop and  Snowflake has streamlit functionality built into Snowsight now (you can host the app within Snowflake essentially).   \nNow for the record I'm a more of sql guy, not really experienced in python/streamlit but I can learn it.  With that said I'm curious to know if people think this is a viable task  for Streamlit to handle. Would it be feasible to build an app that can  ingest spreadsheet data into Snowflake and then make the snowflake  tables update based on user input... from multiple users, potentially at  the same time (so you could have 10-20 users all making updates  simultaneously).  \nI have a buddy who's very experienced with  python/streamlit/etc and he told me his concern would be the  multi-tenancy thing since Streamlit by default doesn't handle that super  well. Anyone have thoughts on this?",
          "author_fullname": "t2_556jqozb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using Streamlit (via Snowflake) as a data ingestion and update tool (versus Sigma BI tool)",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0qg7p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712768166.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted this in the streamlit sub but I don&amp;#39;t think there are as many people viewing that sub. Hopefully some of you here will have input. See below...  &lt;/p&gt;\n\n&lt;p&gt;Not sure how many are familiar with the BI tool Sigma but we have a  client who is planning to use Sigma as a tool to ingest tabular (excel)  reports into Snowflake and then make updates to those reports. Keep in  mind there will probably be 100+ reports being uploaded as individual  tables (and then additional uploads to those tables to keep the data  fresh with new excel files). Once these reports are uploaded they&amp;#39;ll be updated on a weekly basis by multiple users (40+ potentially). Sigma is a  great tool for this use but all those users require a Sigma license  which can make using Sigma expensive for the client.&lt;br/&gt;\nThis is where  Streamlit comes in. My boss suggested maybe building an app to handle  all this functionality in Streamlit since we&amp;#39;re a snowflake shop and  Snowflake has streamlit functionality built into Snowsight now (you can host the app within Snowflake essentially).&lt;br/&gt;\nNow for the record I&amp;#39;m a more of sql guy, not really experienced in python/streamlit but I can learn it.  With that said I&amp;#39;m curious to know if people think this is a viable task  for Streamlit to handle. Would it be feasible to build an app that can  ingest spreadsheet data into Snowflake and then make the snowflake  tables update based on user input... from multiple users, potentially at  the same time (so you could have 10-20 users all making updates  simultaneously).&lt;br/&gt;\nI have a buddy who&amp;#39;s very experienced with  python/streamlit/etc and he told me his concern would be the  multi-tenancy thing since Streamlit by default doesn&amp;#39;t handle that super  well. Anyone have thoughts on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0qg7p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MasterKluch",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0qg7p/using_streamlit_via_snowflake_as_a_data_ingestion/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0qg7p/using_streamlit_via_snowflake_as_a_data_ingestion/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712768166.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Last year, I set out to build a data project to start learning about data engineering. Since then, I\u2019ve had some positive return on investment. \n\nBy building my project, I was able to learn enough about building data pipelines to take on building pipelines at work. My current boss saw what I could do and asked me to build out a data infrastructure with the end goal of building Power BI dashboards.\n\nNow, I\u2019m solely responsible for managing several pipelines that read from different sources, applying transformations, and loading into a data warehouse for Power BI to read from. 30+ people now rely on me to keep the dashboards up to date. \n\nI\u2019ve also took a further dive into Microsoft\u2019s Power Platform with Power Apps and Power Automate. \n\nMy personal project and the Power BI dashboards have caught the attention of the CIO and VP of Finance, respectively. I\u2019m putting my name on the map! \n\nThe CIO wants me to start taking AWS courses with a trainer and to learn more about our company\u2019s IT infrastructure. He liked my passion and motivation. \n\nFunny thing is, I wasn\u2019t originally hired for this type of work but is what I\u2019ve been studying for and it just fell into my lap!\n\nAll this is to say that I\u2019m proud of myself and am making a name for myself here. \n",
          "author_fullname": "t2_bix7v2w5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A year into building a data project",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0qaxv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 89,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 89,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712767811.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last year, I set out to build a data project to start learning about data engineering. Since then, I\u2019ve had some positive return on investment. &lt;/p&gt;\n\n&lt;p&gt;By building my project, I was able to learn enough about building data pipelines to take on building pipelines at work. My current boss saw what I could do and asked me to build out a data infrastructure with the end goal of building Power BI dashboards.&lt;/p&gt;\n\n&lt;p&gt;Now, I\u2019m solely responsible for managing several pipelines that read from different sources, applying transformations, and loading into a data warehouse for Power BI to read from. 30+ people now rely on me to keep the dashboards up to date. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve also took a further dive into Microsoft\u2019s Power Platform with Power Apps and Power Automate. &lt;/p&gt;\n\n&lt;p&gt;My personal project and the Power BI dashboards have caught the attention of the CIO and VP of Finance, respectively. I\u2019m putting my name on the map! &lt;/p&gt;\n\n&lt;p&gt;The CIO wants me to start taking AWS courses with a trainer and to learn more about our company\u2019s IT infrastructure. He liked my passion and motivation. &lt;/p&gt;\n\n&lt;p&gt;Funny thing is, I wasn\u2019t originally hired for this type of work but is what I\u2019ve been studying for and it just fell into my lap!&lt;/p&gt;\n\n&lt;p&gt;All this is to say that I\u2019m proud of myself and am making a name for myself here. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c0qaxv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "digitalghost-dev",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0qaxv/a_year_into_building_a_data_project/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0qaxv/a_year_into_building_a_data_project/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712767811.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I hope someone who has info finds this post .\n\nI have my recruiter call with Coinbase this Friday I have been applying for 6 months and finally able to get someone interested in my profile.\n\ncould someone share your recent experience with Coinbase recruitment process for a Data Engineer role and what to expect in the process!\n\nWould appreciate any relevant information! \n\nPlease and thank you!",
          "author_fullname": "t2_m76f6awr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Coinbase",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0otb3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712764108.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope someone who has info finds this post .&lt;/p&gt;\n\n&lt;p&gt;I have my recruiter call with Coinbase this Friday I have been applying for 6 months and finally able to get someone interested in my profile.&lt;/p&gt;\n\n&lt;p&gt;could someone share your recent experience with Coinbase recruitment process for a Data Engineer role and what to expect in the process!&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any relevant information! &lt;/p&gt;\n\n&lt;p&gt;Please and thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c0otb3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Cat_8466",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0otb3/coinbase/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0otb3/coinbase/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712764108.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "So I am learning about [Aggregated Facts as Dimension Attributes](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/aggregated-fact-attribute/) , and I'm having a hard time wrapping around the practical application of the concept. As I'm thinking it has the potential to create a cyclic data models.\n\nLet's say I have a requirement to add a column **is\\_active\\_customer** that is identified if a customer made any transactions for the past 6 months. How would I design this?  \n\n\nI have the ff. source tables:  \n1. tbl\\_customers  \n2. tbl\\_transactions  \n\n\nHere are the approaches that I can think of:\n\n1. source\\_table -&gt; fact\\_transactions -&gt; dim\\_customer with derived column **is\\_active\\_customer** from fact\\_transactions\n2. source\\_table -&gt; fact\\_transactions -&gt; agg\\_customer (contains the aggregates for customer) then just have this aggregate table to be joined with dim\\_customer  \n\n\nPlease recommend any approaches or concepts that I can add that you can think of.  \n\n\nI hope my question makes sense. Thanks in advance!  \n\n\n&amp;#x200B;",
          "author_fullname": "t2_mbbdv7y98",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best practice to develop derived attributes in DWH? Aggregated Facts as Dimension Attributes - Dimensional Modeling",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0ofix",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712763134.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am learning about &lt;a href=\"https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/aggregated-fact-attribute/\"&gt;Aggregated Facts as Dimension Attributes&lt;/a&gt; , and I&amp;#39;m having a hard time wrapping around the practical application of the concept. As I&amp;#39;m thinking it has the potential to create a cyclic data models.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I have a requirement to add a column &lt;strong&gt;is_active_customer&lt;/strong&gt; that is identified if a customer made any transactions for the past 6 months. How would I design this?  &lt;/p&gt;\n\n&lt;p&gt;I have the ff. source tables:&lt;br/&gt;\n1. tbl_customers&lt;br/&gt;\n2. tbl_transactions  &lt;/p&gt;\n\n&lt;p&gt;Here are the approaches that I can think of:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;source_table -&amp;gt; fact_transactions -&amp;gt; dim_customer with derived column &lt;strong&gt;is_active_customer&lt;/strong&gt; from fact_transactions&lt;/li&gt;\n&lt;li&gt;source_table -&amp;gt; fact_transactions -&amp;gt; agg_customer (contains the aggregates for customer) then just have this aggregate table to be joined with dim_customer&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please recommend any approaches or concepts that I can add that you can think of.  &lt;/p&gt;\n\n&lt;p&gt;I hope my question makes sense. Thanks in advance!  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0ofix",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kindly-Screen-2557",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0ofix/best_practice_to_develop_derived_attributes_in/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0ofix/best_practice_to_develop_derived_attributes_in/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712763134.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I've made a list of tools in the space to know what competition looks like :P\n\nBut it's a fast moving space, so I am sure this community has seen more!  \n[https://github.com/Snowboard-Software/awesome-ai-analytics](https://github.com/Snowboard-Software/awesome-ai-analytics)",
          "author_fullname": "t2_hcahe2uu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "a list of ai analytics tools",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0o54t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712762418.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve made a list of tools in the space to know what competition looks like :P&lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s a fast moving space, so I am sure this community has seen more!&lt;br/&gt;\n&lt;a href=\"https://github.com/Snowboard-Software/awesome-ai-analytics\"&gt;https://github.com/Snowboard-Software/awesome-ai-analytics&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/CL0aYgoNHLx166NtJU9qZxzDqyryX5cZxr8223Flysw.jpg?auto=webp&amp;s=8a48394ccf83833c2615bde9005c6d9881bd9da6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/CL0aYgoNHLx166NtJU9qZxzDqyryX5cZxr8223Flysw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54979556be1d818dde5a8dadd90b9141342a66d7",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/CL0aYgoNHLx166NtJU9qZxzDqyryX5cZxr8223Flysw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a2639d03de14151d05179a562312af5a91a58b0e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/CL0aYgoNHLx166NtJU9qZxzDqyryX5cZxr8223Flysw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=349ee839b690c1cb5badd23d8a5887d73786aaac",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/CL0aYgoNHLx166NtJU9qZxzDqyryX5cZxr8223Flysw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=63d0b7287294f36c7ad3f8c42251565d711ca7e2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/CL0aYgoNHLx166NtJU9qZxzDqyryX5cZxr8223Flysw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa9e618da1194e3fd7c99cb691f32650e2996e29",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/CL0aYgoNHLx166NtJU9qZxzDqyryX5cZxr8223Flysw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69c33b0b376104e961dbef85dd8cd976f6fcb179",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "8C-AmCBX0DWa5lH6SpDKHDXxjaaUzTDdHhfbxSzel2k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0o54t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rickradewagen",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0o54t/a_list_of_ai_analytics_tools/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0o54t/a_list_of_ai_analytics_tools/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712762418.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "",
          "author_fullname": "t2_6pheknqy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Masking PII with Stateful Service Composition and Fluvio Streams",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0o4s2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XcQyGEc4knRjpDtMS9MarrVVwiPpRhFtpwnXdbTq_dY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712762394.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "infinyon.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://infinyon.com/blog/2024/04/mask-pii-stateful-streaming",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nQX0-UoCtJiU2eW5zsCqxIc-FAH9ztZJssnGtQQnoIs.jpg?auto=webp&amp;s=d33922befbf8fb980d094c7f4d7c596998bdc2bc",
                  "width": 3082,
                  "height": 1667
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nQX0-UoCtJiU2eW5zsCqxIc-FAH9ztZJssnGtQQnoIs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1fed2c1354633a897ae2d061e9ba7fb8e7921612",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/nQX0-UoCtJiU2eW5zsCqxIc-FAH9ztZJssnGtQQnoIs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a14d3bc0178a9616882dd255601e4dddcb6931f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/nQX0-UoCtJiU2eW5zsCqxIc-FAH9ztZJssnGtQQnoIs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7acf9862e712de581a1103354c9fc8e79187fe2e",
                    "width": 320,
                    "height": 173
                  },
                  {
                    "url": "https://external-preview.redd.it/nQX0-UoCtJiU2eW5zsCqxIc-FAH9ztZJssnGtQQnoIs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e3fd1dfba880ace51bea77517159db2441b34a41",
                    "width": 640,
                    "height": 346
                  },
                  {
                    "url": "https://external-preview.redd.it/nQX0-UoCtJiU2eW5zsCqxIc-FAH9ztZJssnGtQQnoIs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7369f8afcdfaf15c95ab93386ee4c5973e77e847",
                    "width": 960,
                    "height": 519
                  },
                  {
                    "url": "https://external-preview.redd.it/nQX0-UoCtJiU2eW5zsCqxIc-FAH9ztZJssnGtQQnoIs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=442be51af5dfd2d0cef1968944b6fc31f7c18869",
                    "width": 1080,
                    "height": 584
                  }
                ],
                "variants": {},
                "id": "683I-0ytghVk2NVb0EOGH9-hLx1TI5nEXbwaF7siSVU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c0o4s2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "drc1728",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0o4s2/masking_pii_with_stateful_service_composition_and/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://infinyon.com/blog/2024/04/mask-pii-stateful-streaming",
          "subreddit_subscribers": 176091,
          "created_utc": 1712762394.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "We have this Kafka-to-Oracle pipeline, where multiple Spark Streaming apps, one for each topic, are being launched to retrieve data, process it and write to Oracle. Each topic has its own configuration file (.py with some dictionary) with target table, partition column, processing function, etc. At first, there were a handful of them, but now there are many dozens. What are some ways to organize all of these?\n1) Most proper one\n2) Least painful one\n",
          "author_fullname": "t2_5jfn3csf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to organize many config files?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0lnss",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712755892.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have this Kafka-to-Oracle pipeline, where multiple Spark Streaming apps, one for each topic, are being launched to retrieve data, process it and write to Oracle. Each topic has its own configuration file (.py with some dictionary) with target table, partition column, processing function, etc. At first, there were a handful of them, but now there are many dozens. What are some ways to organize all of these?\n1) Most proper one\n2) Least painful one&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0lnss",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "uselesstrashhh",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0lnss/how_to_organize_many_config_files/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0lnss/how_to_organize_many_config_files/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712755892.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hey all,\n\nI'm working on a project where we have a VM that runs airflow in a docker container. The dags are found directly on the VM.\n\nWhen I joined the project, everything is done directly in production, many obvious problems with that. While I'd like to implement reliable long-term solutions, I have to make due with the little time I have on the project.\n\n**My idea:** I'd like to have a github repo on which the data analysts can push changes to airflow dags. I want to implement a CI/CD pipeline that will run basic tests, then I'd like to automatically pull from the repo into the clone on the VM any time that someone pushes to this repo.\n\nIn a prior company, we used ansible for this. But I didn't touch ansible at the time. I'm confused as to:\n\n* how to set up SSH keys on VM to interact with the github repo\n* how to automatically pull on the repo clone on the VM, any time someone pushes to the remote repo. I don't want to have to run any commands myself on the VM as that defeats the whole purpose\n\nI've gone through many tutorials/videos but can't seem to get closer to what I'm trying to accomplish. Could someone point me in the right direction? In case it wasn't obvious, for context, I know some of what ansible can do, I know it can be used for this use case, but I otherwise have 0 experience with ansible. Thanks!",
          "author_fullname": "t2_gcq3x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "using ansible to sync git repo on VM with remote host",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0kywe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712753918.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a project where we have a VM that runs airflow in a docker container. The dags are found directly on the VM.&lt;/p&gt;\n\n&lt;p&gt;When I joined the project, everything is done directly in production, many obvious problems with that. While I&amp;#39;d like to implement reliable long-term solutions, I have to make due with the little time I have on the project.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My idea:&lt;/strong&gt; I&amp;#39;d like to have a github repo on which the data analysts can push changes to airflow dags. I want to implement a CI/CD pipeline that will run basic tests, then I&amp;#39;d like to automatically pull from the repo into the clone on the VM any time that someone pushes to this repo.&lt;/p&gt;\n\n&lt;p&gt;In a prior company, we used ansible for this. But I didn&amp;#39;t touch ansible at the time. I&amp;#39;m confused as to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;how to set up SSH keys on VM to interact with the github repo&lt;/li&gt;\n&lt;li&gt;how to automatically pull on the repo clone on the VM, any time someone pushes to the remote repo. I don&amp;#39;t want to have to run any commands myself on the VM as that defeats the whole purpose&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve gone through many tutorials/videos but can&amp;#39;t seem to get closer to what I&amp;#39;m trying to accomplish. Could someone point me in the right direction? In case it wasn&amp;#39;t obvious, for context, I know some of what ansible can do, I know it can be used for this use case, but I otherwise have 0 experience with ansible. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0kywe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "johnsonfrusciante",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0kywe/using_ansible_to_sync_git_repo_on_vm_with_remote/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0kywe/using_ansible_to_sync_git_repo_on_vm_with_remote/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712753918.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I need to rant. I have moved through all parts of the field of data from data governance to front end and back end to data lake to ML and Visualizations. My biggest problem is I am the only one in my very small company that gets this all and that's not an exaggeration. So, it's up to me to be architectect, developer, and teacher. I have learned more than I could ever imagine at an incredible speed. However, my environment is in the stone age where Excel spreadsheets are life, and SharePoint is advance and they think of it as a database substitute. Any database I come across is clearly made from someone watching a short show to video on database setup and no design. I have gotten to the point where I can not do it all in setting up and guiding clients. I know you guys are thinking \"run\" but I want for my current company and client to be successful . I have also put my all into this and they really look to me as a SME but politics get in the way of any progress. I feel like there is no progress and I'm just sitting here learning what could be until the day they decide to get with the time's. It's also hilarious that my clients talks about using AI/ML.",
          "author_fullname": "t2_8yface1u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rant of a data professional",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0k5xk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712751475.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to rant. I have moved through all parts of the field of data from data governance to front end and back end to data lake to ML and Visualizations. My biggest problem is I am the only one in my very small company that gets this all and that&amp;#39;s not an exaggeration. So, it&amp;#39;s up to me to be architectect, developer, and teacher. I have learned more than I could ever imagine at an incredible speed. However, my environment is in the stone age where Excel spreadsheets are life, and SharePoint is advance and they think of it as a database substitute. Any database I come across is clearly made from someone watching a short show to video on database setup and no design. I have gotten to the point where I can not do it all in setting up and guiding clients. I know you guys are thinking &amp;quot;run&amp;quot; but I want for my current company and client to be successful . I have also put my all into this and they really look to me as a SME but politics get in the way of any progress. I feel like there is no progress and I&amp;#39;m just sitting here learning what could be until the day they decide to get with the time&amp;#39;s. It&amp;#39;s also hilarious that my clients talks about using AI/ML.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0k5xk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Benmagz",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0k5xk/rant_of_a_data_professional/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0k5xk/rant_of_a_data_professional/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712751475.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi everyone, i have some questions regarding setting up my data model.\n\nThis company works with excel questionnaries which they send to consulting companies. These questionnaries are filled with questions regarding functionalities of the ERP system which the customer should or must have and are personalized for each customer.\n\nThe columns of the excel are as followed:\n\nQuestion id, category, subcategory, question, moscow score and solution (add-on, not possible etc.).\n\nI created a few dimensions like dimCustomer, dimSupplier(consulting company), dimSolution, dimMoscow and dimQuestion. I also have a fact table factAnswers where i have put all the primary keys of my dimensions. Most of these questions of the questionnaire are the same for different consulting companies but some can be different. So in my dimQuestion i have put: question id, question, category, subcategory. Is it necessary to also put a customerId in the dimQuestion to know which question belongs to which customer or could i fix this another way?\n\nThanks \n\n(a noob)",
          "author_fullname": "t2_1jme8wyb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Setting up a data model",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0k56s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712751406.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, i have some questions regarding setting up my data model.&lt;/p&gt;\n\n&lt;p&gt;This company works with excel questionnaries which they send to consulting companies. These questionnaries are filled with questions regarding functionalities of the ERP system which the customer should or must have and are personalized for each customer.&lt;/p&gt;\n\n&lt;p&gt;The columns of the excel are as followed:&lt;/p&gt;\n\n&lt;p&gt;Question id, category, subcategory, question, moscow score and solution (add-on, not possible etc.).&lt;/p&gt;\n\n&lt;p&gt;I created a few dimensions like dimCustomer, dimSupplier(consulting company), dimSolution, dimMoscow and dimQuestion. I also have a fact table factAnswers where i have put all the primary keys of my dimensions. Most of these questions of the questionnaire are the same for different consulting companies but some can be different. So in my dimQuestion i have put: question id, question, category, subcategory. Is it necessary to also put a customerId in the dimQuestion to know which question belongs to which customer or could i fix this another way?&lt;/p&gt;\n\n&lt;p&gt;Thanks &lt;/p&gt;\n\n&lt;p&gt;(a noob)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0k56s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ProfessionalReport99",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0k56s/setting_up_a_data_model/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0k56s/setting_up_a_data_model/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712751406.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "hi team,\n\nas data engineer i got a request to setup code repo, setup up development and prod environment and automate deployment of our code. I have never done anything like that so i am all new to this :D.\n\nWe need to keep track of changes we make to tables,views,sprocs,tasks definitions(sqls) etc.\n\n&amp;#x200B;\n\n**Code Repo** \n\nGithub enterprise. companys choice.\n\nI already got this set i hope :P\n\nI extracted all DDLS from Snowflake using pythong and could load it easily to github.\n\n&amp;#x200B;\n\n**Setting up dev environment in snowflake**\n\nI have no clue how to do it best way......\n\nMy assumption is the best would be somehow to clone entire account / tenant to DEV server this way all reference to objects in sql like ' select \\* from db\\_name.schema\\_name.table\\_name would remain as in prod.  Maybe periodicaly nuke it and resync from prod every 24h **How to set it up? Any clue?**\n\n2nd option is to clone database on a side with '\\_dev' suffix and do stuff there. Once developed and tested, upload the code itself to github and then push on prod.\n\n&amp;#x200B;\n\n**Automating deployment**\n\nI think git hub actions could do that per YT and setup is rather simple\n\n&amp;#x200B;\n\nId like to keep it as simple as possible.\n\nDo you have any assumption how to do it any easier better way?\n\nThanks!\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;",
          "author_fullname": "t2_do9wxbfu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "snowflake ci/cd",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0jvi4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712750566.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi team,&lt;/p&gt;\n\n&lt;p&gt;as data engineer i got a request to setup code repo, setup up development and prod environment and automate deployment of our code. I have never done anything like that so i am all new to this :D.&lt;/p&gt;\n\n&lt;p&gt;We need to keep track of changes we make to tables,views,sprocs,tasks definitions(sqls) etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Code Repo&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;Github enterprise. companys choice.&lt;/p&gt;\n\n&lt;p&gt;I already got this set i hope :P&lt;/p&gt;\n\n&lt;p&gt;I extracted all DDLS from Snowflake using pythong and could load it easily to github.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Setting up dev environment in snowflake&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have no clue how to do it best way......&lt;/p&gt;\n\n&lt;p&gt;My assumption is the best would be somehow to clone entire account / tenant to DEV server this way all reference to objects in sql like &amp;#39; select * from db_name.schema_name.table_name would remain as in prod.  Maybe periodicaly nuke it and resync from prod every 24h &lt;strong&gt;How to set it up? Any clue?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;2nd option is to clone database on a side with &amp;#39;_dev&amp;#39; suffix and do stuff there. Once developed and tested, upload the code itself to github and then push on prod.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Automating deployment&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think git hub actions could do that per YT and setup is rather simple&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Id like to keep it as simple as possible.&lt;/p&gt;\n\n&lt;p&gt;Do you have any assumption how to do it any easier better way?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0jvi4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "87keicam",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0jvi4/snowflake_cicd/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0jvi4/snowflake_cicd/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712750566.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "",
          "author_fullname": "t2_1c6f704",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Taps &amp; Targets: Simplify ETL Through Singer's Data Pipeline Blueprint",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0j83e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": "transparent",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/NjO_P6gYTf3gFIiERfEpNsXkErD02D-PIC0gLskK7gs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712748333.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "datagibberish.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://datagibberish.com/p/singer-101?r=odlo3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p2Cyz4Y3Xouywi81acSUC6H4oyNUew9U5r03l8aeu_g.jpg?auto=webp&amp;s=ec60070e28d98df67a2f7af529dbfeb4566ed86f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p2Cyz4Y3Xouywi81acSUC6H4oyNUew9U5r03l8aeu_g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4b571b75430eaa8ceff2c4f9ab1918b4786c953",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/p2Cyz4Y3Xouywi81acSUC6H4oyNUew9U5r03l8aeu_g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=89f7642d5db9fd9da978532279628b80135ea126",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/p2Cyz4Y3Xouywi81acSUC6H4oyNUew9U5r03l8aeu_g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=834d611335621ad7d4f262dad069b6bd822e029a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/p2Cyz4Y3Xouywi81acSUC6H4oyNUew9U5r03l8aeu_g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ac3bdf4060a2b7cff174ea6e003dc510558c826",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/p2Cyz4Y3Xouywi81acSUC6H4oyNUew9U5r03l8aeu_g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e585026243629e519ffa15ef54923ec6bb8246af",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/p2Cyz4Y3Xouywi81acSUC6H4oyNUew9U5r03l8aeu_g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7fa811a70e2726310e7ef03b4ab1db4cd11548f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "96OQYPE0BjREOzWj6UQX1bH1eZx4S6qBmNRiiHAVSB4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Data Engineering Manager",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c0j83e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ivanovyordan",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/dataengineering/comments/1c0j83e/taps_targets_simplify_etl_through_singers_data/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://datagibberish.com/p/singer-101?r=odlo3",
          "subreddit_subscribers": 176091,
          "created_utc": 1712748333.0,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hello,\n\nI've built an ETL (Extract, Transform, Load) pipeline that extracts data from Sofascore and loads it into BigQuery. I've tried to follow best practices in developing this pipeline, but I'm unsure how well I've implemented them. Could you please review my project and provide feedback on what I can improve, as well as any advice for future projects?\n\n[Sofascore ETL](https://github.com/Aysr01/sofa_score_scraper)",
          "author_fullname": "t2_b95a7eew",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help me with my etl system",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0ioz8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Personal Project Showcase",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1712746807.0,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712746379.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built an ETL (Extract, Transform, Load) pipeline that extracts data from Sofascore and loads it into BigQuery. I&amp;#39;ve tried to follow best practices in developing this pipeline, but I&amp;#39;m unsure how well I&amp;#39;ve implemented them. Could you please review my project and provide feedback on what I can improve, as well as any advice for future projects?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Aysr01/sofa_score_scraper\"&gt;Sofascore ETL&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/068lAfl3MTi_q4ZxwD2-ytCulbnvao_-DkSCZ7Xqyc0.jpg?auto=webp&amp;s=9254b7c01fd5436bdc854713f2f9a075f12100e6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/068lAfl3MTi_q4ZxwD2-ytCulbnvao_-DkSCZ7Xqyc0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=872bf8dfd2f42c0aa0bffe7b9a310e113dc6024d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/068lAfl3MTi_q4ZxwD2-ytCulbnvao_-DkSCZ7Xqyc0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eedaff1bc3ccfe9b3f8499505c03a463a3fed11b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/068lAfl3MTi_q4ZxwD2-ytCulbnvao_-DkSCZ7Xqyc0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5403dfc995fcaaec268496ce64496e982c94232",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/068lAfl3MTi_q4ZxwD2-ytCulbnvao_-DkSCZ7Xqyc0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c22b5a11a7c76dc6fad4699110bdff903ee9e89",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/068lAfl3MTi_q4ZxwD2-ytCulbnvao_-DkSCZ7Xqyc0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d8a3f5dc31d608ee765e681774188ed4742b73b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/068lAfl3MTi_q4ZxwD2-ytCulbnvao_-DkSCZ7Xqyc0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b23750ce123ca6b69b963b0f055dbb32d134bc58",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "xlBXZcQf991-RBif4COKsydcYJVSWXaISBSb1ajYVhw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ddbd37",
          "id": "1c0ioz8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ordinary_Run_2513",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0ioz8/help_me_with_my_etl_system/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0ioz8/help_me_with_my_etl_system/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712746379.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Good day everyone!\n\nLike everyone else, I've been following duckdb's hype and comparisons with Pandas. I have recently started working with data with Pandas, but I would like to know everyone's opinion following the latest trends. You often read about how learning SQL is a better investment than Pandas, and seeing as duckdb uses queries, I wanted to know how everyone employs duckdb in their daily programming tasks.\n\nIn my case, I get information from a database, create a dataframe, and play around with it until I programmatically complete the task to automate it. \n\nUsing duckdb, would that translate to using duckdb, pandas, and python functions or am I missing something?\n\nThank you for sharing your thoughts!",
          "author_fullname": "t2_1v6ggili",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Programming with DuckDB vs Pandas",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0hdho",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712741034.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good day everyone!&lt;/p&gt;\n\n&lt;p&gt;Like everyone else, I&amp;#39;ve been following duckdb&amp;#39;s hype and comparisons with Pandas. I have recently started working with data with Pandas, but I would like to know everyone&amp;#39;s opinion following the latest trends. You often read about how learning SQL is a better investment than Pandas, and seeing as duckdb uses queries, I wanted to know how everyone employs duckdb in their daily programming tasks.&lt;/p&gt;\n\n&lt;p&gt;In my case, I get information from a database, create a dataframe, and play around with it until I programmatically complete the task to automate it. &lt;/p&gt;\n\n&lt;p&gt;Using duckdb, would that translate to using duckdb, pandas, and python functions or am I missing something?&lt;/p&gt;\n\n&lt;p&gt;Thank you for sharing your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0hdho",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CharacterScience",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0hdho/programming_with_duckdb_vs_pandas/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0hdho/programming_with_duckdb_vs_pandas/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712741034.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "So I joined a certain consulting firm as a Graduate in 2022, however I have yet to do any data engineering work despite them calling me a data engineer. The projects I've been on, I've either done nothing or it's extremely simple SQL. It essentially feels like I've been on bench for the entire time I've been here. \n\nI haven't wasted my time however, been upskilling in Python, AWS, Azure, SQL, Snowflake, Kafka, Databricks. I've been trying to apply for other jobs, but naturally my lack of actual DE experience is holding me back.\n\nWhat are some possible moves I could take to help me move jobs?",
          "author_fullname": "t2_14b7wo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "1 and a half years with no proper experience",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0gzxt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 38,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 38,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712739375.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I joined a certain consulting firm as a Graduate in 2022, however I have yet to do any data engineering work despite them calling me a data engineer. The projects I&amp;#39;ve been on, I&amp;#39;ve either done nothing or it&amp;#39;s extremely simple SQL. It essentially feels like I&amp;#39;ve been on bench for the entire time I&amp;#39;ve been here. &lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t wasted my time however, been upskilling in Python, AWS, Azure, SQL, Snowflake, Kafka, Databricks. I&amp;#39;ve been trying to apply for other jobs, but naturally my lack of actual DE experience is holding me back.&lt;/p&gt;\n\n&lt;p&gt;What are some possible moves I could take to help me move jobs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0gzxt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Solid-Sloth",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0gzxt/1_and_a_half_years_with_no_proper_experience/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0gzxt/1_and_a_half_years_with_no_proper_experience/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712739375.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi,\n\nI have a Synapse pipeline that starts with a \"set variable\" activity, and after that immediately triggers 2 notebook activities that should run in parallel.\n\nBoth these notebooks have some logging logic but I notice in the logging that notebook1 does all the work first, and after that everything from notebook2 comes in.\n\nI see in the pipeline that these notebooks indeed start at the exact same time, but don't seem to be actually doing things at the same time.\n\n&amp;#x200B;\n\nAnyone know what i'm doing wrong? (Both notebooks use the same spark pool)",
          "author_fullname": "t2_16t847",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Synapse notebooks don't run in parallel",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0gboa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712736361.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have a Synapse pipeline that starts with a &amp;quot;set variable&amp;quot; activity, and after that immediately triggers 2 notebook activities that should run in parallel.&lt;/p&gt;\n\n&lt;p&gt;Both these notebooks have some logging logic but I notice in the logging that notebook1 does all the work first, and after that everything from notebook2 comes in.&lt;/p&gt;\n\n&lt;p&gt;I see in the pipeline that these notebooks indeed start at the exact same time, but don&amp;#39;t seem to be actually doing things at the same time.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyone know what i&amp;#39;m doing wrong? (Both notebooks use the same spark pool)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0gboa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "drollerfoot7",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0gboa/synapse_notebooks_dont_run_in_parallel/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0gboa/synapse_notebooks_dont_run_in_parallel/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712736361.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hey there,\n\nDoes anyone know of proven metrics for measuring the complexity of schemas/databases (Oracle in my example)?\n\nThanks",
          "author_fullname": "t2_2y99k833",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Schema / DB Complexity Metrics",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0ffed",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712732570.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of proven metrics for measuring the complexity of schemas/databases (Oracle in my example)?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c0ffed",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "schubidubidubid",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0ffed/schema_db_complexity_metrics/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0ffed/schema_db_complexity_metrics/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712732570.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi guys, \n\nBoth me and my friend are looking for a job. We have decided to do some projects. We are trained as data engineers in Azure but got no professional experience. \n\nI want to make 3 repos belonging to the 3 projects. What information should I give in such repos? I mean if it's pyspark, I can just give the notebooks. \n\nBut what to do if it's a project that uses ADF? Suppose the project is to make a simple data pipeline. ",
          "author_fullname": "t2_9peiwedk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need some help regarding data engineering projects...",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0ca07",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712721185.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;Both me and my friend are looking for a job. We have decided to do some projects. We are trained as data engineers in Azure but got no professional experience. &lt;/p&gt;\n\n&lt;p&gt;I want to make 3 repos belonging to the 3 projects. What information should I give in such repos? I mean if it&amp;#39;s pyspark, I can just give the notebooks. &lt;/p&gt;\n\n&lt;p&gt;But what to do if it&amp;#39;s a project that uses ADF? Suppose the project is to make a simple data pipeline. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c0ca07",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LucaMarko",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0ca07/need_some_help_regarding_data_engineering_projects/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0ca07/need_some_help_regarding_data_engineering_projects/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712721185.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I hope this is not a recycled post, and if this is the wrong sub I will delete and place it in the correct one. This sub has been so helpful in my growth in self learning when posting my personal projects. I am finally looking for a my first position, I have self taught experience in SQL, Python, ETL and PowerBi implemented all in my projects. I am looking for just an entry level ETL position, coming from IT support into a totally new profession. I obviously don\u2019t feel qualified at all when reading what all is needed but I understand this is a profession where you\u2019re always learning. For those with jobs in data engineering or ETL, what suggestions/advice would you have for me while applying? \n\nTIA all. ",
          "author_fullname": "t2_8txv38ph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for my first job..",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c0ascf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712716630.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope this is not a recycled post, and if this is the wrong sub I will delete and place it in the correct one. This sub has been so helpful in my growth in self learning when posting my personal projects. I am finally looking for a my first position, I have self taught experience in SQL, Python, ETL and PowerBi implemented all in my projects. I am looking for just an entry level ETL position, coming from IT support into a totally new profession. I obviously don\u2019t feel qualified at all when reading what all is needed but I understand this is a profession where you\u2019re always learning. For those with jobs in data engineering or ETL, what suggestions/advice would you have for me while applying? &lt;/p&gt;\n\n&lt;p&gt;TIA all. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c0ascf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fraiz24",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c0ascf/looking_for_my_first_job/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c0ascf/looking_for_my_first_job/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712716630.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Sorry this ended up being so long.\n\nI graduated with a master's in biomedical informatics over 5 years ago. Didn't even want to do it, I was just told my plan to get another bachelor's (in CS) was stupid and I thought this would help me transition into healthcare IT/data. I have a bachelor's in biochemistry. I graduated with a high GPA in both (around 4.0), but didn't do much. I was depressed, withdrawn, and ended up working dead end job for a while. Didn't bother with medical school, healthcare IT, or any decent option out there that I had open to me. I could write extensively about this whole situation, but it's probably not a good idea...  \n\n\nThrough some luck I managed to snag a role a little over a year ago as a data analyst, but not in healthcare like most of the people I know who studied informatics. However, I've been looking to leave the company for a while due to various reasons, one being that I'm not getting enough work and the promised opportunities to get more into data engineering and working with the actual data team never happened and likely won't happen.  \n\n\nI'm already finding it difficult to get another job, likely due to the market. However, I also strongly believe it's because of my donut hole post college and getting a master's in something that's too \"niche\" or useless. Heck, I can't even select \"informatics\" as a field of study for some applications. Even healthcare IT places aren't interested in hiring me at this point.  \n\n\nMy main goal is to try to transition to data engineer, but I'm not even sure if I have a future in this industry (that's not even getting into AI or saturation just based on my current situation alone). I've thought about doing an online CS program (OSU or WGU) to help me but is this a good idea? With my current job (remote) I have the time to do work on the side plus I don't commute, so I feel like I'm overthinking this and could do it anyway to boost my chances. Just afraid about the cost.  Feeling lost. Feels like years of anxiety ruined my life and things just keep piling on.",
          "author_fullname": "t2_ca3nw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Current analyst wanting to go into data engineering. Am I in a bad situation?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c09xqn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712714204.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry this ended up being so long.&lt;/p&gt;\n\n&lt;p&gt;I graduated with a master&amp;#39;s in biomedical informatics over 5 years ago. Didn&amp;#39;t even want to do it, I was just told my plan to get another bachelor&amp;#39;s (in CS) was stupid and I thought this would help me transition into healthcare IT/data. I have a bachelor&amp;#39;s in biochemistry. I graduated with a high GPA in both (around 4.0), but didn&amp;#39;t do much. I was depressed, withdrawn, and ended up working dead end job for a while. Didn&amp;#39;t bother with medical school, healthcare IT, or any decent option out there that I had open to me. I could write extensively about this whole situation, but it&amp;#39;s probably not a good idea...  &lt;/p&gt;\n\n&lt;p&gt;Through some luck I managed to snag a role a little over a year ago as a data analyst, but not in healthcare like most of the people I know who studied informatics. However, I&amp;#39;ve been looking to leave the company for a while due to various reasons, one being that I&amp;#39;m not getting enough work and the promised opportunities to get more into data engineering and working with the actual data team never happened and likely won&amp;#39;t happen.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m already finding it difficult to get another job, likely due to the market. However, I also strongly believe it&amp;#39;s because of my donut hole post college and getting a master&amp;#39;s in something that&amp;#39;s too &amp;quot;niche&amp;quot; or useless. Heck, I can&amp;#39;t even select &amp;quot;informatics&amp;quot; as a field of study for some applications. Even healthcare IT places aren&amp;#39;t interested in hiring me at this point.  &lt;/p&gt;\n\n&lt;p&gt;My main goal is to try to transition to data engineer, but I&amp;#39;m not even sure if I have a future in this industry (that&amp;#39;s not even getting into AI or saturation just based on my current situation alone). I&amp;#39;ve thought about doing an online CS program (OSU or WGU) to help me but is this a good idea? With my current job (remote) I have the time to do work on the side plus I don&amp;#39;t commute, so I feel like I&amp;#39;m overthinking this and could do it anyway to boost my chances. Just afraid about the cost.  Feeling lost. Feels like years of anxiety ruined my life and things just keep piling on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c09xqn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "veganmeat",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c09xqn/current_analyst_wanting_to_go_into_data/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c09xqn/current_analyst_wanting_to_go_into_data/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712714204.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I've been using Cloud Function for years, but to this day it gives me a lot of work to do in a basic way: receive a request, process a request, return a request.\n\nIt is a very massive and repetitive activity that is always configured, whether with a pubsub, a request, etc.\n\nToday my time has a high level of rework and wasted time configuring new serverless codes in Cloud Function, so I ask:\n\nIs there any low-code or more practical solution for quickly configuring new serverless solutions?\n\nCan be other plataform.",
          "author_fullname": "t2_ahryr1kv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Alternative for Cloud Function?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c09o99",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712713435.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using Cloud Function for years, but to this day it gives me a lot of work to do in a basic way: receive a request, process a request, return a request.&lt;/p&gt;\n\n&lt;p&gt;It is a very massive and repetitive activity that is always configured, whether with a pubsub, a request, etc.&lt;/p&gt;\n\n&lt;p&gt;Today my time has a high level of rework and wasted time configuring new serverless codes in Cloud Function, so I ask:&lt;/p&gt;\n\n&lt;p&gt;Is there any low-code or more practical solution for quickly configuring new serverless solutions?&lt;/p&gt;\n\n&lt;p&gt;Can be other plataform.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c09o99",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Koninhooz",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c09o99/alternative_for_cloud_function/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c09o99/alternative_for_cloud_function/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712713435.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I recently came across many senior business intelligence engineer roles at Amazon that specifically call outs ETL with multi-TB datasets.  \n\nThe posting doesn\u2019t talk about Hadoop/distributed computing.  \n\nCan I get insight from people that have worked with datasets that large and talk about what your day to day skillsets are?  \n\nThe largest I\u2019ve worked with was about 500GB and used partitions.",
          "author_fullname": "t2_oooz0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Experience in programming, extracting, cleaning multi-TB datasets",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c09748",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712712062.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently came across many senior business intelligence engineer roles at Amazon that specifically call outs ETL with multi-TB datasets.  &lt;/p&gt;\n\n&lt;p&gt;The posting doesn\u2019t talk about Hadoop/distributed computing.  &lt;/p&gt;\n\n&lt;p&gt;Can I get insight from people that have worked with datasets that large and talk about what your day to day skillsets are?  &lt;/p&gt;\n\n&lt;p&gt;The largest I\u2019ve worked with was about 500GB and used partitions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c09748",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UrbanCrusader24",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c09748/experience_in_programming_extracting_cleaning/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c09748/experience_in_programming_extracting_cleaning/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712712062.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "# Expected Result\n\nNot sure what the proper sub is for this, but I'm experimenting with using polars for unnesting data and was curious if there is a better way to do this operation:  \n\n\nI am trying to use the `explode` function on a column with a dtype of `List`. Ideally, I'd like to add an index for that list (ultimately in service of forming a composite key) prior to `explode`ing it. \n\nI want to go from this:\n\n    print(df)\n    \n    shape: (3, 3)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2502\n    \u2502 ---            \u2506 ---          \u2506 ---                \u2502\n    \u2502 i64            \u2506 f64          \u2506 list[i64]          \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 6              \u2506 0.054997     \u2506 [50]               \u2502\n    \u2502 16             \u2506 0.709144     \u2506 [15, 49, 20]       \u2502\n    \u2502 65             \u2506 0.164574     \u2506 [18]               \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nto this:\n\n    print(df_unnest)\n    \n    shape: (5, 4)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2506 index \u2502\n    \u2502 ---            \u2506 ---          \u2506 ---                \u2506 ---   \u2502\n    \u2502 i64            \u2506 f64          \u2506 i64                \u2506 i64   \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 6              \u2506 0.054997     \u2506 50                 \u2506 0     \u2502\n    \u2502 16             \u2506 0.709144     \u2506 15                 \u2506 0     \u2502\n    \u2502 16             \u2506 0.709144     \u2506 49                 \u2506 1     \u2502\n    \u2502 16             \u2506 0.709144     \u2506 20                 \u2506 2     \u2502\n    \u2502 65             \u2506 0.164574     \u2506 18                 \u2506 0     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Problem\n\nI don't know how/if it's possible to do this with built in expressions for efficiency's sake. The only way I've figured out how to accomplish this is: \n\n1. using the `map_elements` function (which I want to avoid for performance reasons) \n\n    df_unnest = df.select(\n        pl.all(), \n        pl.col('Nested List Column')\\\n            .map_elements(\n                lambda x: [x for x in range(len(x))], return_dtype=pl.List(pl.Int64)\n            ).alias(\"index\")).explode(['Nested List Column', 'index']).sort('Integer Column',)\n    print(df_unnest)\n    \n    shape: (5, 4)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2506 index \u2502\n    \u2502 ---            \u2506 ---          \u2506 ---                \u2506 ---   \u2502\n    \u2502 i64            \u2506 f64          \u2506 i64                \u2506 i64   \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 6              \u2506 0.054997     \u2506 50                 \u2506 0     \u2502\n    \u2502 16             \u2506 0.709144     \u2506 15                 \u2506 0     \u2502\n    \u2502 16             \u2506 0.709144     \u2506 49                 \u2506 1     \u2502\n    \u2502 16             \u2506 0.709144     \u2506 20                 \u2506 2     \u2502\n    \u2502 65             \u2506 0.164574     \u2506 18                 \u2506 0     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2. `explode`ing it -&gt; creating an \"index\" -&gt; `group_by`ing the non-exploded columns while ranking the \"index\" -&gt; and then `explode`ing again on the list and index list (which likely won't produce a reproducible result and is likely also not performant). I've included the examples below. \n\n    df_explode_group = df\\\n        .explode('Nested List Column')\\\n        .with_row_index('index')\\\n        .group_by(pl.all().exclude(['Nested List Column', 'index']))\\\n        .agg(pl.col('Nested List Column'), pl.col('index').rank('ordinal') - 1).sort('Integer Column')\n    print(df_explode_group)\n    \n    shape: (3, 4)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2506 index     \u2502\n    \u2502 ---            \u2506 ---          \u2506 ---                \u2506 ---       \u2502\n    \u2502 i64            \u2506 f64          \u2506 list[i64]          \u2506 list[u32] \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 6              \u2506 0.054997     \u2506 [50]               \u2506 [0]       \u2502\n    \u2502 16             \u2506 0.709144     \u2506 [15, 49, 20]       \u2506 [0, 1, 2] \u2502\n    \u2502 65             \u2506 0.164574     \u2506 [18]               \u2506 [0]       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \n    df_explode_group_explode = df_explode_group\\\n        .explode(pl.col(['Nested List Column','index']))\\\n        .sort('Integer Column')\n    print(df_explode_group_explode)\n    \n    shape: (5, 4)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2506 index \u2502\n    \u2502 ---            \u2506 ---          \u2506 ---                \u2506 ---   \u2502\n    \u2502 i64            \u2506 f64          \u2506 i64                \u2506 u32   \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 6              \u2506 0.054997     \u2506 50                 \u2506 0     \u2502\n    \u2502 16             \u2506 0.709144     \u2506 15                 \u2506 0     \u2502\n    \u2502 16             \u2506 0.709144     \u2506 49                 \u2506 1     \u2502\n    \u2502 16             \u2506 0.709144     \u2506 20                 \u2506 2     \u2502\n    \u2502 65             \u2506 0.164574     \u2506 18                 \u2506 0     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAny thoughts or discussion would be helpful!\n\n&amp;#x200B;",
          "author_fullname": "t2_15uzwf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Experimenting with Polars: Indexing Exploding Columns",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c094p3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712711878.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Expected Result&lt;/h1&gt;\n\n&lt;p&gt;Not sure what the proper sub is for this, but I&amp;#39;m experimenting with using polars for unnesting data and was curious if there is a better way to do this operation:  &lt;/p&gt;\n\n&lt;p&gt;I am trying to use the &lt;code&gt;explode&lt;/code&gt; function on a column with a dtype of &lt;code&gt;List&lt;/code&gt;. Ideally, I&amp;#39;d like to add an index for that list (ultimately in service of forming a composite key) prior to &lt;code&gt;explode&lt;/code&gt;ing it. &lt;/p&gt;\n\n&lt;p&gt;I want to go from this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;print(df)\n\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2502\n\u2502 ---            \u2506 ---          \u2506 ---                \u2502\n\u2502 i64            \u2506 f64          \u2506 list[i64]          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6              \u2506 0.054997     \u2506 [50]               \u2502\n\u2502 16             \u2506 0.709144     \u2506 [15, 49, 20]       \u2502\n\u2502 65             \u2506 0.164574     \u2506 [18]               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;to this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;print(df_unnest)\n\nshape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2506 index \u2502\n\u2502 ---            \u2506 ---          \u2506 ---                \u2506 ---   \u2502\n\u2502 i64            \u2506 f64          \u2506 i64                \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6              \u2506 0.054997     \u2506 50                 \u2506 0     \u2502\n\u2502 16             \u2506 0.709144     \u2506 15                 \u2506 0     \u2502\n\u2502 16             \u2506 0.709144     \u2506 49                 \u2506 1     \u2502\n\u2502 16             \u2506 0.709144     \u2506 20                 \u2506 2     \u2502\n\u2502 65             \u2506 0.164574     \u2506 18                 \u2506 0     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Problem&lt;/h1&gt;\n\n&lt;p&gt;I don&amp;#39;t know how/if it&amp;#39;s possible to do this with built in expressions for efficiency&amp;#39;s sake. The only way I&amp;#39;ve figured out how to accomplish this is: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;using the &lt;code&gt;map_elements&lt;/code&gt; function (which I want to avoid for performance reasons) &lt;/p&gt;\n\n&lt;p&gt;df_unnest = df.select(\n    pl.all(), \n    pl.col(&amp;#39;Nested List Column&amp;#39;)\\\n        .map_elements(\n            lambda x: [x for x in range(len(x))], return_dtype=pl.List(pl.Int64)\n        ).alias(&amp;quot;index&amp;quot;)).explode([&amp;#39;Nested List Column&amp;#39;, &amp;#39;index&amp;#39;]).sort(&amp;#39;Integer Column&amp;#39;,)\nprint(df_unnest)&lt;/p&gt;\n\n&lt;p&gt;shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2506 index \u2502\n\u2502 ---            \u2506 ---          \u2506 ---                \u2506 ---   \u2502\n\u2502 i64            \u2506 f64          \u2506 i64                \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6              \u2506 0.054997     \u2506 50                 \u2506 0     \u2502\n\u2502 16             \u2506 0.709144     \u2506 15                 \u2506 0     \u2502\n\u2502 16             \u2506 0.709144     \u2506 49                 \u2506 1     \u2502\n\u2502 16             \u2506 0.709144     \u2506 20                 \u2506 2     \u2502\n\u2502 65             \u2506 0.164574     \u2506 18                 \u2506 0     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;code&gt;explode&lt;/code&gt;ing it -&amp;gt; creating an &amp;quot;index&amp;quot; -&amp;gt; &lt;code&gt;group_by&lt;/code&gt;ing the non-exploded columns while ranking the &amp;quot;index&amp;quot; -&amp;gt; and then &lt;code&gt;explode&lt;/code&gt;ing again on the list and index list (which likely won&amp;#39;t produce a reproducible result and is likely also not performant). I&amp;#39;ve included the examples below. &lt;/p&gt;\n\n&lt;p&gt;df_explode_group = df\\\n    .explode(&amp;#39;Nested List Column&amp;#39;)\\\n    .with_row_index(&amp;#39;index&amp;#39;)\\\n    .group_by(pl.all().exclude([&amp;#39;Nested List Column&amp;#39;, &amp;#39;index&amp;#39;]))\\\n    .agg(pl.col(&amp;#39;Nested List Column&amp;#39;), pl.col(&amp;#39;index&amp;#39;).rank(&amp;#39;ordinal&amp;#39;) - 1).sort(&amp;#39;Integer Column&amp;#39;)\nprint(df_explode_group)&lt;/p&gt;\n\n&lt;p&gt;shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2506 index     \u2502\n\u2502 ---            \u2506 ---          \u2506 ---                \u2506 ---       \u2502\n\u2502 i64            \u2506 f64          \u2506 list[i64]          \u2506 list[u32] \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6              \u2506 0.054997     \u2506 [50]               \u2506 [0]       \u2502\n\u2502 16             \u2506 0.709144     \u2506 [15, 49, 20]       \u2506 [0, 1, 2] \u2502\n\u2502 65             \u2506 0.164574     \u2506 [18]               \u2506 [0]       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518&lt;/p&gt;\n\n&lt;p&gt;df_explode_group_explode = df_explode_group\\\n    .explode(pl.col([&amp;#39;Nested List Column&amp;#39;,&amp;#39;index&amp;#39;]))\\\n    .sort(&amp;#39;Integer Column&amp;#39;)\nprint(df_explode_group_explode)&lt;/p&gt;\n\n&lt;p&gt;shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Integer Column \u2506 Float Column \u2506 Nested List Column \u2506 index \u2502\n\u2502 ---            \u2506 ---          \u2506 ---                \u2506 ---   \u2502\n\u2502 i64            \u2506 f64          \u2506 i64                \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6              \u2506 0.054997     \u2506 50                 \u2506 0     \u2502\n\u2502 16             \u2506 0.709144     \u2506 15                 \u2506 0     \u2502\n\u2502 16             \u2506 0.709144     \u2506 49                 \u2506 1     \u2502\n\u2502 16             \u2506 0.709144     \u2506 20                 \u2506 2     \u2502\n\u2502 65             \u2506 0.164574     \u2506 18                 \u2506 0     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any thoughts or discussion would be helpful!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c094p3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EngiNerd9000",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c094p3/experimenting_with_polars_indexing_exploding/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c094p3/experimenting_with_polars_indexing_exploding/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712711878.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "My use case is basically this:\n\nWe have a RDS instance where we have tables of data. We need to make these tables available in our customer's data warehouses for them to do their own queries (read replicas won't work as they need our tables available in the same environment as the rest of their data). We'd be syncing a subset of our tables (and a subset of columns within those) down to our customer's warehouses, including initial loads and also incremental updates. Customers may have different types of warehouses\n\n&amp;#x200B;\n\nThe main things we care about in our solution are:\n\n1. Supports multiple warehouse destination types. most important ones we need to support are Redshift, MS SQL Server, Azure, and Snowflake. Also ideal to support BigQuery but less critical\n2. Syncing would ideally be near real-time but we could also go for something that has a high frequency. Ideally every 15 minutes at worst\n3. Handles schema updates well. We have separate environments for each client so we don't want to go through a major process for each client, each time we add new fields to one of the tables\n4. Provider should be able to handle a decent scale of data. We don't have an insane scale of data but also not tiny. A hypothetical client could have about 400GB of data and 100M rows to be synced. Most of this would be initial load, much less would change incrementally\n5. Cost-effective (e.g. FiveTran pricing is totally outside of our range). \n6. However, we do have limited staff (no dedicated data engineer) so we are not against spending a bit more than a self-hosted solution would cost if it saves us a bunch of time. In fact, we prefer a provider that handles more for us if there is a cost-effective option. We have the skills to be able to set an open-source tool up and self-host it if the other criteria are well met though!\n7. Any monitoring capabilities are a huge plus as we will likely have to provide some proof to customers of accurate syncs. However, this is not terribly difficult to setup if we have to so not as important as the other criteria\n\nWanted to see here if anyone has experience trying any of the tools out there and whether or not you had a good experience with them? Any suggestions that could be ideal for our use case? A few options we've considered are: Prequel, Airbyte (self-hosted), DMS, Debezium (although likely too much lift for our timeline), and Meltano. Thanks in advance for any suggestions you're able to provide!",
          "author_fullname": "t2_678tmzn2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions or Past Experiences with Data Sharing Providers?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c079r5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712706755.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My use case is basically this:&lt;/p&gt;\n\n&lt;p&gt;We have a RDS instance where we have tables of data. We need to make these tables available in our customer&amp;#39;s data warehouses for them to do their own queries (read replicas won&amp;#39;t work as they need our tables available in the same environment as the rest of their data). We&amp;#39;d be syncing a subset of our tables (and a subset of columns within those) down to our customer&amp;#39;s warehouses, including initial loads and also incremental updates. Customers may have different types of warehouses&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The main things we care about in our solution are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Supports multiple warehouse destination types. most important ones we need to support are Redshift, MS SQL Server, Azure, and Snowflake. Also ideal to support BigQuery but less critical&lt;/li&gt;\n&lt;li&gt;Syncing would ideally be near real-time but we could also go for something that has a high frequency. Ideally every 15 minutes at worst&lt;/li&gt;\n&lt;li&gt;Handles schema updates well. We have separate environments for each client so we don&amp;#39;t want to go through a major process for each client, each time we add new fields to one of the tables&lt;/li&gt;\n&lt;li&gt;Provider should be able to handle a decent scale of data. We don&amp;#39;t have an insane scale of data but also not tiny. A hypothetical client could have about 400GB of data and 100M rows to be synced. Most of this would be initial load, much less would change incrementally&lt;/li&gt;\n&lt;li&gt;Cost-effective (e.g. FiveTran pricing is totally outside of our range). &lt;/li&gt;\n&lt;li&gt;However, we do have limited staff (no dedicated data engineer) so we are not against spending a bit more than a self-hosted solution would cost if it saves us a bunch of time. In fact, we prefer a provider that handles more for us if there is a cost-effective option. We have the skills to be able to set an open-source tool up and self-host it if the other criteria are well met though!&lt;/li&gt;\n&lt;li&gt;Any monitoring capabilities are a huge plus as we will likely have to provide some proof to customers of accurate syncs. However, this is not terribly difficult to setup if we have to so not as important as the other criteria&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Wanted to see here if anyone has experience trying any of the tools out there and whether or not you had a good experience with them? Any suggestions that could be ideal for our use case? A few options we&amp;#39;ve considered are: Prequel, Airbyte (self-hosted), DMS, Debezium (although likely too much lift for our timeline), and Meltano. Thanks in advance for any suggestions you&amp;#39;re able to provide!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c079r5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Single_Advisor8900",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c079r5/suggestions_or_past_experiences_with_data_sharing/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c079r5/suggestions_or_past_experiences_with_data_sharing/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712706755.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hello I was just wondering if anyone here is using DBT for data ingest and migration.\n\nMy aim is to migrate data from a postgres server to my BigQuery project using DBT cloud (the git and cloud repo have already and the connection profile has been set-up to BQ) but I'm not exactly sure if I can connect to a Postgres database with this set-up the website mentions a [profile.yml](https://docs.getdbt.com/docs/core/connect-data-platform/postgres-setup) file but this is only for DBT core users\n\nI'm really new to this so any help would be appreciated \n",
          "author_fullname": "t2_fr1r9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DBT Postgres to BigQuery",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c06ha0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712704673.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello I was just wondering if anyone here is using DBT for data ingest and migration.&lt;/p&gt;\n\n&lt;p&gt;My aim is to migrate data from a postgres server to my BigQuery project using DBT cloud (the git and cloud repo have already and the connection profile has been set-up to BQ) but I&amp;#39;m not exactly sure if I can connect to a Postgres database with this set-up the website mentions a &lt;a href=\"https://docs.getdbt.com/docs/core/connect-data-platform/postgres-setup\"&gt;profile.yml&lt;/a&gt; file but this is only for DBT core users&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really new to this so any help would be appreciated &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?auto=webp&amp;s=2a89f01968bbb7160773570a5739ba364e017ebf",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e39c972215449e24ba187a3b3e6d0289aad02d1b",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e48b5b0440098be5b7b54dcdd6d78e80f77e948",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c583ec988ffb5d6d8292b88b38a2a7ac9fc2b799",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a97be3626c69aab79c2204db47f040a6a8bb9820",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ba90b674ccf1906f5a13abd09b27db16d203bd0",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197f95d4689989cecbdb537c3aa18035536b0c50",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "s9XQCWayWEjVSYNiK4ez8RIl3EBcstjT4Cv_3rHuPvk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c06ha0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "solely_magnus",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c06ha0/dbt_postgres_to_bigquery/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c06ha0/dbt_postgres_to_bigquery/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712704673.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Been struggling with this for a while, and now I am wondering if I\u2019m doing something wrong or what I can do better.\n\nContext: I have about 50k parquet files in s3 that need to be grouped and invested into redshift into a handful of tables.\n\nProblem: Files within the same group sometimes have a slightly different schema, but I was able to standardize it by adding missing columns to files that don\u2019t contain them. Also, I explicitly defined datatypes for each group to make sure they\u2019re all in the exact same format. When I issue a copy command to redshift I keep getting a redshift spectrum error of incompatible schemas for given columns. Specifically, it will state that the table is defined with data type X and file has data type Y. Even though, explicitly defined the schema.\n\nAny feedback or ideas how to go about this?\n\nEdit 1: looks like my issue was files that were completely empty. Took those out, and the ingest went smoothly. ",
          "author_fullname": "t2_7e04ujnq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ingesting Parquet Files into Redshift",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c04znf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1712762969.0,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712700972.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been struggling with this for a while, and now I am wondering if I\u2019m doing something wrong or what I can do better.&lt;/p&gt;\n\n&lt;p&gt;Context: I have about 50k parquet files in s3 that need to be grouped and invested into redshift into a handful of tables.&lt;/p&gt;\n\n&lt;p&gt;Problem: Files within the same group sometimes have a slightly different schema, but I was able to standardize it by adding missing columns to files that don\u2019t contain them. Also, I explicitly defined datatypes for each group to make sure they\u2019re all in the exact same format. When I issue a copy command to redshift I keep getting a redshift spectrum error of incompatible schemas for given columns. Specifically, it will state that the table is defined with data type X and file has data type Y. Even though, explicitly defined the schema.&lt;/p&gt;\n\n&lt;p&gt;Any feedback or ideas how to go about this?&lt;/p&gt;\n\n&lt;p&gt;Edit 1: looks like my issue was files that were completely empty. Took those out, and the ingest went smoothly. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Big Data Engineer That Broke All ETLs",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c04znf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatGrayZ",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/dataengineering/comments/1c04znf/ingesting_parquet_files_into_redshift/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c04znf/ingesting_parquet_files_into_redshift/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712700972.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Anyone take the CDMP recently on the new online platform? How was the experience, is it on-demand now or do you still schedule in advance?\n\nThank you!",
          "author_fullname": "t2_e0jtj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CDMP new Platform",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c04yhs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Career",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712700898.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone take the CDMP recently on the new online platform? How was the experience, is it on-demand now or do you still schedule in advance?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#349e48",
          "id": "1c04yhs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BlunderbusDriver",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c04yhs/cdmp_new_platform/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c04yhs/cdmp_new_platform/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712700898.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "I'm looking to get a feel for how folks are utilizing Delta Live Tables to build out their lakehouses and how folks are organizing their workflow. My team and I are working on building out a lakehouse to make our business users lives a little bit easier (i.e. instead of having to hit 3 tables to get all customers for a particular domain, they can rely on a single customer dimension). The goal is to provide a simple star schema for reporting analysts to interface with and for our data scientists to interact with so they can focus on reports and data science rather than fighting the data to get it in the right shape / standard.  \n\n\nThe process that we have built as a first pass is using Delta Live Tables and is a simple truncate and reload (not the best but it was intended as a quick first pass to enable some current work). Now we are working on pivoting to making this work more durable for the long term.\n\n&amp;#x200B;\n\nMy initial plan to organize the data was as follows:  \n1. Raw data is landed via Azure Data Factory in the format /source/table/yyyy/mm/dd and the files will typically be parquet. Files are likely to be an initial full load and subsequent loads would grab the last seven days of updates (per the modified date column).\n\n2. Autoloader will append data to append-only tables and record metadata (input file, ingestion date, etc)\n\n3. Begin Modeling - With the DLT apply\\_changes API, combine necessary tables to create each dimension. \n\n4. Merge into the fact table.\n\n5. Create gold level tables per project needs. If a reporting team needs a star schema with certain filters applied, I can create that at the gold level. If a data scientist needs one big table, I can do this at the gold level.\n\n&amp;#x200B;\n\n1 and 2 are essentially bronze level data. Steps 3 and 4 are where standardization, cleaning, and modeling (star schema) are happening. After watching  [Behind the Hype - The Medallion Architecture Doesn't Work - YouTube](https://www.youtube.com/watch?v=fz4tax6nKZM) , I am questioning if I'm approaching this wrong. The pattern I'm seeing in some other examples (this video included) are:  \n1. Load raw data into directories.\n\n2. Use Autoloader to create append only tables.\n\n3. Use apply\\_changes on all autoloader tables to create an SCD-1 or SCD-2 table per requirements. Apply data cleaning steps here.\n\n4. Create a star schema at the gold level.\n\n&amp;#x200B;\n\nHow are other folks doing this in practice? If the second set of steps, how are you managing bringing together SCD tables into a single dimension table?\n\n&amp;#x200B;\n\nThanks for any help folks can provide!",
          "author_fullname": "t2_a4ilrn30",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Databricks - Lakehouse &amp; Delta Live Tables Guidance",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c03hmy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712697363.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to get a feel for how folks are utilizing Delta Live Tables to build out their lakehouses and how folks are organizing their workflow. My team and I are working on building out a lakehouse to make our business users lives a little bit easier (i.e. instead of having to hit 3 tables to get all customers for a particular domain, they can rely on a single customer dimension). The goal is to provide a simple star schema for reporting analysts to interface with and for our data scientists to interact with so they can focus on reports and data science rather than fighting the data to get it in the right shape / standard.  &lt;/p&gt;\n\n&lt;p&gt;The process that we have built as a first pass is using Delta Live Tables and is a simple truncate and reload (not the best but it was intended as a quick first pass to enable some current work). Now we are working on pivoting to making this work more durable for the long term.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My initial plan to organize the data was as follows:&lt;br/&gt;\n1. Raw data is landed via Azure Data Factory in the format /source/table/yyyy/mm/dd and the files will typically be parquet. Files are likely to be an initial full load and subsequent loads would grab the last seven days of updates (per the modified date column).&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Autoloader will append data to append-only tables and record metadata (input file, ingestion date, etc)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Begin Modeling - With the DLT apply_changes API, combine necessary tables to create each dimension. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Merge into the fact table.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Create gold level tables per project needs. If a reporting team needs a star schema with certain filters applied, I can create that at the gold level. If a data scientist needs one big table, I can do this at the gold level.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1 and 2 are essentially bronze level data. Steps 3 and 4 are where standardization, cleaning, and modeling (star schema) are happening. After watching  &lt;a href=\"https://www.youtube.com/watch?v=fz4tax6nKZM\"&gt;Behind the Hype - The Medallion Architecture Doesn&amp;#39;t Work - YouTube&lt;/a&gt; , I am questioning if I&amp;#39;m approaching this wrong. The pattern I&amp;#39;m seeing in some other examples (this video included) are:&lt;br/&gt;\n1. Load raw data into directories.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Use Autoloader to create append only tables.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use apply_changes on all autoloader tables to create an SCD-1 or SCD-2 table per requirements. Apply data cleaning steps here.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Create a star schema at the gold level.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How are other folks doing this in practice? If the second set of steps, how are you managing bringing together SCD tables into a single dimension table?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for any help folks can provide!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kHqp9GgV9gleyN2rO4w4JISjRfATgc1UUbPeV4uCmso.jpg?auto=webp&amp;s=9babd61feb979da48d8457455f2f06c6d0bc60d6",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kHqp9GgV9gleyN2rO4w4JISjRfATgc1UUbPeV4uCmso.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5af2016e55300e6c11e7683b03278cf996931854",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/kHqp9GgV9gleyN2rO4w4JISjRfATgc1UUbPeV4uCmso.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=035dd87b44ab87592e6b0ee506979ee5ea18ea68",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/kHqp9GgV9gleyN2rO4w4JISjRfATgc1UUbPeV4uCmso.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ec6ffc9a7c1bc1e5535c36d1ac21d2432262516",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "ntuQHncbNhnVWIsuPyAdxhq8jVnFNa9wpMwBgS5--YA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c03hmy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DisastrousCase3062",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c03hmy/databricks_lakehouse_delta_live_tables_guidance/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c03hmy/databricks_lakehouse_delta_live_tables_guidance/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712697363.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Given the stuff that went on with xy, it seems like the industry it trying to find ways of better securing open source supply lines. A lot of people say that no hardening can accomplish this result because the issue had to do with social engineering.\n\nThis makes me wonder if we can engineer a stronger social platform. Maybe if there was a role for open source projects that consisted of security audits. Audits would openly submit results, components audited, methods of audit, and their identity. From the user perspective, they should care for the number of distinct auditors as well as the cumulative reputation of all auditors, down to the level of each component. From there, scores can be created to have more confidence in each component, taking into account the number of releases since the last audits.\n\nOr something\u2026 what do you think?",
          "author_fullname": "t2_uqm6fk35",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does it make sense to bolster metrics like `audit contributors` for open source projects?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c014gb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712691619.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given the stuff that went on with xy, it seems like the industry it trying to find ways of better securing open source supply lines. A lot of people say that no hardening can accomplish this result because the issue had to do with social engineering.&lt;/p&gt;\n\n&lt;p&gt;This makes me wonder if we can engineer a stronger social platform. Maybe if there was a role for open source projects that consisted of security audits. Audits would openly submit results, components audited, methods of audit, and their identity. From the user perspective, they should care for the number of distinct auditors as well as the cumulative reputation of all auditors, down to the level of each component. From there, scores can be created to have more confidence in each component, taking into account the number of releases since the last audits.&lt;/p&gt;\n\n&lt;p&gt;Or something\u2026 what do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c014gb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DuckDatum",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c014gb/does_it_make_sense_to_bolster_metrics_like_audit/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c014gb/does_it_make_sense_to_bolster_metrics_like_audit/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712691619.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Looking for advice on ways to make a postgres DB accessible to my colleagues. \n\nSo I have a couple of data sources internally that I use to populate a postgres database roughly once a month. The data sources are an s3 bucket containing a large number of xml files, plus a small number of json files that reference the xml files. I also gather metadata from a few image servers and load this into my database so that I can query everything together.\n\nMy current workflow is:\n* Use cli to get the current set of xml and json files from s3 into a local dir\n* Run a couple of python scripts on my machine to process these local files and write to a local postgres db\n* Run another python script that scans the image server that I'm connected to and load that metadata into the postgres db\n* Run any post-population SQL scripts that modify the data as required\n* Query the data to answer questions about data, analyse it etc.\n\nI do have the postgres db running in docker, mainly for admin purposes but also in case it helps later. The python scripts I just execute in order, and then I do any SQL stuff in DBeaver.\n\nI set this up for myself as it helps me do my job which is largely about managing this data. \n\nHowever, it would be useful for other people to be able to access the db to query it. The DB is currently on my machine in docker so what are my options for making it accessible to others? I don't have access to the aws account in which the s3 bucket lives, but I may be able to request permission to host a pgdb in RDS or something.\n\nJust curious how others might solve this!\n\nThanks for reading :)",
          "author_fullname": "t2_5n8mvd7p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best way to make my DB accessible to others in my org?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c00toj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712690855.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for advice on ways to make a postgres DB accessible to my colleagues. &lt;/p&gt;\n\n&lt;p&gt;So I have a couple of data sources internally that I use to populate a postgres database roughly once a month. The data sources are an s3 bucket containing a large number of xml files, plus a small number of json files that reference the xml files. I also gather metadata from a few image servers and load this into my database so that I can query everything together.&lt;/p&gt;\n\n&lt;p&gt;My current workflow is:\n* Use cli to get the current set of xml and json files from s3 into a local dir\n* Run a couple of python scripts on my machine to process these local files and write to a local postgres db\n* Run another python script that scans the image server that I&amp;#39;m connected to and load that metadata into the postgres db\n* Run any post-population SQL scripts that modify the data as required\n* Query the data to answer questions about data, analyse it etc.&lt;/p&gt;\n\n&lt;p&gt;I do have the postgres db running in docker, mainly for admin purposes but also in case it helps later. The python scripts I just execute in order, and then I do any SQL stuff in DBeaver.&lt;/p&gt;\n\n&lt;p&gt;I set this up for myself as it helps me do my job which is largely about managing this data. &lt;/p&gt;\n\n&lt;p&gt;However, it would be useful for other people to be able to access the db to query it. The DB is currently on my machine in docker so what are my options for making it accessible to others? I don&amp;#39;t have access to the aws account in which the s3 bucket lives, but I may be able to request permission to host a pgdb in RDS or something.&lt;/p&gt;\n\n&lt;p&gt;Just curious how others might solve this!&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1c00toj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kaiso_gunkan",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c00toj/best_way_to_make_my_db_accessible_to_others_in_my/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1c00toj/best_way_to_make_my_db_accessible_to_others_in_my/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712690855.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hi Everyone. My experience is limited to using PowerBi and other visualization tools. I'm interested in building a low cost data pipeline to help a friend. The elements inside the highlighted box (Shopify, JASCI and SFTP connector) exist today. I'd like to use the SFTP to extract the data to Azure DW and then visualize with PowerBI.\n\nHigh level requirements:\n1. Data is extracted daily using SFTP\n2. Data is transformed: formatting, extraction, etc.. using (?)\n3. Data is appended to existing tables using Azure DW\n4. Data is visualized in PowerBI\n\nA few questions\n1. Is this architecture the right way to think about the pipeline?\n\n2. Do I need to use the SFTP to first extract the data to Azure Blob?\n\n3. Is this a straightforward project that could be supported by a contractor off Upwork?\n\n4. Are there inexpensive connectors that will extract the data to a DW?\n\nThanks in advance for any advice you can provide. \n\n\n\n\n\n\n\n\n",
          "author_fullname": "t2_agvb5d4o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Advice on data architecture and execution",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c00m4w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Blog",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ny9oUsIRjv8vBVv0G6IB4N-lh16Yfm8xOK20y-9JJoE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1712690334.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone. My experience is limited to using PowerBi and other visualization tools. I&amp;#39;m interested in building a low cost data pipeline to help a friend. The elements inside the highlighted box (Shopify, JASCI and SFTP connector) exist today. I&amp;#39;d like to use the SFTP to extract the data to Azure DW and then visualize with PowerBI.&lt;/p&gt;\n\n&lt;p&gt;High level requirements:\n1. Data is extracted daily using SFTP\n2. Data is transformed: formatting, extraction, etc.. using (?)\n3. Data is appended to existing tables using Azure DW\n4. Data is visualized in PowerBI&lt;/p&gt;\n\n&lt;p&gt;A few questions\n1. Is this architecture the right way to think about the pipeline?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Do I need to use the SFTP to first extract the data to Azure Blob?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is this a straightforward project that could be supported by a contractor off Upwork?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are there inexpensive connectors that will extract the data to a DW?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for any advice you can provide. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ebl443so7itc1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ebl443so7itc1.jpeg?auto=webp&amp;s=c3f5c12731b593a7d2c598aeab5b74f666f874cd",
                  "width": 1137,
                  "height": 1548
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ebl443so7itc1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5558d325a6cb6adce5478f0016119da333d5c5c0",
                    "width": 108,
                    "height": 147
                  },
                  {
                    "url": "https://preview.redd.it/ebl443so7itc1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7973850016e9a3e1b5ad3adc106028c297b1435",
                    "width": 216,
                    "height": 294
                  },
                  {
                    "url": "https://preview.redd.it/ebl443so7itc1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d84d050ca29a47ce87e786cf7cfaac3274e188cb",
                    "width": 320,
                    "height": 435
                  },
                  {
                    "url": "https://preview.redd.it/ebl443so7itc1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=53708736ad0b5e03b6ad1cc2b1b583a6ced23f9a",
                    "width": 640,
                    "height": 871
                  },
                  {
                    "url": "https://preview.redd.it/ebl443so7itc1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce5bccf3dc90ceb5620f0c20fedc01c170366a7f",
                    "width": 960,
                    "height": 1307
                  },
                  {
                    "url": "https://preview.redd.it/ebl443so7itc1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4eb7b5610c9212ea308d826562c0134d61cb7021",
                    "width": 1080,
                    "height": 1470
                  }
                ],
                "variants": {},
                "id": "iB8-6KFuvjNyAUk0VNAK7BoX0QH_g9REOjOMsaGiJhU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1c00m4w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "National-Dare-4890",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c00m4w/advice_on_data_architecture_and_execution/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://i.redd.it/ebl443so7itc1.jpeg",
          "subreddit_subscribers": 176091,
          "created_utc": 1712690334.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Do you prefer using SQL instead? I got the impression that Pandas is primarily for Data Analyst / Scientist\n\n  \nEdit: the poll question is: Do you use Pandas professionally on a daily basis?\n\n[View Poll](https://www.reddit.com/poll/1c00fw8)",
          "author_fullname": "t2_c0yysfvj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you use Pandas professionally on a daily basis?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1c00fw8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1712733694.0,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712689899.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you prefer using SQL instead? I got the impression that Pandas is primarily for Data Analyst / Scientist&lt;/p&gt;\n\n&lt;p&gt;Edit: the poll question is: Do you use Pandas professionally on a daily basis?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1c00fw8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1c00fw8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FaithlessnessNorth65",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "poll_data": {
            "prediction_status": null,
            "total_stake_amount": null,
            "voting_end_timestamp": 1713121899291,
            "options": [
              {
                "text": "Yes",
                "id": "27698988"
              },
              {
                "text": "No",
                "id": "27698989"
              }
            ],
            "vote_updates_remained": null,
            "is_prediction": false,
            "resolved_option_id": null,
            "user_won_amount": null,
            "user_selection": null,
            "total_vote_count": 149,
            "tournament_id": null
          },
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1c00fw8/do_you_use_pandas_professionally_on_a_daily_basis/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "mod_reports": [],
          "url": "https://www.reddit.com/r/dataengineering/comments/1c00fw8/do_you_use_pandas_professionally_on_a_daily_basis/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712689899.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Hello everyone!\n\nAs a new BI Data Analyst at a company that previously relied on Sharepoint for data storage, I'm navigating through the transition from .xlsx or .csv files to MS Fabric and OneLake following successful negotiations. However, I'm currently facing challenges with .csv files from ServiceNow. My goal is to utilize SQL to create data frames within Fabric, although I'm currently using Gen2 Dataflows as an interim solution. In this scenario, would you recommend converting the .csv format to JSON and then writing SQL queries to load the data into frames? I would greatly appreciate any advice from more experienced engineers.",
          "author_fullname": "t2_80s64tdh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you work with .csv in Fabric??",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1bzz0mz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712686457.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;As a new BI Data Analyst at a company that previously relied on Sharepoint for data storage, I&amp;#39;m navigating through the transition from .xlsx or .csv files to MS Fabric and OneLake following successful negotiations. However, I&amp;#39;m currently facing challenges with .csv files from ServiceNow. My goal is to utilize SQL to create data frames within Fabric, although I&amp;#39;m currently using Gen2 Dataflows as an interim solution. In this scenario, would you recommend converting the .csv format to JSON and then writing SQL queries to load the data into frames? I would greatly appreciate any advice from more experienced engineers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1bzz0mz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MessFew",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1bzz0mz/how_do_you_work_with_csv_in_fabric/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1bzz0mz/how_do_you_work_with_csv_in_fabric/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712686457.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "TLDR: My company is proposing making nearly 60 people redundant in a month\u2018s time, all hired 2-4 months ago. \n\nEarly on this year I started a role as a trainee Data Engineer at a company which provides paid training to employees to act as consultants for their clients. I was so excited to start this job and have been really enjoying the work. For some extra detail, this company takes in training cohorts for different disciplines (e.g. cloud,  data product, machine learning).\n\nYesterday afternoon a briefing call was scheduled for 4pm. Nearly 60 of us, spanning across 3 cohorts, were told that the company is proposing we are made redundant in roughly a month\u2019s time. Obviously this came as a huge shock, I didn\u2019t really process it yesterday but it was difficult to wake up to this morning. \n\nThe company is starting a one month redundancy consultation period (collective consultation) where elected members of the cohort will negotiate with the company. During this period we have been asked to work under business as usual. It is stated that redundancy is only being proposed at the moment and that if we can come up with alternatives they will consider them. They have also said they have considered alternatives so I doubt there\u2019s much hope of us not being laid off. \n\nThe meeting itself was a bit eye opening. We were shown a PowerPoint presentation on the reasons they are taking this action and what the process of collective consultation will entail. At the end there was a bit about mental health, but as soon as the meeting had completed the speakers were out of there fast as a rocket so no one had the opportunity to ask any questions and we were left to process this news alone. \n\nI\u2019m feeling really upset. The company has said that this is due to a lack of demand from their clients, but I find it hard to believe they didn\u2019t know this was a risk at the start of this year, or two months ago when they took in the most recent cohort. Many of the members of my cohort have relocated to London for this job, and are now signed on to 12-month rental contracts. Additionally, people left jobs or gave up opportunities for these positions. \n\nAnyway I came on to vent a bit but also to ask for some advice. I was wondering if anyone has experience going through a redundancy consultation period before, and if any of the outcomes were positive? \n\nAdditionally, I\u2019d love to know if anyone had any recommendations of where I could apply for data engineer roles in London? (Or if anyone could recommend good recruiters?) I\u2019m hopeful that with the training I\u2019ve gained I\u2019m in a bit of a better position than before but I\u2019m really dreading being unemployed (having a job had done wonders for my mental health).\n\nFinally, I\u2019d just like to say don\u2019t be like me - always listen to glassdoor reviews and google reviews before you accept an offer. I was so excited to be offered a the position I ignored any doubts I had, I wish I had listened to my gut now. ",
          "author_fullname": "t2_f8jseokkh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Large-scale redundancy at my company (UK)",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1bzy50a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712684331.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: My company is proposing making nearly 60 people redundant in a month\u2018s time, all hired 2-4 months ago. &lt;/p&gt;\n\n&lt;p&gt;Early on this year I started a role as a trainee Data Engineer at a company which provides paid training to employees to act as consultants for their clients. I was so excited to start this job and have been really enjoying the work. For some extra detail, this company takes in training cohorts for different disciplines (e.g. cloud,  data product, machine learning).&lt;/p&gt;\n\n&lt;p&gt;Yesterday afternoon a briefing call was scheduled for 4pm. Nearly 60 of us, spanning across 3 cohorts, were told that the company is proposing we are made redundant in roughly a month\u2019s time. Obviously this came as a huge shock, I didn\u2019t really process it yesterday but it was difficult to wake up to this morning. &lt;/p&gt;\n\n&lt;p&gt;The company is starting a one month redundancy consultation period (collective consultation) where elected members of the cohort will negotiate with the company. During this period we have been asked to work under business as usual. It is stated that redundancy is only being proposed at the moment and that if we can come up with alternatives they will consider them. They have also said they have considered alternatives so I doubt there\u2019s much hope of us not being laid off. &lt;/p&gt;\n\n&lt;p&gt;The meeting itself was a bit eye opening. We were shown a PowerPoint presentation on the reasons they are taking this action and what the process of collective consultation will entail. At the end there was a bit about mental health, but as soon as the meeting had completed the speakers were out of there fast as a rocket so no one had the opportunity to ask any questions and we were left to process this news alone. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m feeling really upset. The company has said that this is due to a lack of demand from their clients, but I find it hard to believe they didn\u2019t know this was a risk at the start of this year, or two months ago when they took in the most recent cohort. Many of the members of my cohort have relocated to London for this job, and are now signed on to 12-month rental contracts. Additionally, people left jobs or gave up opportunities for these positions. &lt;/p&gt;\n\n&lt;p&gt;Anyway I came on to vent a bit but also to ask for some advice. I was wondering if anyone has experience going through a redundancy consultation period before, and if any of the outcomes were positive? &lt;/p&gt;\n\n&lt;p&gt;Additionally, I\u2019d love to know if anyone had any recommendations of where I could apply for data engineer roles in London? (Or if anyone could recommend good recruiters?) I\u2019m hopeful that with the training I\u2019ve gained I\u2019m in a bit of a better position than before but I\u2019m really dreading being unemployed (having a job had done wonders for my mental health).&lt;/p&gt;\n\n&lt;p&gt;Finally, I\u2019d just like to say don\u2019t be like me - always listen to glassdoor reviews and google reviews before you accept an offer. I was so excited to be offered a the position I ignored any doubts I had, I wish I had listened to my gut now. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ea0027",
          "id": "1bzy50a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PreferenceMoist1919",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1bzy50a/largescale_redundancy_at_my_company_uk/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1bzy50a/largescale_redundancy_at_my_company_uk/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712684331.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "We have a few sources of data that gets collected by a single, updating, ahistoric repository; data then passes through to other data stores (Elastic, RDSs, etc.) from this repository and through other pipelines.\n\nI'm tasked with refactoring the data architecture, at least for a specific department. Ideally, we'd want to use this ahistoric repository for data governance reasons, taking snapshots of the data, storing it in a data lake, then loading it into a data warehouse for transformations with Airflow and dbt - a typical ELT pipeline, imo.\n\nI think that's sufficient - it was my initial proposal and I'm just planning on building out a star schema in our DW for driving analytics.\n\nThe thing is, it's becoming clearer just how messy things are: not all data through this ahistoric repository is flagged as coming through this, so it's hard to see which data is automatically fair game. Plus, we have different environments for different platforms; I'm currently not allowed to use a prod instnace of one of our RDSs for reading and copying data into DW, and this may end up becoming an even larger project with other departments hopping on board and having this be a larger refactor.\n\nThe thing is, given that data is still planning on being extracted from our sources are varying, regular cadences, and that it's generally the same type of data over time, I think this relatively simple solution is still sufficient, but I'm not sure if I'm missing out on anything.\n\nDo you think I should consider alternatives? Should I provide more information to really help lay out the context?",
          "author_fullname": "t2_svn12",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this solution for our problem a relatively simple data warehouse setup?",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1bzy2mk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712684168.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a few sources of data that gets collected by a single, updating, ahistoric repository; data then passes through to other data stores (Elastic, RDSs, etc.) from this repository and through other pipelines.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m tasked with refactoring the data architecture, at least for a specific department. Ideally, we&amp;#39;d want to use this ahistoric repository for data governance reasons, taking snapshots of the data, storing it in a data lake, then loading it into a data warehouse for transformations with Airflow and dbt - a typical ELT pipeline, imo.&lt;/p&gt;\n\n&lt;p&gt;I think that&amp;#39;s sufficient - it was my initial proposal and I&amp;#39;m just planning on building out a star schema in our DW for driving analytics.&lt;/p&gt;\n\n&lt;p&gt;The thing is, it&amp;#39;s becoming clearer just how messy things are: not all data through this ahistoric repository is flagged as coming through this, so it&amp;#39;s hard to see which data is automatically fair game. Plus, we have different environments for different platforms; I&amp;#39;m currently not allowed to use a prod instnace of one of our RDSs for reading and copying data into DW, and this may end up becoming an even larger project with other departments hopping on board and having this be a larger refactor.&lt;/p&gt;\n\n&lt;p&gt;The thing is, given that data is still planning on being extracted from our sources are varying, regular cadences, and that it&amp;#39;s generally the same type of data over time, I think this relatively simple solution is still sufficient, but I&amp;#39;m not sure if I&amp;#39;m missing out on anything.&lt;/p&gt;\n\n&lt;p&gt;Do you think I should consider alternatives? Should I provide more information to really help lay out the context?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1bzy2mk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "paxmlank",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1bzy2mk/is_this_solution_for_our_problem_a_relatively/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1bzy2mk/is_this_solution_for_our_problem_a_relatively/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712684168.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "This is so embarrassing to ask that I have written this post several times and erased it before submitting.\n\nThe IT guy at work won't upgrade my Data Engineer's RAM from 16GB to 32GB since his RAM capacity is always in the 90%s. He has a Windows machine and uses a lot of tools like DataGrip, which he has experience with.\n\nI'm really annoyed that I have to keep justifying why he needs it.  \nHe handles a lot of data and I think it is beneficial that he uses tools that he is comfortable to work with. Also, RAM is dirt cheap nowadays.   \n\n\nAre there any other reasons I should include?  \n",
          "author_fullname": "t2_6pwdpk7j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Asking for more RAM for Data Engineering jobs",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1bzvyhk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 73,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712678895.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is so embarrassing to ask that I have written this post several times and erased it before submitting.&lt;/p&gt;\n\n&lt;p&gt;The IT guy at work won&amp;#39;t upgrade my Data Engineer&amp;#39;s RAM from 16GB to 32GB since his RAM capacity is always in the 90%s. He has a Windows machine and uses a lot of tools like DataGrip, which he has experience with.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really annoyed that I have to keep justifying why he needs it.&lt;br/&gt;\nHe handles a lot of data and I think it is beneficial that he uses tools that he is comfortable to work with. Also, RAM is dirt cheap nowadays.   &lt;/p&gt;\n\n&lt;p&gt;Are there any other reasons I should include?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1bzvyhk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "charlenecassar",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1bzvyhk/asking_for_more_ram_for_data_engineering_jobs/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1bzvyhk/asking_for_more_ram_for_data_engineering_jobs/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712678895.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "dataengineering",
          "selftext": "Can we create all of our source tables in the public schema, or that\u2019s not a best practice ?",
          "author_fullname": "t2_jwvruqzii",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Public schema in postgres",
          "link_flair_richtext": [],
          "subreddit_name_prefixed": "r/dataengineering",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1bzufap",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1.0,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1712675129.0,
          "link_flair_type": "text",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.dataengineering",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can we create all of our source tables in the public schema, or that\u2019s not a best practice ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": true,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_36en4",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ff4500",
          "id": "1bzufap",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Defiant-Farm7910",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "whitelist_status": "all_ads",
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/dataengineering/comments/1bzufap/public_schema_in_postgres/",
          "parent_whitelist_status": "all_ads",
          "stickied": false,
          "url": "https://www.reddit.com/r/dataengineering/comments/1bzufap/public_schema_in_postgres/",
          "subreddit_subscribers": 176091,
          "created_utc": 1712675129.0,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}